# Active-Learning-Backend


# Active Learning API (Django + CatBoost)

This is a Django-based backend for an **Active Learning system** using **CatBoost**. The system:
- Queries the most **informative instances** for labeling.
- Allows users to **label** instances via the frontend.
- Trains a **CatBoost model** once enough labels are collected.

## üöÄ Features
- **Django REST API** for querying & submitting labels
- **Active Learning** for iterative model improvement
- **CatBoost Classifier** for anomaly detection
- **SQLite Database** for storing labeled/unlabeled data

## üèóÔ∏è Installation & Setup

### 1Ô∏è‚É£ Clone the Repository
```bash
git clone https://github.com/yourusername/active-learning-backend.git
cd active-learning-backend
```

### 2Ô∏è‚É£ Set Up Virtual Environment
```bash
python -m venv venv
source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
```

### 3Ô∏è‚É£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 4Ô∏è‚É£ Apply Database Migrations
```bash
python manage.py migrate
```

### 5Ô∏è‚É£ Load Initial Data (If Needed)
```bash
python manage.py shell
exec(open('labeling/load_data.py').read())
```

### 6Ô∏è‚É£ Run the Server
```bash
python manage.py runserver
```
Access the API at: **http://127.0.0.1:8000/**

## üîó API Endpoints
| Method | Endpoint | Description |
|--------|---------|-------------|
| `GET`  | `/api/get_uncertain_instances/` | Retrieve unlabeled instances |
| `POST` | `/api/submit_labels/` | Submit labels for instances |
| `POST` | `/api/train_model/` | Train the CatBoost model |

## üìù Notes
- The API requires **at least 10 labeled instances** before training.
- The trained **CatBoost model is cached** for future predictions.

## üìå Author
Developed by **Langat Vincent** üöÄ

## üìú License
This project is licensed under the MIT License.

---

## üì§ Push to GitHub
```bash
git init
git add .
git commit -m "Initial commit"
git branch -M main
git remote add origin https://github.com/yourusername/active-learning-backend.git
git push -u origin main
```

Let me know if you need any modifications! üöÄ


# Air-Quality-in-Nicosia-Cyprus


# Sensor Calibration and Air Quality Monitoring in Nicosia, Cyprus

>This study presents a comprehensive application of advanced machine learning (ML) techniques to calibrate low-cost gas sensors (LCSs) used in urban air quality monitoring in Nicosia, Cyprus. While LCSs are cost-effective and enhance the spatial and temporal resolution of air quality networks, their performance is often degraded by environmental factors such as temperature, humidity, and cross-sensitivities to non-target gases. To address these challenges, several ML algorithms were evaluated for calibrating LCS measurements of CO, NO‚ÇÇ, O‚ÇÉ, and SO‚ÇÇ against reference instruments, with Random Forest emerging as the most effective model. The study further analyzed the impact of temporal resolution on calibration performance, revealing up to 21% improvement when using higher resolution data. Additionally, it explored the minimum amount of training data needed to maintain calibration quality under different calibration frequencies. Results showed that with strategic data sampling and retrospective calibration, the required training data could be reduced to as little as 22%, while still meeting the EU‚Äôs criteria for indicative measurements. These findings demonstrate that ML calibration significantly enhances LCS reliability, supporting their wider adoption in air quality monitoring applications.

## üìç Introduction

Air pollution remains one of the most pressing environmental and public health challenges of the 21st century. Exposure to elevated levels of air pollutants such as carbon monoxide (CO), nitrogen dioxide (NO‚ÇÇ), ozone (O‚ÇÉ), and sulfur dioxide (SO‚ÇÇ) has been consistently linked to respiratory illnesses, cardiovascular conditions, and increased rates of premature mortality. As a result, continuous and accurate monitoring of air quality is essential for public health protection and environmental policy enforcement.

Traditionally, air quality monitoring is carried out using reference-grade analyzers at designated observational stations. These instruments offer highly precise and reliable measurements that conform to international standards, making them suitable for regulatory applications. However, their high installation and maintenance costs limit their widespread deployment, leading to sparse spatial coverage‚Äîespecially in urban areas where pollution exposure is highly localized and variable.

To address the limitations of conventional monitoring networks, the scientific community has increasingly turned to low-cost sensors (LCSs), particularly those based on electrochemical principles. These sensors offer a promising means to enhance the spatial and temporal resolution of air quality monitoring systems at a fraction of the cost. Their affordability, ease of use, and portability have made them attractive not only for institutional monitoring but also for personal and community-level air quality assessments.

Despite their advantages, LCSs face a number of performance challenges that hinder their use in regulatory and high-precision monitoring. Key issues include signal drift over time, sensitivity to temperature and humidity fluctuations, cross-interference from non-target gases, and non-linear response patterns across different pollutant concentration ranges. These limitations often result in significant deviations from reference-grade instrument readings, particularly when LCSs are used in uncontrolled real-world environments.

To mitigate these shortcomings, post-deployment calibration has become a central strategy, with machine learning (ML) models emerging as a particularly effective solution. Unlike traditional calibration methods, ML algorithms can account for complex, non-linear relationships between raw sensor readings and environmental conditions, enabling more accurate mapping to reference-grade values. This approach typically requires the colocation of LCSs and reference instruments over a defined period to gather training data that reflects real-world variability.

In this project, we evaluate the performance of five ML algorithms‚ÄîLinear Regression (LR), Support Vector Regression (SVR), Random Forest (RF), Artificial Neural Networks (ANN), and Extreme Gradient Boosting (XGBoost)‚Äîin calibrating LCS data collected over six months at a traffic monitoring station in Nicosia, Cyprus. The study investigates not only the predictive accuracy of each algorithm but also the influence of practical factors such as the temporal resolution of the data, sampling strategy, and calibration frequency. A key focus is on determining the minimum data requirements for training while still achieving data quality objectives (DQOs) as defined by the European Union (EU) and the United States Environmental Protection Agency (EPA), thus evaluating whether calibrated LCSs can serve as reliable tools for indicative air quality monitoring.


---

## üéØ Project Objectives

The overarching goal of this project is to evaluate and improve the performance of low-cost air quality sensors (LCSs) using machine learning (ML) techniques under real-world urban conditions. The specific objectives of the study are outlined below:

### üìå 1. Evaluate Baseline Sensor Performance in the Field
Before applying any advanced calibration techniques, the study begins by assessing how LCSs‚Äîtypically calibrated under controlled laboratory conditions‚Äîperform when deployed in the field. This involves comparing raw sensor outputs with co-located reference-grade measurements to understand the extent of deviations and limitations due to environmental variables such as temperature, humidity, and interfering pollutants.

###  ‚öôÔ∏è 2. Apply and Compare Machine Learning Models for In-Situ Calibration
A core objective is to implement and compare the performance of five ML algorithms‚ÄîLinear Regression (LR), Support Vector Regression (SVR), Random Forest (RF), Artificial Neural Networks (ANN), and Extreme Gradient Boosting (XGBoost)‚Äîfor post-deployment calibration of sensor data. Each model is evaluated based on its ability to correct measurement biases and produce outputs that closely align with reference-grade instruments.

###  üîÅ 3. Analyze Data Sampling Strategies and Calibration Frequencies
The study examines how the choice of data sampling strategy (e.g., continuous vs. intermittent sampling) and calibration frequency (e.g., monthly, quarterly, biannually) affect the accuracy and robustness of the ML calibration models. This analysis is critical for determining cost-effective yet reliable deployment strategies for long-term sensor operation in urban air quality monitoring networks.

### üìä 4. Determine Minimum Data Requirements for Effective Calibration
Another objective is to investigate the minimum amount of training data required to build accurate and reliable ML calibration models. The aim is to strike a balance between minimizing data collection efforts and maximizing model performance, while still meeting the quality thresholds set by regulatory bodies.

###  üìè 5. Assess Regulatory Compliance of Calibrated Outputs
The post-calibration outputs are evaluated against the data quality objectives (DQOs) defined in the European Union‚Äôs Ambient Air Quality Directive (2008/50/EC) and the United States Environmental Protection Agency (EPA) guidelines. Specifically, the study assesses whether the calibrated sensor data meets the criteria for "indicative measurements," which require expanded uncertainties within prescribed limits for different pollutants.

###  üß† 6. Explore Feature Importance and Environmental Influences
Finally, the study explores the relative importance of various input features‚Äîsuch as temperature, relative humidity, and the presence of interfering gases‚Äîin influencing sensor measurements. This analysis helps to interpret the behavior of ML models and provides insights into which variables most significantly affect the accuracy of LCSs, thus guiding future sensor design and deployment strategies.

---

## üõ†Ô∏è Experimental Setup

In this work, I employed four electrochemical sensors to measure the concentrations of carbon monoxide (CO), nitrogen dioxide (NO‚ÇÇ), ozone (O‚ÇÉ), and sulfur dioxide (SO‚ÇÇ). Each sensor generates two raw analog voltage signals: one from the working electrode, which responds to the target gas, and another from the auxiliary electrode, which provides a background reference signal. These voltage signals are converted into gas concentrations, expressed in parts per billion (ppb), using calibration equations provided by the manufacturer, which are based on laboratory conditions. For clarity in this analysis, these computed values are referred to as laboratory (LAB) calibrated concentrations. The measurements were conducted over a six-month period, from 2 October 2019 to 31 March 2020, at a national regulatory air quality monitoring station located in Nicosia. The station is positioned about 10 meters from a major urban avenue and is equipped with reference-grade instruments capable of measuring the same four pollutants. These instruments undergo calibration at least once every three months or after maintenance activities. The site experienced Mediterranean climatic conditions with variable temperature and humidity, posing real-world environmental challenges for sensor calibration.

The calibration of low-cost sensor (LCS) measurements using machine learning (ML) began with data preparation and synchronization. Reference instrument readings, along with temperature and relative humidity (RH) measurements from nearby sensors, were recorded at two-minute intervals, while the LCSs collected data every two seconds. To align these datasets, LCS measurements were averaged over two-minute intervals and merged with the corresponding reference values, temperature, and RH data. Data cleaning involved removing rows with missing values and those with negative net sensor signals‚Äîcalculated by subtracting the auxiliary electrode signal from the working electrode signal. The cleaned dataset included as input variables: net sensor signals (NSS), temperature, RH, and temporal features such as month, day of the week, and hour. These temporal features were introduced to capture seasonal, weekly, and daily variability in pollutant levels. Additionally, due to known cross-sensitivities, reference concentrations of ozone and nitrogen dioxide were included as input features when calibrating the NO‚ÇÇ and O‚ÇÉ sensors, respectively.

<img src="figures/process.png" alt="Site Location" width="850"/>

Five ML models were employed for the calibration task: Linear Regression (LR), Support Vector Regression (SVR), Random Forest (RF), Artificial Neural Network (ANN), and Extreme Gradient Boosting (XGBoost). All model training and evaluations were conducted in Python using the Anaconda environment. The full dataset, covering six months, was split into training and testing subsets‚Äîwhere the first 80% of each month‚Äôs data was used for training and tuning model parameters, and the remaining 20% was used for testing. Hyperparameters for SVR and ANN were optimized through five-fold cross-validation combined with a grid search. In contrast, hyperparameter tuning for RF and XGBoost was performed using an automated machine learning library. Any remaining parameters were left at their default values to maintain consistency across models.

Following calibration, model performance was assessed using several statistical indicators. The best-performing model was then selected for further analysis to evaluate the importance of individual features in the calibration process. Additionally, this model was used to investigate the effects of varying the temporal resolution of training data, different data sampling strategies, and calibration frequency on the amount of data required for model training. Feature importance was assessed using the permutation feature importance method available within the Scikit-learn ML library, allowing a deeper understanding of which variables most influenced the accuracy of calibrated sensor outputs.

## üõ†Ô∏è Data splitting schemes
To understand how the sampling strategy of training data affects the calibration frequency and the quantity of data required for effective model training, a series of experiments were conducted using CO, NO‚ÇÇ, and O‚ÇÉ low-cost sensors. Calibration was carried out at three different frequencies‚Äîmonthly, every three months, and every six months. For the monthly calibration, training data was selected from the beginning of each month, capturing early-month conditions to inform the calibration models. This approach aimed to assess whether limited yet regularly collected data could maintain calibration accuracy over the course of each month.


  <img src="figures/fg3.png" alt="Site Location" width="1000"/>

For the three-month and six-month calibration intervals, two distinct data sampling strategies were evaluated. The first strategy involved selecting training data exclusively from the beginning of the calibration period. This method tested whether a snapshot of data at the start of the interval could be representative enough for accurate predictions throughout the entire three- or six-month period. The performance of the calibration models under this strategy would indicate the feasibility of infrequent but concentrated data collection efforts, which could be more practical for long-term deployments.

The second strategy for the longer calibration periods involved sampling training data from multiple points within the entire calibration interval‚Äîspecifically at the start of each month within the three- or six-month periods. This approach was designed to explore whether spreading out the sampling effort and collecting training data more evenly over time could yield better calibration outcomes. The goal was to determine whether a more distributed dataset would improve model generalizability across variable conditions, such as seasonal or weekly fluctuations. Comparisons between the two sampling strategies provided insight into optimal practices for balancing data collection effort, calibration frequency, and model performance.

---

## ü§ñ Machine Learning Models Applied

Five different ML algorithms were evaluated:

### Linear Regression (LR)
A simple baseline model that assumes a linear relationship between input features and target pollutant concentrations.

### Support Vector Regression (SVR)
A kernel-based model capable of modeling non-linear dependencies by transforming the feature space.

### Random Forest Regression (RF)
An ensemble of decision trees that averages multiple predictions to minimize overfitting and variance, particularly robust for complex, noisy datasets.

### Artificial Neural Networks (ANN)
Deep learning models consisting of interconnected neurons arranged in layers, capable of capturing intricate non-linear relationships between inputs and outputs.

### Extreme Gradient Boosting (XGBoost)
An optimized version of gradient-boosted trees, known for its high predictive power and efficiency on structured datasets.

All models incorporated auxiliary features like temperature and RH, critical for correcting environmental interferences affecting sensor signals.

Hyperparameter optimization was conducted through a combination of grid search and AutoML techniques (Microsoft FLAML).

---

## üìè Performance Evaluation
Model performance was evaluated using a combination of statistical metrics that assess different aspects of how well the calibration models fit the reference data. The first set of metrics includes the Pearson correlation coefficient (r), which quantifies the strength and direction of the linear relationship between the predicted (calibrated) values and the reference measurements. A high correlation indicates that the model predictions closely follow the trends in the reference data. The coefficient of determination (R¬≤) complements this by measuring the proportion of variance in the reference data that is explained by the model. Meanwhile, the normalized root mean squared error (NRMSE) provides a normalized measure of the average prediction error, expressing how far off the calibrated values are from the reference values relative to the mean reference concentration. These metrics together give a comprehensive view of accuracy, reliability, and precision of the calibration models.

<img src="figures/fr1.png" alt="Calibration Performance Overview" width="1000"/>

In addition to these traditional metrics, the evaluation also involved an analysis of bias and variance through the use of target diagrams. These diagrams help diagnose whether a model is overfitting or underfitting the data by visually separating the total error into components related to bias (systematic error) and variance (random error). Overfitting models typically show low bias but high variance, meaning they capture noise rather than underlying patterns, while underfitting models have high bias but low variance, failing to capture the true data structure. The root mean square error (RMSE) of the model is decomposed into the mean bias error (MBE), which captures systematic deviations, and the centered root mean square error (CRMSE), which captures unsystematic deviations. This decomposition enables a deeper understanding of the sources of model error and guides improvements in calibration approaches.

<img src="figures/fr2.png" alt="Calibration Performance Overview" width="1000"/>

Beyond these statistical assessments, the calibration models were also evaluated following FAIRMODE (Forum for Air Quality Modelling in Europe) guidelines, which provide a comprehensive framework for assessing low-cost sensor data quality and calibration efficacy. FAIRMODE emphasizes not only accuracy and precision but also practical considerations such as representativeness, uncertainty quantification, and regulatory compliance. The Relative Expanded Uncertainty (REU) metric, a key FAIRMODE indicator, was calculated to assess whether the calibrated low-cost sensor data meet established data quality objectives based on the EU Air Quality Directive. In addition, precision and bias error estimators were used to evaluate the feasibility of monthly and seasonal calibration schemes for different air quality applications with varying accuracy requirements. These evaluations ensure that the calibrated data are both scientifically robust and suitable for use in regulatory or indicative monitoring contexts, aligning with FAIRMODE‚Äôs goal of harmonizing air quality measurement standards and enhancing the credibility of low-cost sensor networks.


---

# üìà Results and Discussion

## 1. Baseline Performance of Low-Cost Sensors Versus Reference Measurements

Before applying any machine learning calibration, the raw outputs from the low-cost sensors (LCSs) were compared directly against co-located reference instruments. Statistical analyses including Shapiro-Wilk normality tests, t-tests for mean differences, and Fligner-Killeen variance tests were performed to assess the nature of discrepancies between sensor and reference data. It was found that the raw LCS signals exhibited significant deviations from the reference measurements across all target pollutants, particularly for SO‚ÇÇ. This highlighted the necessity of calibration even before considering operational deployment.

The discrepancies between LCS and reference measurements were attributed to a combination of sensor-specific limitations. Signal drift over time, cross-sensitivities to interfering gases, and environmental influences like temperature and relative humidity played substantial roles. Particularly for gases like NO‚ÇÇ and O‚ÇÉ, cross-sensitivity effects were significant and worsened sensor performance. Moreover, SO‚ÇÇ measurements were consistently poor, likely because ambient SO‚ÇÇ concentrations during the measurement period were close to or below the detection limit of the electrochemical sensors.


These baseline results strongly justified the need for machine learning-based calibration. Without effective correction strategies, the raw LCS outputs could not be considered reliable for even non-regulatory monitoring purposes. Furthermore, these findings provided a realistic benchmark against which the effectiveness of different machine learning models could later be assessed.

---

## 2. Machine Learning Calibration Performance Overview

After applying machine learning calibration techniques, substantial improvements were observed across all sensors, except for SO‚ÇÇ. Post-calibration, the Pearson correlation coefficients (r) exceeded 0.9 for CO, NO‚ÇÇ, and O‚ÇÉ, indicating a very strong linear relationship between the calibrated sensor outputs and the reference-grade measurements. This substantial gain in correlation demonstrates that ML models were highly effective in correcting for the non-linearities, environmental biases, and cross-sensitivities inherent in the raw sensor data.

<img src="figures/fig4.png" alt="Calibration Performance Overview" width="1000"/>

Random Forest (RF) consistently delivered the best calibration results across pollutants, outperforming ANN, XGBoost, SVR, and Linear Regression. RF‚Äôs superior performance can be attributed to its ensemble nature, which averages multiple decision trees and thus captures non-linear patterns without overfitting. While ANN and XGBoost also performed strongly, RF models were found to be more robust, especially under varying environmental conditions.

SO‚ÇÇ calibration, however, remained problematic even after ML correction. Despite model efforts, SO‚ÇÇ sensors did not achieve significant improvement because ambient concentrations were too low for the sensors to provide reliable readings. This underscores a key limitation in using electrochemical LCSs for gases present at very low ambient levels: when concentrations are close to the sensor's detection threshold, even sophisticated machine learning models cannot fully overcome the fundamental limitations of the sensor hardware.

---

## 3. Bias and Variance Analysis Through Target Diagrams

Target diagrams were used to decompose model errors into bias and variance components, providing deeper insights into calibration performance. For CO, NO‚ÇÇ, and O‚ÇÉ, machine learning models‚Äîparticularly RF‚Äîsuccessfully reduced both bias and variance compared to laboratory calibration baselines. In contrast, LAB calibrations showed systematic positive biases for NO‚ÇÇ and O‚ÇÉ, reflecting consistent overestimations relative to the reference measurements. This highlights one of the critical weaknesses of laboratory-based calibration when applied to field conditions: lack of adaptability to environmental and cross-sensitivity effects.

<img src="figures/fig6.png" alt="Target Diagram" width="850"/>

For CO sensors, RF models achieved near-zero bias and very low normalized RMSE (nRMSE), confirming that calibration models effectively corrected both systematic and random errors. The situation was similar for NO‚ÇÇ and O‚ÇÉ, albeit with slightly higher nRMSE values due to more pronounced environmental dependencies. Importantly, all ML-calibrated points fell within the unit circle of the target diagrams, implying that the models did not exhibit signs of overfitting and generalized well to unseen data.

In the case of SO‚ÇÇ, none of the models, including RF, managed to reduce bias or variance significantly. The high nRMSE and biases observed even after calibration suggest that environmental factors and low ambient SO‚ÇÇ concentrations overwhelmed the predictive capabilities of the models. This finding reinforces that ML calibration is not a panacea‚Äîsensor limitations at very low pollutant levels impose fundamental constraints that even advanced algorithms cannot fully overcome.

---

## 4. Compliance with EU Directive 2008/50/EC and US EPA Standards

Evaluating compliance against the EU DQOs revealed that ML-calibrated CO, NO‚ÇÇ, and O‚ÇÉ measurements successfully met the thresholds required for indicative monitoring. RF models, in particular, demonstrated relative expanded uncertainties (REUs) well below the 25% and 30% limits stipulated by EU Directive 2008/50/EC. This marks a significant achievement, indicating that with proper calibration, low-cost sensors can contribute meaningful data for non-regulatory urban air quality monitoring networks.

<img src="figures/fig7.png" alt="EU Compliance" width="1000"/>

SO‚ÇÇ calibration, however, failed to meet the DQOs under any model. The primary reasons include the sensor's poor signal-to-noise ratio at ambient concentrations and possible cross-sensitivities not adequately corrected even with advanced modeling. Thus, while ML techniques can greatly enhance LCS performance for certain pollutants, gas-specific limitations must be considered when planning sensor deployments.

<img src="figures/fig8.png" alt="EU Compliance" width="1000"/>

US EPA DQO evaluations using precision and bias thresholds confirmed similar trends. ML models met the less stringent non-regulatory thresholds (25‚Äì30% errors) for CO, NO‚ÇÇ, and O‚ÇÉ, making them suitable for applications like citizen science, community monitoring, and hotspot identification. However, none of the sensors achieved the <10% precision and bias errors necessary for regulatory enforcement purposes. This reinforces the current positioning of LCS networks as complementary, rather than primary, air quality monitoring solutions.

---

## 5. Feature Sensitivity and Variable Importance


Random Forest feature importance analysis provided critical insights into what factors most influenced sensor calibration performance. For CO sensors, the Net Sensor Signal (NSS) was overwhelmingly the most important feature, indicating that CO LCS outputs were relatively unaffected by environmental variables or cross-sensitivities. This explains why CO sensors showed good baseline performance even before machine learning calibration.

<img src="figures/fig9.png" alt="Feature Importance" width="850"/>

In contrast, calibrations for NO‚ÇÇ and O‚ÇÉ depended heavily on temperature, relative humidity, and cross-sensitivities to other gases. Including auxiliary variables as features improved calibration R¬≤ scores by up to 9%, emphasizing that environmental compensation is essential for accurate modeling. Notably, NO‚ÇÇ calibration benefited significantly from the inclusion of O‚ÇÉ measurements as an input feature, and vice versa, underscoring the value of accounting for cross-sensitivities in calibration workflows.

For SO‚ÇÇ, no dominant feature emerged. Both environmental variables and cross-sensitivities contributed inconsistently, which partially explains the persistent poor performance of SO‚ÇÇ calibration models. Overall, this feature sensitivity analysis informs future sensor network designs: robust auxiliary sensing (e.g., temperature, RH) is essential for improving calibration accuracy.

---

## 6. Effect of Training Data Volume and Calibration Frequency

An analysis of training data requirements showed that increasing the fraction of available data improved model performance, but gains plateaued beyond a certain point (around 70%). Monthly calibration cycles consistently outperformed 3- or 6-month intervals, primarily because they minimized the effects of seasonal drift and sensor aging, which degrade model accuracy over time if not periodically corrected.

<img src="figures/fig11.png" alt="Training Data Impact" width="1000"/>

Moreover, employing an interceptive sampling strategy‚Äîselecting samples across the full range of conditions rather than just temporally contiguous data‚Äîgreatly reduced the amount of training data needed. In some cases, using only 22% of available data was sufficient to maintain performance comparable to models trained on 80% or more data. This has profound implications for deployment costs: co-location periods with reference stations can be dramatically shortened without sacrificing calibration quality.

These findings underscore the importance of not only model selection but also intelligent data acquisition strategies in real-world LCS deployments. By optimizing both, operators can achieve high-quality monitoring while minimizing operational and logistical burdens.

---

# üìã Conclusion

Machine learning, particularly Random Forest models, has demonstrated the capability to calibrate low-cost air quality sensors to near-regulatory standards under real-world conditions. While not all pollutants (specifically SO‚ÇÇ) could be reliably corrected due to hardware detection limits, CO, NO‚ÇÇ, and O‚ÇÉ measurements were significantly improved, achieving European Union and US EPA indicative monitoring thresholds. By optimizing calibration frequency and training data strategies, deployment costs can be substantially reduced, making dense LCS networks feasible for cities aiming to expand their air quality surveillance capabilities.

---

# üìÇ Project Structure

```bash
Sensor-Calibration-AirQuality-Nicosia/
‚îú‚îÄ‚îÄ data/                # Raw and processed datasets
‚îú‚îÄ‚îÄ figures/             # All visualizations (fig1.png to fig11.png)
‚îú‚îÄ‚îÄ notebooks/           # Analysis Jupyter Notebooks
‚îú‚îÄ‚îÄ src/                 # Calibration model scripts
‚îú‚îÄ‚îÄ README.md            # This documentation
‚îî‚îÄ‚îÄ requirements.txt     # Environment dependencies


# Anomaly-Fraud-detection-in-Finance



# Anomaly Detection in Credit Card Transactions Using PyOD and Microsoft AutoML (FLAML)


### Introduction

Anomaly detection in financial markets plays a crucial role in identifying irregularities or outliers that deviate significantly from normal patterns, thus offering invaluable insights for risk management and fraud detection. As financial transactions grow increasingly complex and voluminous, traditional methods often struggle to cope with the scale and diversity of data generated. This is where advanced techniques such as PyOD (Python Outlier Detection) and Microsoft AutoML (FLAML) come into play.

**PyOD** provides a comprehensive suite of algorithms tailored for anomaly detection, including statistical approaches, proximity-based methods, and machine learning models like Isolation Forest and Autoencoders. These algorithms excel in detecting anomalies across various financial metrics such as transaction amounts, frequency, and temporal patterns, offering flexibility and robust performance in differentiating legitimate transactions from potentially fraudulent ones.

**Microsoft AutoML**, powered by **FLAML (Fast Lightweight AutoML)**, represents a significant leap forward in automating the machine learning pipeline for anomaly detection in financial markets. FLAML optimizes hyperparameters and selects the best model from a range of candidates, enabling efficient and accurate anomaly detection without requiring extensive manual intervention. By harnessing the power of automated machine learning, financial institutions can expedite the deployment of anomaly detection systems, enhance detection accuracy, and adapt rapidly to evolving financial landscapes.

Thus, integrating PyOD and Microsoft AutoML (FLAML) empowers financial institutions to stay ahead in detecting anomalies, mitigating risks, and safeguarding financial assets with heightened precision and efficiency.

---

### Objective

In this work, I have considered an imbalanced dataset of credit card frauds ([Kaggle Dataset](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)) with the target labels being authentic and fraudulent and evaluated a number of PyOD and AutoML (FLAML) models in identifying transactions that are, in some sense, different from the usual, authentic transactions.

As the data is highly imbalanced, I have implemented different balancing approaches such as undersampling, oversampling and SMOTE, and used the best model to evaluate the performance under each case.

Furthermore, as opposed to using accuracy alone as the evaluation metric, I have used different evaluation metrics such as **Precision**, **Recall**, **F1-score**, and **ROC-AUC**, which are considered robust especially when dealing with imbalanced datasets.

---

### Data

The dataset contains information on the transactions made using credit cards by European cardholders, in two particular days of September 2013. It presents a total of **284,807 transactions**, of which **492 were fraudulent**. Clearly, the dataset is highly imbalanced, the positive class (fraudulent transactions) accounting for only **0.173%** of all transactions.

**Columns in the dataset are:**

- `Time`: The time (in seconds) elapsed between the transaction and the very first transaction  
- `V1` to `V28`: Obtained from principle component analysis (PCA) transformation on original features that are not available due to confidentiality  
- `Amount`: The amount of the transaction  
- `Class`: The status of the transaction with respect to authenticity. The class of an authentic (resp. fraudulent) transaction is taken to be 0  

---

### Evaluation Metrics

Let TP, TN, FP and FN respectively denote the number of true positives, true negatives, false positives and false negatives among the predictions made by a particular classification model.

Below we give the definitions of some evaluation metrics based on these four quantities:

- **Accuracy** = (TP + TN) / (TP + TN + FP + FN)  
- **Precision** = TP / (TP + FP)  
- **Recall** = TP / (TP + FN)  
- **F1-Score** = 2 √ó Precision √ó Recall / (Precision + Recall)  
- **F2-score** = 5 √ó TP / (5 √ó TP + 4 √ó FN + FP)  

**ROC-AUC** is a metric used to evaluate the performance of binary classification models. It measures the ability of the model to distinguish between classes by plotting the True Positive Rate (TPR) against the False Positive Rate (FPR) at various thresholds.

---

### Feature Selection

High dimensionality creates difficulties for anomaly detection. When the number of features increases, the data becomes sparse, and anomalies may be hidden by irrelevant or noisy attributes ‚Äî a problem known as the **curse of dimensionality**.

To address this, I compared the distribution of each feature for both classes (fraudulent and authentic). Features with similar distributions across both classes were excluded, and only those with clearly different distributions were retained for further analysis.

![Figure 1: Features exhibiting different distributions](figures/fig1.png)

---

### Results and Discussion

Models were trained and evaluated using multiple metrics. Based on results in Figure 2, **FLAML‚Äôs ExtraTreeClassifier** emerged as the top performer:

![Figure 2: Model evaluation comparison](figures/fig2.png)

- **F1-score**: 0.744  
- **Precision**: 0.817  
- **Recall**: 0.684  
- **Accuracy**: 0.999  
- **ROC-AUC**: 0.966

Other observations:

- **LOF** had a good accuracy (0.989) but F1-score and recall of 0.0 ‚Äî poor fraud detection.
- **OCSVM** had low precision (0.015).
- **HBOS**: Strong ROC-AUC (0.967), F1-score (0.241), recall (0.857).
- **IForest and MCD**: Recall > 0.8, but lower precision than FLAML.

---

### Comparison Using Balancing Methods

FLAML‚Äôs ExtraTreeClassifier was evaluated under:

| Dataset     | Accuracy | Precision | Recall | F1-score | ROC-AUC |
|-------------|----------|-----------|--------|----------|---------|
| Imbalanced  | 0.999    | 0.817     | 0.684  | 0.744    | 0.966   |
| SMOTE       | 0.999    | 0.712     | 0.857  | 0.778    | 0.947   |
| SMOTEENN    | 0.998    | 0.535     | 0.867  | 0.661    | 0.974   |

---

### Confusion Matrices

![Figure 3: Confusion matrices - Imbalanced, SMOTE, and SMOTEENN](figures/fig3.png)

- **Imbalanced**: Excellent overall accuracy but low fraud detection (TP rate 0.32)  
- **SMOTE**: Improved recall to 0.857 (TP rate 0.86)  
- **SMOTEENN**: Highest recall (0.867), slight drop in precision

---

### Conclusion

FLAML‚Äôs ExtraTreeClassifier excels across imbalanced and balanced datasets. It maintains high accuracy (0.999), precision (0.817), and ROC-AUC (0.966) on imbalanced data. SMOTE and SMOTEENN both improve recall and F1-score at the cost of some precision.

While LOF and OCSVM struggled with detecting fraud, models like HBOS, IForest, and MCD showed promise but didn‚Äôt outperform FLAML.

Model choice depends on whether the goal is to minimize false negatives (favoring recall) or false positives (favoring precision).

---



# Interpretable Anomaly Detection in Credit Card Fraud Using LIME and SHAP


### Objective

In this work, I have used an imbalanced dataset of credit card frauds (with the target labels being authentic and fraudulent) to assess:

- Local and global explainability of different anomaly detectors in predicting anomalies in credit card transactions.
- Whether anomaly detectors agree on predictions, and if so, whether they give rise to similar LIME and SHAP value explanations; and what happens when they disagree.

---

### Data Description and Feature Selection

The dataset contains information on transactions made using credit cards by European cardholders, covering two days in September 2013. It presents a total of **284,807 transactions**, of which **492 were fraudulent** ‚Äî meaning the dataset is highly imbalanced (fraud cases constitute only **0.173%**).

**Columns in the dataset:**

- **Time**: Seconds elapsed since the first transaction  
- **V1 to V28**: Principal components from PCA transformation (original features withheld for confidentiality)  
- **Amount**: The transaction amount  
- **Class**: Label ‚Äî 0 for authentic, 1 for fraudulent

The total feature dimensionality after engineering is **34**. To enhance efficiency for SHAP and LIME explanations, I applied **Recursive Feature Elimination (RFE)**, **SelectFromModel (SFM)**, and **BorutaPy** to select the **top 10 features**.

---

### Explaining Models

To understand feature contributions, I trained six anomaly detectors:

- Logistic Regression  
- Random Forest  
- Gradient Boosting  
- LGBM Classifier  
- KNN (from PyOD)  
- Isolation Forest (from PyOD)  

Explainability was assessed from two perspectives:

- **Global explanations** (average feature impact across the dataset)  
- **Local explanations** (feature impact on a single prediction)  

White-box models like Logistic Regression and Tree-based models offer built-in explanation mechanisms (e.g., coefficients, feature importance). For complex black-box models, model-agnostic tools like **LIME** and **SHAP** were used.

---

### Model Performance Overview

Figure 1 compares models by **ROC-AUC score** and **training time**:

- **LGBM Classifier** performed best in ROC-AUC and training time.  
- KNN and Gradient Boosting had strong accuracy but longer training times.  
- **Random Forest** and **Isolation Forest** had short training durations ‚Äî making them suitable for computationally intensive explanations like SHAP.

![Figure 1: ROC-AUC and training times](figures/fg1.png)

---

### Global Explanation

#### Logistic Regression Coefficients

Logistic Regression allows interpretation via feature coefficients. The magnitude and direction (positive or negative) of coefficients provide insight into global feature importance.

![Figure 2: Logistic Regression feature importance](figures/fg2.png)

From Figure 2, feature **V10** contributes the most, while **V17** contributes the least. All features except **V11** have negative influence on predictions.

---

#### Tree-based Feature Importance vs. Permutation Importance

Tree models compute feature importance using **mean decrease in impurity** (e.g., Gini importance).  
**Permutation feature importance** shuffles individual features to measure performance drops, offering model-agnostic robustness.

- Built-in importance: Random Forest, Gradient Boosting, LGBM, Isolation Forest  
- Permutation importance: Calculated via sklearn  

![Figure 3: Built-in feature importance](figures/fg3.png)  
![Figure 4: Permutation feature importance](figures/fg4.png)

From Figure 4, features **V10**, **V11**, and **V17** consistently appear among top contributors, unlike in Figure 3. This shows permutation importance provides **more reliable and robust insights**, particularly when features are correlated or non-linear.

---

### Local Explanation

For local explanation (instance-level), I used LIME and SHAP on three cases:

1. All three models (Gradient Boosting, Isolation Forest, LGBM) agree on correct fraud prediction  
2. Only Gradient Boosting and Isolation Forest agree  
3. Only Isolation Forest and LGBM agree  

#### Case 1: All Models Agree

![Figure 5a: LIME Explanation - All Agree](figures/fg5.png)  
![Figure 5b: SHAP Explanation - All Agree](figures/fg5.png)

At this instance, **V17** was a strong contributor. LIME and SHAP agreed well for Isolation Forest, but differed across Gradient Boosting and LGBM, even though all predicted correctly ‚Äî indicating architectural differences in learning patterns.

---

#### Case 2: Gradient Boosting and Isolation Forest Agree

![Figure 6a: LIME - GB & IF](figures/fg6.png)  
![Figure 6b: SHAP - GB & IF](figures/fg6.png)

Even though only two models agreed, **V17** remained a top contributor across explanations. The inconsistencies further reinforce how interpretability depends on model structure.

---

#### Case 3: Isolation Forest and LGBM Agree

![Figure 7a: LIME - IF & LGBM](figures/fg7.png)  
![Figure 7b: SHAP - IF & LGBM](figures/fg7.png)

Similar to prior cases, **V17**, **V10**, and **V13** were repeatedly identified as impactful features, confirming their importance across models and interpretability techniques.

---

### Conclusion

This study demonstrates the effectiveness of different anomaly detection models in predicting credit card fraud using an imbalanced dataset. It highlights the value of **global** and **local explainability** using LIME and SHAP, which provides actionable insights and model transparency.

Key takeaways:

- **LGBM** and **Random Forest** offer strong performance and fast training  
- **Permutation importance** offers robustness for feature selection  
- Even when predictions agree, **explanations can vary significantly** between models  

Future work can explore **ensemble or hybrid models** to balance predictive performance and interpretability.

---

# Assessing tme complexity of  SHAP Explainers

### Objective

In this work, I have used two datasets: credit data and census data to evaluate and compare the computational efficiency (execution time) of 3 tree explainer algorithms:

- TreeShap algorithm built within the SHAP package  
- FastTreeShap v1 and FastTreeShap v2 algorithms built within the FastTreeShap package as defined in Lundberg, S. M., & Lee, S. I. (2017). The two algorithms are modifications of TreeShap to fully allow parallel computing.

From Lundberg, S. M., & Lee, S. I. (2017), the time complexity of a tree detector in calculating SHAP values is a function of a number of variables including the:

1. Number of samples used  
2. Number of estimators  
3. Maximum depth of each tree

Using Random Forest (RF) and Isolation Forest (IF) detectors and varying these variables, I have examined their execution times when calculating SHAP values using TreeShap, FastTreeShap v1 and FastTreeShap v2 algorithms.

Table 1 shows the description of the datasets. The number of instances in the credit transaction datasets was higher than 200,000. To make the SHAP calculations tractable in a reasonable time, I created a sub-sample of 100,000 instances for this dataset.

#### Table 1: Dataset description

| Dataset | # Instances         | # Attributes (original) | # Attributes (feature engineering) |
|---------|---------------------|--------------------------|-------------------------------------|
| Credit  | 100,000 (sub-sample)| 30                       | 34                                  |
| Census  | 48,882              | 14                       | 64                                  |

Prior to comparing the three SHAP algorithms in terms of execution times, we compared the SHAP values calculated by FastTreeShap v1 and FastTreeShap v2 against those calculated by the baseline algorithm (TreeShap), and found that the maximum difference is insignificant (lower than 10‚Åª‚Å∑).

Figure 1 shows the top 3 feature rankings based on SHAP values for RF model implemented on Census data. The results show that the SHAP calculations based on TreeShap, FastTreeShap v1 and FastTreeShap v2 algorithms are similar and provide similar explainability.

![Figure 1: RF SHAP calculations on census data obtained using the three algorithms](figures/f1.png)

> Data Sources:  
> - [Credit Card Fraud Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)  
> - [Census Income Dataset](https://archive.ics.uci.edu/dataset/20/census+income)

---

### Varying the Number of Samples

Figure 2 shows the execution times for each of the SHAP algorithms when the number of samples is varied from 1,000 to 10,000. Note that to effect these calculations, the number of estimators is set to 100 for both models. The maximum depth parameter is set to 8 for the RF model while for IF, based on its documentation, it is already fixed at ceil(log‚ÇÇ(n)), where *n* is the number of samples.

![Figure 2: Running times for varying number of samples](figures/f2.png)

The results show that SHAP calculations via FastTreeShap v1 and FastTreeShap v2 are generally faster compared to TreeShap. The speed-up in SHAP calculations is more pronounced for FastTreeShap v2 compared to FastTreeShap v1, especially at higher sample sizes.

---

### Varying the Maximum Depth

As initially mentioned, the maximum depth of IF is fixed at ceil(log‚ÇÇ(n)). Here I only examined the effect of RF‚Äôs maximum depth in SHAP calculation execution for the two datasets. To do this, I have fixed the number of samples at 10,000 and set the RF number of estimators at 100.

![Figure 3: Running times for varying maximum depth](figures/f3.png)

The result in Figure 3 shows that at lower values of maximum depth (<4), the difference in the execution times for the three algorithms seems to be minimal. As the values of maximum depth increase, the SHAP calculation execution times generally grow exponentially. The speed-up for FastTreeShap v2 is more pronounced than that of FastTreeShap v1, especially at higher values of maximum depth.

---

### Varying the Number of Estimators

To assess the effect of a model‚Äôs number of estimators on SHAP calculation execution time, I fixed the number of samples at 10,000 for the two models and maximum depth at 8 for the RF model.

Figure 4 shows the execution time of the three algorithms when the number of estimators for each model is varied from 40 to 200. It is worth noting that the speed-up for FastTreeShap v2 is significantly higher compared to that of FastTreeShap v1, especially when the number of estimators is set at high values.

At smaller values of number of estimators (<60), SHAP calculations using TreeShap seem to be generally faster than calculations using FastTreeShap v1. As the number of estimators increases beyond 60, however, the execution time for FastTreeShap v1 improves compared to TreeShap.

![Figure 4: Running times for varying number of estimators](figures/f4.png)

---

### Summary

Table 2 illustrates the average speed-up achieved by FastTreeShap v1 and FastTreeShap v2 in computing SHAP values for Credit and Census data. Depending on the dataset and the variable of concern, the average speed-up for FastTreeShap v1 ranges from 1.08 to 1.84, while that of FastTreeShap v2 ranges from 1.67 to 4.58.

#### Table 2: Average Speed-Up

| Dataset | Variable         | Speed-up (RF)         | Speed-up (IF)         |
|---------|------------------|------------------------|------------------------|
|         |                  | v1       | v2          | v1       | v2          |
| Credit  | No. of samples   | 1.84√ó    | **4.58√ó**   | 1.29√ó    | 3.15√ó       |
|         | Maximum depth    | 1.35√ó    | 3.03√ó       | ‚Äì        | ‚Äì           |
|         | No. of estimators| 1.13√ó    | 2.35√ó       | 1.29√ó    | 3.15√ó       |
| Census  | No. of samples   | 1.12√ó    | 1.69√ó       | 1.16√ó    | 2.07√ó       |
|         | Maximum depth    | 1.15√ó    | 1.67√ó       | ‚Äì        | ‚Äì           |
|         | No. of estimators| 1.12√ó    | 2.01√ó       | 1.08√ó    | 2.03√ó       |

---

### Conclusion

The comparison of SHAP value computation algorithms revealed that **FastTreeShap v2** significantly accelerates execution times compared to TreeShap and FastTreeShap v1.

- Achieved up to a **4.58√ó speed-up** on the Credit dataset  
- Up to **2.07√ó** on the Census dataset  
- This improvement is most notable with increased sample sizes, deeper tree depths, and more estimators

These results underscore **FastTreeShap v2‚Äôs superior efficiency**, making it highly suitable for **large-scale and complex model analyses**. Overall, FastTreeShap v2 offers substantial computational advantages, enhancing practical applicability in machine learning tasks.

---

### References

- [Credit Card Fraud Dataset ‚Äì Kaggle](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)  
- C. Molnar, *Interpretable Machine Learning*, 2019  
- J. Brownlee, *Permutation Feature Importance in Python*, 2020  
- T. Chen et al., *FLAML: A Fast Lightweight AutoML Library*, Microsoft Research  
- M. A. Ahmed et al., ‚ÄúInterpretable Models for Healthcare Using Feature Importance Techniques,‚Äù *Procedia Computer Science*, 2015  
- C. Molnar, *Interpretable Machine Learning*, 2019  
- J. Brownlee, ‚ÄúPermutation Feature Importance in Python,‚Äù *Machine Learning Mastery*, 2020  
- S. Rashidi, ‚ÄúExplaining Machine Learning Models with Feature Importance,‚Äù *Towards Data Science*, 2021  
- T. V. Pham, ‚ÄúExplaining the LIME,‚Äù *Towards Data Science*, 2019  
- S. Sengupta, ‚ÄúIntroduction to SHAP Values,‚Äù *Towards Data Science*, 2020
- Lundberg, S. M., & Lee, S. I. (2017). *Fast TreeSHAP: Accelerating SHAP Value Computation for Trees*. Proceedings of the 34th International Conference on Machine Learning (ICML)  
- Molnar, C. (2019). *Interpretable Machine Learning: A Guide for Making Black Box Models Explainable*  
- Sengupta. (2020). *Interpretable Machine Learning: Introduction to SHAP Values*. Towards Data Science



# Brain-Tumor-Detection-Using-Deep-Learning


# Brain-Tumor-Detection-Using-Deep-Learning
Early brain tumor detection holds paramount importance in early intervention and tailored treatment, significantly impacting patient outcomes and quality of life. Machine learning (ML) is revolutionizing this field by enhancing detection accuracy, speed, and efficiency. ML algorithms analyze vast amounts of medical data, including brain imaging scans such as MRI and CT, to identify patterns indicative of tumors. By automating the analysis process, ML expedites diagnosis, enabling healthcare professionals to detect tumors earlier and more reliably. Moreover, ML algorithms can predict tumor behavior, assist in treatment planning, and even personalize therapies based on individual patient characteristics. Furthermore, ongoing advancements in ML algorithms and imaging technologies continue to refine brain tumor detection methods, pushing the boundaries of what is possible in neuro-oncology. In essence, the integration of machine learning into brain tumor detection not only improves diagnostic capabilities but also paves the way for more precise, patient-centered care strategies, ultimately transforming the landscape of brain tumor management.

 Over the past few days, I have dived into testing the power of different CNN architectures in detecting a person's brain tumor status based on computerised MRI scans. I have used publicly available kaggle brain tumor MRI scan datasets (https://lnkd.in/drHX_pti). I started by implementing a custom CNN as a baseline model and with curiosity of leveraging the power of transfer learning, I tested three other pre-defined CNN architectures: MobileNetV2, DenseNet169 and ResNet 50. It is amazing that all of these models achieved an accuracy of over 92% on the test set. This accuracy further improved to as high as 99.8% by creating an ensemble prediction of all the four models. An ensemble was created by stacking their predictions and using Microsoft's AutoML (FLAML) to to search for an optimised final learner. For those eager to explore further, I have attached the notebook in pdf for your use. As always, comments for enrichment are much welcomed.


# Cancer-Detection-using-CNN


ùóüùòÇùóªùó¥ ùóÆùóªùó± ùóñùóºùóπùóºùóª ùóñùóÆùóªùó∞ùó≤ùóø ùóóùó≤ùòÅùó≤ùó∞ùòÅùó∂ùóºùóª ùòÇùòÄùó∂ùóªùó¥ ùóóùó≤ùó≤ùóΩ ùóüùó≤ùóÆùóøùóªùó∂ùóªùó¥


In the fight against cancer, early detection and accurate diagnosis are crucial. Machine learning and deep learning technologies have significantly advanced this effort, especially in detecting lung and colon cancers. These technologies excel at analyzing medical imaging and patient data, offering exceptional precision and transforming traditional diagnostic methods. By leveraging algorithms to spot patterns and anomalies in imaging data, healthcare providers can identify cancers at earlier stages, improving the chances of successful treatment and enhancing patient survival.


Deep learning models, such as convolutional neural networks (CNNs), have revolutionized medical image analysis. For lung cancer, these models accurately interpret CT scans, detecting subtle signs of malignancy that may be overlooked by humans. Similarly, for colon cancer, deep learning algorithms can identify polyps and precancerous lesions in endoscopic images with high precision. These advancements not only boost diagnostic accuracy but also reduce the workload on radiologists and pathologists, allowing them to concentrate on complex cases and treatment planning. Integrating these technologies into clinical practice represents a significant leap forward in personalized medicine, offering the potential to save lives and improve care quality.


Over the past few days, I have dived into testing the power of different CNN architectures in detecting a person's Lung and Colon Cancer status based on computerized CT scans. I have used publicly available kaggle Lung and Colon CT scan datasets (https://lnkd.in/dz_n_pYY). I started by implementing a custom CNN as a baseline model and with curiosity of leveraging the power of transfer learning, I tested two other pre-defined CNN architectures: MobileNetV2 and DenseNet169. It is amazing that all of these models achieved an accuracy of over 97% on the test set. This accuracy further improved to as high as 99.8% by creating an ensemble prediction of all the three models. An ensemble was created by stacking their predictions and using Microsoft's AutoML (FLAML)  to search for an optimized final learner. For those eager to explore further, I have attached the notebook in pdf for your use. As always, comments for enrichment are much welcomed.


# Cancer-Detection-using-Deep-Learning


# Codes


# Covid-19-Detection-Deep-Learning


## üß† Deep Learning-Based Detection of COVID-19 from CT Scans Using Convolutional Neural Networks


This study investigates the application of various Convolutional Neural Network (CNN) architectures for the detection of COVID-19 from computed tomography (CT) scan images. Given the critical need for rapid and accurate diagnostics during pandemics, deep learning offers a powerful tool to support radiologists and healthcare systems. We utilized the publicly available SARS-CoV-2 CT scan dataset, which comprises 2,482 labeled CT images, to train and validate a range of CNN models tailored for binary classification: COVID-19 positive and negative.

To assess the efficacy of deep learning in this medical imaging task, we implemented and compared several architectures, including a Custom CNN, MobileNetV2, DenseNet169, and ResNet50. Additionally, we constructed an ensemble model that combines the strengths of individual architectures to enhance overall predictive performance. All models were trained with appropriate data augmentation techniques and evaluated using standard performance metrics such as accuracy, precision, recall, and F1-score. The ensemble model demonstrated superior performance, achieving a remarkable accuracy of 99%, outperforming each individual architecture.

Our findings highlight the significant potential of CNN-based models in automating and accelerating COVID-19 diagnostics from CT scans. The results suggest that deep learning can augment clinical decision-making by providing scalable and reliable diagnostic support. This approach can be particularly impactful in resource-constrained settings, where rapid triage is critical. The success of the ensemble model also underscores the value of combining diverse network features to achieve more robust and generalized performance in medical imaging tasks.

## Introduction

The COVID-19 pandemic rapidly evolved into a global health emergency, exerting immense pressure on diagnostic and healthcare infrastructures worldwide. Early detection of infected individuals is crucial for implementing timely isolation and treatment strategies, ultimately reducing the transmission rate and strain on hospitals. Although reverse transcription polymerase chain reaction (RT-PCR) has been the gold standard for COVID-19 testing, it is often associated with significant drawbacks such as limited availability, delayed results, and a risk of false negatives, particularly during the virus's early stages.

In response to these limitations, artificial intelligence (AI), and in particular deep learning, has emerged as a powerful alternative for accelerating and augmenting COVID-19 diagnostics. Among the most effective techniques are Convolutional Neural Networks (CNNs), which have demonstrated state-of-the-art performance in a variety of image classification and pattern recognition tasks. CNNs are particularly well-suited to medical imaging, where they can automatically detect complex features and anomalies in radiographic images that may elude manual observation.

Computed Tomography (CT) imaging plays a critical role in visualizing the extent of lung infection, making it an ideal candidate for deep learning-driven COVID-19 diagnosis. Unlike chest X-rays, CT scans offer high-resolution cross-sectional images of the lungs, allowing for more detailed analysis of infection spread and severity. This granularity enhances the ability of CNNs to distinguish between healthy, pneumonia-affected, and COVID-19-infected lung tissues with high confidence.

In this study, we explore the use of various CNN architectures‚Äîincluding CustomCNN, MobileNetV2, DenseNet169, and ResNet50‚Äîfor the classification of CT scans as COVID-19 positive or negative. Each architecture offers a unique balance of complexity, accuracy, and computational efficiency, making them suitable for different deployment scenarios, from cloud-based systems to edge devices in low-resource settings. The models are trained and validated on a curated SARS-CoV-2 CT scan dataset collected from Brazilian hospitals, ensuring diversity and clinical relevance.

To further enhance diagnostic performance, we implement an ensemble model that combines the predictive strengths of all four CNN architectures. Ensemble learning leverages model diversity to reduce variance and bias, often achieving higher accuracy and generalization than individual models. This multi-model fusion is particularly beneficial in medical imaging, where even minor improvements in diagnostic accuracy can have significant clinical impact.

The broader goal of this research is to develop a scalable, efficient, and reliable AI-assisted diagnostic tool that can support radiologists and frontline healthcare workers. By evaluating model performance using metrics such as accuracy, sensitivity, specificity, and F1-score, we provide a comprehensive analysis of each architecture‚Äôs diagnostic potential. We also discuss the feasibility of real-world deployment, potential limitations of the dataset, and future directions for improving model robustness and interpretability in clinical settings.

## Objectives

- üöÄ **Benchmark Performance**: Evaluate and compare the classification performance of multiple CNN architectures using the SARS-CoV-2 CT scan dataset.

- üß† **Harness Ensemble Learning**: Enhance diagnostic accuracy and reliability through an ensemble approach that integrates predictions from diverse models.

- üè• **Support Real-Time Diagnosis**: Assess the practical feasibility of deploying CNN-based models in clinical environments as real-time decision support tools.

- üìä **Measure Generalization**: Analyze key performance metrics including accuracy, sensitivity, specificity, and F1-score to understand model reliability.

- üîç **Address Dataset Constraints**: Examine the impact of dataset limitations on model performance and generalizability, identifying areas for further data enrichment.

- üåê **Bridge Research and Application**: Highlight the potential for integrating deep learning-based COVID-19 diagnostics into existing medical workflows, particularly in resource-constrained settings.

## Dataset

This study utilizes the **SARS-CoV-2 CT scan dataset**, a publicly available resource hosted on [Kaggle](https://www.kaggle.com/datasets/plameneduardo/sarscov2-ctscan-dataset). The dataset consists of a total of **2,482 CT scan images**, evenly split between **1,252 COVID-19 positive cases** and **1,230 negative cases**. These images were collected from real patients at hospitals in **S√£o Paulo, Brazil**, and manually labeled by medical professionals. The balanced nature of this dataset provides an ideal foundation for training deep learning models, minimizing the risks of bias toward one class and ensuring a fair evaluation of classification performance.

![Figure 1: Age distribution](sample.png)

Before training, several preprocessing steps were performed to prepare the dataset for input into the CNN models. All CT images were **resized to a consistent resolution**, and pixel values were **normalized** to standardize contrast and intensity levels across the dataset. The data was then **randomly split into training, validation, and test sets** to allow for robust model training and unbiased evaluation. Data augmentation techniques such as horizontal flipping and rotation were also applied to increase the dataset's diversity and help models generalize better to unseen data.

You can access and explore the dataset directly from Kaggle via the following link:  
üîó [SARS-CoV-2 CT Scan Dataset on Kaggle](https://www.kaggle.com/datasets/plameneduardo/sarscov2-ctscan-dataset)

## Tools Used

This project leverages a variety of tools and libraries to build, train, and evaluate deep learning models for COVID-19 detection from CT scan images:

- **Python 3.8+** ‚Äî The primary programming language used for data processing, model development, and evaluation.
- **TensorFlow / Keras** ‚Äî Deep learning frameworks utilized for building and training the CNN architectures including CustomCNN, MobileNetV2, DenseNet169, and ResNet50.
- **NumPy** ‚Äî For efficient numerical computations and array manipulations.
- **Pandas** ‚Äî Data manipulation and management, especially for organizing metadata and results.
- **Matplotlib & Seaborn** ‚Äî Visualization libraries used to plot training curves, confusion matrices, and other performance metrics.
- **scikit-learn** ‚Äî For computing evaluation metrics such as accuracy, sensitivity, specificity, and generating confusion matrices.
- **OpenCV / PIL** ‚Äî Image processing libraries for preprocessing CT scans including resizing and normalization.
- **Jupyter Notebook** ‚Äî Development environment for interactive experimentation and visualization of results.
- **Kaggle** ‚Äî Source platform for the SARS-CoV-2 CT scan dataset used in this study.
- **Git & GitHub** ‚Äî Version control and collaborative platform for code management and sharing.

These tools collectively enabled efficient experimentation, reproducible workflows, and clear presentation of results.


## Model Architectures

Convolutional Neural Networks (CNNs) are a specialized class of deep neural networks designed to process and analyze visual data. Unlike traditional fully connected neural networks, CNNs exploit the spatial structure of images through local connections and shared weights, making them more efficient and effective for image classification tasks. CNNs have been instrumental in advancing the field of medical imaging, where high-resolution and complex patterns need to be interpreted accurately‚Äîparticularly relevant for detecting COVID-19 through CT scans.

A typical CNN architecture consists of several key components: **convolutional layers**, **activation functions**, **pooling layers**, **batch normalization**, **dropout**, and **fully connected (dense) layers**. The convolutional layers apply a set of filters (kernels) that slide over the input image to detect low-level features such as edges and gradients. These features are stored in feature maps that retain spatial information, which is essential for identifying regions affected by diseases like COVID-19.

Following the convolution operation, **activation functions** like ReLU (Rectified Linear Unit) are used to introduce non-linearity into the model. Without these functions, the network would behave like a linear classifier, limiting its ability to learn complex representations. **Pooling layers**, such as max pooling, are then used to reduce the spatial dimensions of feature maps. This not only reduces computational complexity but also helps the model become more robust to slight translations and distortions in the input images.

![Figure 1: Age distribution](architecture.png)

To stabilize and accelerate training, **batch normalization** is applied, normalizing the output of previous layers and maintaining a stable distribution of activations. Additionally, **dropout layers** are used to prevent overfitting by randomly deactivating a subset of neurons during training, forcing the network to learn more robust, distributed representations. Finally, the network's output is passed through one or more **fully connected layers**, culminating in a classification layer‚Äîtypically using a softmax or sigmoid function depending on the task.

In this study, we implemented and evaluated four different CNN architectures. The first, a **CustomCNN**, was designed from scratch and includes multiple convolutional, pooling, and dense layers. This architecture allowed us to have full control over the number of layers, kernel sizes, and activations, making it ideal for initial experimentation and baseline comparison. Despite being relatively shallow, CustomCNN performed competitively and provided valuable insights into the design of effective COVID-19 classifiers.

The second model, **MobileNetV2**, is a lightweight and efficient CNN architecture optimized for real-time inference on mobile and edge devices. MobileNetV2 employs **depthwise separable convolutions**, which break down standard convolution into two smaller operations: depthwise and pointwise convolutions. This drastically reduces the number of parameters and computational cost. Additionally, it uses **inverted residual blocks** and **linear bottlenecks**, which help retain feature quality even with reduced dimensionality. MobileNetV2 strikes a balance between accuracy and efficiency, making it highly suitable for deployment in clinical settings with limited computational resources.

We also integrated **DenseNet169**, a deeper and more complex CNN architecture characterized by its **dense connectivity**. In DenseNet, each layer receives input from all preceding layers, facilitating better feature propagation and reuse. This connectivity pattern alleviates the vanishing gradient problem and leads to more compact and efficient models. DenseNet169 is particularly effective for medical imaging tasks, where detailed features need to be preserved across many layers. Its ability to leverage previously learned features results in improved gradient flow and stronger performance on high-resolution CT images.

The fourth architecture, **ResNet50**, introduces a different strategy to deal with the challenges of training deep networks: **residual learning**. ResNet50 incorporates **identity shortcut connections** that bypass one or more layers, allowing the network to learn residual mappings instead of direct transformations. These skip connections enable deeper networks to be trained more effectively without suffering from vanishing gradients or degraded accuracy. ResNet50‚Äôs 50-layer depth allows it to model highly abstract features, making it well-suited for complex classification tasks like COVID-19 detection from CT scans.

To harness the strengths of all these models, we constructed an **ensemble model** that combines their individual predictions. Ensemble learning is a well-known technique for improving generalization by aggregating the outputs of multiple models. In our approach, the final prediction is made by averaging the softmax probabilities or using majority voting across the models. This method reduces model variance and enhances robustness, especially in cases where individual architectures may be sensitive to specific image features or noise. The ensemble consistently outperformed each standalone model, achieving a peak classification accuracy of 99%.

Overall, the combination of diverse CNN architectures‚Äîfrom lightweight models to deep, feature-rich networks‚Äîdemonstrates the flexibility and power of deep learning in medical diagnostics. Each model brings its own advantages, whether it‚Äôs interpretability, efficiency, depth, or resilience. The use of ensemble learning further underscores the value of architectural diversity in complex classification tasks. Together, these models form a comprehensive diagnostic framework that not only excels in accuracy but also shows promise for real-time, scalable deployment in healthcare environments.

## Results and Discussion
The evaluation of five CNN-based models‚ÄîCustomCNN, MobileNetV2, DenseNet169, ResNet50, and an Ensemble‚Äîdemonstrated the effectiveness of deep learning in detecting COVID-19 from CT scans. All models were trained and tested on the SARS-CoV-2 CT scan dataset using a consistent pipeline of preprocessing, augmentation, and training protocols. Performance was measured using key metrics: accuracy, loss, sensitivity, and specificity. These metrics provide both an overview of model correctness and a deeper understanding of diagnostic reliability.

Below is a summary of the final performance metrics for each model:

<div align="center">

| Model       | Training Accuracy | Test Accuracy | Test Loss | Sensitivity | Specificity |
|-------------|-------------------|---------------|-----------|-------------|-------------|
| CustomCNN   | ~94%              | ~93%          | ~0.20     | 97%         | 91%         |
| MobileNetV2 | ~95%              | ~94%          | ~0.18     | 96%         | 92%         |
| DenseNet169 | ~96%              | ~95%          | ~0.15     | 97%         | 93%         |
| ResNet50    | ~96%              | ~95%          | ~0.13     | 98%         | 94%         |
| Ensemble    | -                 | 99%           | 0.05      | 99%         | 99%         |

</div>



The results show a clear trend: as model complexity increased, performance improved‚Äîbut with diminishing returns past a certain point. The CustomCNN, while the simplest in design, offered respectable performance. Achieving 93% test accuracy with a sensitivity of 97% and specificity of 91%, it shows that even relatively shallow networks can perform well when trained on well-curated, balanced datasets. However, the higher test loss indicates greater prediction uncertainty compared to deeper models.
![Figure 1: Age distribution](customCNN_accuracy.png)
![Figure 1: Age distribution](customCNN_loss.png)
![Figure 1: Age distribution](customCNN_confusion.png)

MobileNetV2 outperformed CustomCNN across all metrics with only a slight increase in complexity. With a test accuracy of 94%, loss of 0.18, and a good balance between sensitivity and specificity (96% and 92%, respectively), it demonstrates the benefits of architectural efficiency. Given its low memory and computational demands, MobileNetV2 is highly promising for real-time COVID-19 detection in mobile health applications, especially in rural or under-resourced regions.
![Figure 1: Age distribution](MobileNetV2_Accuracy.png)
![Figure 1: Age distribution](MobileNetV2_loss.png)
![Figure 1: Age distribution](MobileNetV2_confusion.png)

DenseNet169 and ResNet50 both achieved 95% test accuracy, but ResNet50 edged ahead in both sensitivity and specificity. DenseNet‚Äôs dense connectivity pattern helped preserve and propagate gradients, allowing for better feature reuse and less vanishing during backpropagation. This helped it maintain a low test loss of 0.15 and high sensitivity of 97%. Still, it required significantly more computational resources, with long training times and higher GPU memory usage.

![Figure 1: Age distribution](DenseNet169_accuracy.png)
![Figure 1: Age distribution](DenseNet169_loss.png)
![Figure 1: Age distribution](DenseNet169_confusion.png)

ResNet50, utilizing residual skip connections to simplify learning, showed superior diagnostic reliability. With a test loss of only 0.13, sensitivity of 98%, and specificity of 94%, it produced the most confident and stable predictions among the single models. The ability of ResNet to mitigate vanishing gradients with depth enabled it to abstract more complex lung tissue patterns, giving it an edge in medical feature interpretation.
![Figure 1: Age distribution](ResNet50_accuracy.png)
![Figure 1: Age distribution](ResNet50_loss.png)
![Figure 1: Age distribution](ResNet50_confusion.png)

Perhaps the most important result lies in the ensemble model. By aggregating the outputs of the four individual models, the ensemble achieved an impressive 99% accuracy on the test set, with nearly perfect sensitivity and specificity. This dramatic improvement underscores the value of combining diverse model perspectives, especially in a sensitive domain like medical imaging. The reduction in false negatives (undetected COVID cases) and false positives (misclassified healthy cases) is particularly critical in real-world diagnostics, where both outcomes carry significant risk. The ensemble‚Äôs low test loss of 0.05 confirms its robust confidence in predictions. In practical terms, this means it makes fewer high-confidence incorrect decisions, an important factor in clinical acceptance. The success of this model supports the notion that even if individual models have minor weaknesses, their collective decision-making leads to more reliable and generalized outputs.

![Figure 1: Age distribution](Ensemble.png)

Training curves across all models show steady convergence with minimal signs of overfitting. Early stopping and dropout layers helped stabilize training. In particular, CustomCNN and MobileNetV2 converged quickly, while DenseNet and ResNet required more epochs but eventually achieved smoother validation trajectories, indicating stable learning of increasingly complex patterns. Visual inspection of sample predictions and analysis of confusion matrices reinforced the numeric evaluation. COVID-positive scans were consistently identified across models, with misclassifications mostly arising in borderline or low-quality images. The ensemble model was particularly resilient to such ambiguities, often correcting the mistakes of individual models.

The confusion matrix for the ensemble model showed very few false negatives or positives, confirming its high diagnostic precision. High sensitivity ensures that infected patients are correctly identified and isolated, while high specificity avoids unnecessary anxiety or treatment for healthy individuals. In a pandemic scenario, this balance is not just desirable but vital for effective healthcare deployment. Though all models generalize well, performance gains from model complexity plateau after a point. DenseNet and ResNet offer marginal benefits over MobileNetV2 in accuracy but at the cost of compute and inference time. MobileNetV2, meanwhile, delivers efficiency and robustness, making it more viable for deployment in time-critical environments.

One of the key findings is that CustomCNN, despite being manually designed with no pretrained weights, achieved a performance level competitive with MobileNetV2. This suggests that well-tuned, domain-specific CNNs can be an alternative to heavy, general-purpose pretrained architectures in certain scenarios.
Another major takeaway is the importance of sensitivity in pandemic response. The ability to detect actual COVID-19 cases with high accuracy can drastically reduce disease transmission. In this regard, ResNet50 and the ensemble model stand out, reducing false negatives to nearly negligible levels. Training time also plays a role in real-world applicability. While DenseNet and ResNet require long training cycles, the ensemble strategy allows training them in parallel and aggregating their knowledge, making deployment more practical when infrastructure allows.

Furthermore, this experiment confirms the value of architecture diversity. Each model brought different strengths‚Äîbe it efficiency, robustness, or deep abstraction. Combining these characteristics in the ensemble allowed it to generalize well even on challenging cases. Notably, the ensemble model performed best on low-contrast CT images, where individual models were prone to minor errors. The probabilistic smoothing across model outputs created a more stable classification output in ambiguous regions of the lung. Model robustness was also reflected in minimal variance between training and test accuracy, confirming low overfitting risk. This is attributed to effective regularization and a well-balanced dataset. Still, performance in the presence of noisy, real-world data remains to be validated.

Interestingly, despite being deep models, both DenseNet169 and ResNet50 avoided degradation in accuracy‚Äîa problem common in deep networks‚Äîdue to their respective use of skip and dense connections. These architectures showed that network design can enable depth without sacrificing performance. We also observed that training curves were relatively smooth across all models, indicating no catastrophic forgetting or instability during the learning process. This shows that CT-based COVID detection is a well-posed problem for CNNs when the data is clean and well-labeled.

Despite the success of these models, we note that the dataset‚Äîwhile high in quality‚Äîoriginates exclusively from Brazilian medical institutions. This limits demographic and scanner variability, and the results might not generalize globally without additional data from diverse regions and machines. To validate generalizability, future testing on multinational datasets would be essential. Demographic variability, scan machine differences, and disease severity all impact image characteristics and thus model performance. The models also lacked built-in interpretability mechanisms. In clinical practice, radiologists and healthcare providers need transparency. Explainable AI techniques such as Grad-CAM or LIME should be integrated into future iterations for visualizing decision-making.

While ResNet50 and DenseNet169 offer excellent accuracy, their computational demand makes them less ideal for deployment in resource-constrained settings. MobileNetV2, in contrast, offers a balance of performance and speed, with lower memory usage and real-time inference potential. Given the ensemble model‚Äôs superior performance, it represents the most viable candidate for real-world deployment in centralized diagnostic centers. However, MobileNetV2 remains ideal for portable diagnostic applications where resources are limited but accuracy cannot be compromised. These findings position deep learning as a powerful supplementary tool in pandemic preparedness, capable of accelerating triage, supporting radiologists, and improving diagnostic throughput. With continued optimization and validation, such systems could reshape the diagnostic landscape.

## Conclusion

This study evaluated and compared the performance of four prominent CNN architectures‚ÄîCustomCNN, MobileNetV2, DenseNet169, and ResNet50‚Äîalongside an ensemble model, for the classification of COVID-19 from CT scan images. The results demonstrate that deep learning models can reliably detect infection patterns in lung scans, with the ensemble model achieving exceptional diagnostic performance, reaching 99% accuracy, sensitivity, and specificity. While each individual model showed strong merit, their collective strength through ensembling delivered the most stable and reliable results, minimizing false positives and negatives‚Äîan essential factor for clinical adoption.

Our findings confirm the viability of CNN-based methods as supplementary tools for rapid and automated diagnostics in pandemic scenarios. The high sensitivity and specificity across models indicate their potential to reduce misdiagnosis risks, thereby supporting and augmenting clinical workflows. Additionally, the diversity in model architecture suggests deployment flexibility, ranging from lightweight models suitable for mobile or edge devices to more complex networks that excel in centralized diagnostic labs. This work highlights the critical role that AI and deep learning can play in enhancing medical imaging and accelerating healthcare responses in crisis-driven situations.

Furthermore, the study underscores the importance of continual model refinement and validation on diverse datasets to maintain robustness as new virus variants emerge and imaging technologies evolve. Integrating explainability and interpretability techniques will be key to building clinical trust and facilitating the integration of these AI tools into existing diagnostic pathways.

---

## Recommendations

- **Adopt ensemble models** in centralized healthcare facilities to maximize diagnostic accuracy and minimize variance across predictions.
- **Deploy MobileNetV2-based models** on mobile or edge devices to provide rapid diagnostics in low-resource, rural, or point-of-care settings.
- **Incorporate explainability techniques** such as Grad-CAM to enhance clinician trust and improve the interpretability of model decisions.
- **Validate models on larger, multi-center datasets** to ensure robustness and generalizability across diverse patient demographics and imaging equipment.
- **Optimize models for real-time inference** to enable seamless integration with hospital PACS systems and clinical workflows.
- **Continuously update and retrain models** with new data reflecting evolving virus strains and imaging protocols to maintain diagnostic accuracy.

---

## References

- Plamen Eduardo. SARS-CoV-2 CT-Scan Dataset. Kaggle. [https://www.kaggle.com/plameneduardo/sarscov2-ctscan-dataset](https://www.kaggle.com/plameneduardo/sarscov2-ctscan-dataset)
- Chollet, F. (2017). Xception: Deep learning with depthwise separable convolutions. *CVPR*.
- He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. *CVPR*.
- Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely connected convolutional networks. *CVPR*.






# Credit-Scoring-Lending-automation


# üí≥ Credit Scoring Lending Automation

## üìö Introduction

The automation of lending decisions through machine learning is transforming the financial landscape by bringing speed, scalability, and data-driven precision to credit risk assessment. Historically, lending decisions were made using time-consuming manual processes that relied heavily on human judgment and rigid rule-based systems. These conventional approaches often suffered from inefficiencies, inconsistencies, and embedded biases, which not only delayed loan processing but also created barriers for underserved populations. The integration of machine learning allows institutions to transcend these limitations, leveraging vast datasets to make faster and more equitable decisions.

Machine learning models have the capability to analyze thousands of borrower attributes in seconds‚Äîfrom demographic and financial history to behavioral and transactional patterns‚Äîenabling more accurate predictions of default risk. This automation accelerates the underwriting process, reduces operational costs, and minimizes human error. Moreover, it allows financial institutions to process high volumes of loan applications simultaneously, creating a scalable system that adapts to demand fluctuations without the need to proportionally increase personnel or resources. These efficiencies are particularly valuable for fintech startups and digital lenders striving to offer instant or same-day approvals.

In this project, we assess the impact of three powerful machine learning algorithms‚Äî**Random Forest**, **XGBoost**, and **LightGBM**‚Äîon the loan decision-making pipeline. Each of these algorithms brings unique strengths: Random Forest excels in stability and interpretability, XGBoost offers high accuracy and fine-tuned control over model behavior, and LightGBM stands out for its training speed and performance with large datasets. By comparing their performance on real-world credit datasets, we explore how predictive accuracy, processing speed, and computational efficiency translate into measurable gains in operational cost reduction and customer experience improvement.

Beyond mere automation, the models developed in this project are designed to support **personalized lending**. By generating individualized risk scores, lenders can tailor interest rates and loan terms to align with each applicant's creditworthiness. This dynamic pricing not only improves profitability but also democratizes access to credit by appropriately managing risk. Overall, this work illustrates how a data-driven approach to lending can deliver strategic advantages across speed, fairness, scale, and profitability‚Äîpositioning machine learning as a cornerstone in the future of digital financial services.

---

## üìå Business Problem

Manual processing of loan applications remains a significant bottleneck in many financial institutions. This traditional approach demands extensive human labor to review applicant information, assess creditworthiness, and make lending decisions. The process is not only time-consuming but also costly, as it requires a dedicated team of underwriters and analysts to manage the growing volume of applications. As the demand for loans increases, relying solely on manual review becomes inefficient and unsustainable, limiting the institution‚Äôs ability to scale operations and meet customer expectations.

Moreover, manual decision-making is inherently prone to human error and subjective bias. Different reviewers may interpret the same application inconsistently, resulting in unpredictable approval outcomes. This inconsistency can undermine trust in the lending process and lead to unfair denials or approvals, exposing the institution to regulatory scrutiny and reputational risk. In addition, slow loan processing times can frustrate customers, reducing satisfaction and increasing the likelihood of applicants turning to competitors or unregulated lenders.

The business impacts of these challenges are profound:

1. **Increased Operational Costs** ‚Äî High labor expenses are incurred to maintain large underwriting teams, reducing profit margins and creating inefficiencies in resource allocation.
2. **Longer Turnaround Times** ‚Äî Delays in loan approval adversely affect customer experience and can lead to lost business opportunities, especially in a competitive lending market.
3. **Inconsistent Decision Quality** ‚Äî Subjectivity in manual reviews can result in poor credit decisions, either exposing lenders to higher default risk or unnecessarily restricting credit access.
4. **Scalability Constraints** ‚Äî Without automation, institutions struggle to handle surges in application volume, limiting their market reach and growth potential.
5. **Regulatory and Compliance Risks** ‚Äî Lack of standardized and transparent decision-making processes can increase vulnerability to compliance violations and penalties.

Addressing these issues through automated, machine learning-based credit scoring can dramatically improve efficiency, consistency, and scalability, ultimately enhancing both business performance and customer satisfaction.


---

## üéØ Main Task

The primary goal of this project is to develop a machine learning model that automates the assessment and decision-making process for loan applications. This system must minimize incorrect approvals (which can lead to financial losses) and incorrect rejections (which can damage customer relations and reduce revenue), while maximizing decision accuracy. Ultimately, the model should drive operational efficiency, reduce dependence on manual labor, and scale seamlessly to handle increasing loan volumes‚Äîall critical to sustaining competitiveness and profitability in the lending market.

To achieve this, the approach involves several business-focused tasks:

1. **Data Collection and Preparation: Building a Reliable Foundation**  
   Collect and cleanse extensive loan application data, including customer demographics, credit history, and repayment records. Proper data preparation ensures the model is trained on accurate, consistent information, directly impacting decision quality and reducing costly errors in lending.

2. **Feature Engineering: Extracting Business-Relevant Insights**  
   Develop predictive features that reflect borrower creditworthiness, such as income stability, debt ratios, and credit utilization. Well-crafted features enable the model to capture subtle risk patterns, enhancing predictive accuracy and supporting smarter lending decisions that lower default rates and protect the institution‚Äôs capital.

3. **Model Selection and Training: Maximizing Predictive Performance**  
   Train and compare machine learning algorithms‚ÄîRandom Forest, XGBoost, and LightGBM‚Äîthat have proven successful in credit scoring. Optimizing these models with cross-validation ensures high accuracy and robustness, which translates into better risk management, fewer losses, and improved customer trust.

4. **Performance Evaluation: Balancing Risk and Opportunity**  
   Use metrics like precision, recall, and AUC to evaluate model performance, with particular focus on reducing false positives and negatives. This careful evaluation minimizes financial risk from bad loans and preserves growth opportunities by approving creditworthy applicants, thereby sustaining business growth.

5. **Cost Reduction Analysis: Driving Operational Efficiency**  
   Quantify the reduction in loan processing time and labor costs achieved through automation. Faster, automated decisions decrease overhead expenses and improve customer satisfaction by delivering prompt loan approvals, giving the lender a competitive edge in the market.

6. **Deployment and Integration: Enabling Scalable and Compliant Operations**  
   Ensure the model can be seamlessly integrated into existing loan processing systems for real-time decision-making. Incorporate monitoring and retraining protocols to maintain performance over time, while meeting regulatory standards and maintaining transparency‚Äîcritical for sustainable, scalable business operations.

This comprehensive, business-driven approach ensures the project delivers not only a high-performing credit scoring model but also measurable improvements in risk management, cost efficiency, and customer experience.


---

## üõ†Ô∏è Tools and Data Management

This project utilized a comprehensive suite of tools and platforms to support efficient data management, model development, deployment, and business intelligence, ensuring a seamless end-to-end lending automation workflow.

- **Data Storage and Retrieval**:  
  Loan application data was securely stored in a **PostgreSQL** database and accessed via **DBeaver**, which enabled precise SQL querying and data extraction. This direct integration with the data source ensured that machine learning models were trained on accurate, up-to-date data, reducing preprocessing time and enhancing the responsiveness of the lending decision system.

- **Machine Learning and Hyperparameter Optimization**:  
  Models were developed using state-of-the-art algorithms including **LightGBM (LGBM)**, **Random Forest (RF)**, and **XGBoost**, implemented in Python. Model tuning and validation were automated using **Microsoft AutoML (FLAML)**, optimizing performance while minimizing manual intervention. This facilitated the rapid development of robust models that drive accurate loan approvals and risk assessments, directly impacting operational efficiency and cost reduction.

- **Model Deployment and Collaboration**:  
  To ensure scalability and ease of deployment, the best-performing model (LGBM) was containerized using **Docker**. Containerization guarantees consistent environments across development, testing, and production, enabling smooth integration with existing IT infrastructure and faster rollout of updates. Collaborative development and version control were managed through **Vision Control**, ensuring code quality, traceability, and coordinated teamwork ‚Äî critical for maintaining reliability in financial applications.

- **Business Intelligence and Reporting**:  
  For post-risk analysis, monitoring, and reporting, the project leveraged **Preset**, a modern business intelligence platform. Preset provided interactive dashboards and real-time insights into model performance, risk metrics, and cost savings, empowering business stakeholders to make informed decisions and track the impact of automation on loan processing efficiency.

- **Business Impact**:  
  The integration of these technologies supports an agile, transparent, and scalable lending automation system. Efficient data querying from PostgreSQL, automated model optimization, reliable containerized deployment, and rich BI reporting collectively reduce operational costs, mitigate risk, and enhance customer satisfaction through faster, data-driven loan decisions.

This holistic technology stack enables the financial institution to stay competitive, responsive, and compliant in a rapidly evolving lending landscape.



## üöÄ Performance Evaluation of the Models

Model performance is critical in ensuring reliable and profitable lending decisions. To maximize predictive accuracy, hyper-parameter tuning was performed using Microsoft AutoML‚Äôs FLAML framework, which leverages automated cross-validation to systematically explore parameter combinations. This optimization process ensures that each model‚ÄîRandom Forest, XGBoost, and LightGBM‚Äîoperates at peak efficiency, reducing misclassifications that could lead to costly loan defaults or missed revenue opportunities.

By carefully managing the ‚Äòtime budget‚Äô during tuning, the approach balances computational expense with model improvement, allowing the institution to deploy effective models faster without incurring excessive resource costs. The resulting models demonstrate high accuracy, precision, and recall, directly impacting the lender‚Äôs risk mitigation strategies and enhancing customer satisfaction through quicker, more accurate loan decisions.

Ultimately, this rigorous evaluation and tuning process supports scalable, automated credit scoring systems that minimize human bias and error, reduce operational costs, and improve portfolio quality‚Äîkey factors that strengthen competitive advantage and drive long-term business growth in the increasingly data-driven financial services sector.

![Performance Evaluation Table](table_22.png)

As shown in the evaluation results, the three machine learning models achieved accuracy scores ranging between 72% and 76%, with the Light Gradient Boosting Machine (LGBM) Classifier outperforming the others across all key metrics. This positions LGBM as the leading model in striking a crucial balance between precision and recall, which is essential for correctly approving creditworthy applicants while minimizing risky approvals. For lenders, this balanced performance translates into more reliable decisions that safeguard revenue while maintaining customer satisfaction through fair treatment.

The F1-score, a harmonic mean of precision and recall, is highest for LGBM, indicating its superior ability to balance the trade-offs between false positives (wrongly approving risky applicants) and false negatives (wrongly rejecting good applicants). From a business perspective, this balanced approach reduces costly errors in loan processing: fewer defaults due to risky loans slipping through, and fewer missed revenue opportunities from unjust rejections. The LGBM model thus aligns well with the dual goals of risk mitigation and market competitiveness.

Recall is especially critical in credit scoring, and LGBM demonstrates the highest recall rate, meaning it excels at identifying applications that should be rejected. This reduces the risk of approving potentially harmful loans (false negatives), which directly lowers financial losses and bad debt provisions. For financial institutions, deploying a model with high recall protects capital and reinforces regulatory compliance by minimizing the risk of extending credit to unqualified borrowers.

In contrast, while Random Forest (RF) provides a reasonably reliable alternative, it falls slightly behind LGBM in precision and recall metrics. Nevertheless, RF can still be leveraged in scenarios where interpretability or model stability is prioritized. Offering a dependable fallback, RF supports continuous operations if LGBM requires retraining or adjustment, ensuring the lending platform remains robust.

On the other hand, XGBoost underperforms relative to the other models, with lower precision, recall, and overall accuracy. This means it generates more false positives and false negatives, increasing both financial risk and customer dissatisfaction due to inappropriate approvals or rejections. Although XGBoost is widely popular for classification tasks, its current performance suggests a need for further hyper-parameter tuning, feature engineering, or model ensemble strategies to unlock its potential in this specific credit scoring context.

For evaluation, the dataset consisted of 9,898 loan applications, split into 90% for training and 10% for testing with unseen data. This approach ensures that model performance reflects real-world generalizability rather than overfitting. The use of a large and representative test set provides lenders confidence that these results will translate into improved decision-making and operational efficiency at scale, empowering institutions to process high application volumes swiftly while maintaining sound risk controls.

---

## üìà Further Evaluation Based on Confusion Matrices

The Light Gradient Boosting Machine (LGBM) classifier excels at correctly approving valid loan applications, achieving a true positive rate of 84%. This strong ability to identify genuine, creditworthy applicants directly translates into increased loan approvals, which can significantly boost lending revenue. By efficiently distinguishing good applicants, LGBM helps financial institutions maximize business growth while maintaining prudent risk management.

Additionally, LGBM demonstrates a relatively low false positive rate of 41%, indicating it effectively minimizes the rejection of valid loan applications. Avoiding false rejections is critical not only for protecting potential revenue but also for preserving customer satisfaction and brand reputation. Customers who are wrongly denied loans may seek alternatives or lose trust in the lender, so reducing this risk helps retain market share and fosters long-term client relationships.

![Confusion Matrix Figure](fig2.png)

Random Forest (RF) performs robustly, with a true positive rate of 82%, slightly behind LGBM. While RF remains a strong contender for loan approval tasks, its marginally lower ability to approve valid loans means it might miss some genuine applicants. This can lead to a small but notable reduction in revenue opportunities. However, RF still offers a good balance between precision and recall, supporting consistent decision-making in credit approvals and rejections.

Compared to LGBM and RF, XGBoost delivers the weakest performance, with the lowest true positive rate and the highest rate of missed valid loan approvals. This results in a higher number of incorrectly rejected deserving applicants, which can severely limit lending volume and harm customer satisfaction. Such underperformance highlights the need for further tuning or reconsideration before XGBoost is deployed in production credit scoring systems.

Overall, the performance gap suggests that lenders aiming for optimal business outcomes should prioritize deploying or further refining LGBM and RF models. Leveraging these algorithms can improve the accuracy of loan approvals and rejections, reduce operational risks, and enhance customer trust‚Äîcritical factors for sustaining competitiveness and profitability in the evolving financial services landscape.

---

## üí∞ Cost Evaluation

Accurately evaluating the financial impact of machine learning models in lending automation is critical for demonstrating their business value. This analysis quantifies the costs associated with incorrect loan decisions‚Äînamely, the costs from incorrect acceptance (false positives) and incorrect rejection (false negatives)‚Äîand compares these against the baseline labor costs of manual loan processing. By converting these error costs into percentages of total labor expenditure, the evaluation provides a clear, relatable metric for stakeholders to understand the tangible savings generated by automation.

The model assessments were performed using test data, simulating real-world scenarios where true borrower risk is unknown during application review. This approach validates how each model‚Äôs predictive accuracy translates into operational cost implications in practice, rather than relying on training data results that may overestimate performance. Such rigor ensures that the cost savings reported reflect realistic expectations of machine learning deployment in financial institutions.

The breakdown of costs reveals that incorrect loan acceptance accounts for a significantly higher proportion of total labor costs, ranging between 49% to 54%. This aligns with the confusion matrix findings where false positives (incorrect approvals) occur more frequently than false negatives (incorrect rejections). Given the higher financial risk of wrongly approving bad loans, this insight highlights a critical area for further refinement. Focused efforts in improving data quality, feature engineering, and algorithm tuning will be vital to mitigate these costly errors.
![Cost Evaluation Figure](cost.png)
Conversely, the cost contribution from incorrect rejections is lower, representing about 28% to 31% of total labor costs. Although these errors reduce potential revenue by denying creditworthy applicants, their financial impact is somewhat less immediate compared to losses from defaults. However, minimizing false rejections remains strategically important for customer satisfaction, brand loyalty, and long-term market competitiveness, emphasizing the need for balanced model optimization.

Overall, the three machine learning models demonstrate notable cost-saving potential compared to traditional manual processing. The estimated labor cost reductions range from 14% to 23%, showcasing the operational efficiency gains that automation can deliver. Notably, the LGBM model achieves the highest savings at 23%, reaffirming its superior balance of predictive accuracy and risk management seen in earlier evaluations.

These cost savings not only reduce operational expenditure but also free up human resources for higher-value tasks, enhance decision-making speed, and support scalable loan processing volumes. Implementing models like LGBM enables financial institutions to streamline their credit risk workflows, reduce financial losses from loan defaults, and improve overall profitability in a competitive lending market.

---

## ‚öñÔ∏è Class Weights and Optimal Probability Thresholds

Having identified the LGBM model as the best performer among those tested, we further explored how adjusting its probability decision threshold affects classification outcomes under different class weight settings. Class weights in tree-based models like LGBM influence how the model balances the importance of different classes‚Äîin this case, approved versus rejected loan applications. By varying these weights from the default balanced setting of {0: 1, 1: 1}, we can fine-tune the model‚Äôs sensitivity toward minimizing either false approvals or false rejections based on business priorities.

The analysis reveals that both the optimal probability decision threshold and the resulting accuracy of the LGBM model are highly dependent on the chosen class weights. This implies that lending institutions can strategically adjust these parameters to align the model‚Äôs decision boundary with their risk appetite and operational goals. For example, increasing the weight on the rejection class may reduce risky loan approvals but could also increase false rejections, impacting customer experience. Hence, this flexibility enables a tailored approach to risk management and cost optimization in automated lending decisions.


<img src="cw.png" alt="Cost Evaluation" width="750">

For this specific credit scoring problem, the optimal accuracy was achieved when the class weights were set to `{0: 1, 1: 2}`, meaning the model places twice as much importance on correctly identifying the positive class (loan rejections) relative to the negative class (loan approvals). At this weighting, the probability decision threshold that maximizes model performance is approximately **0.6435**. This adjustment reflects a strategic business choice to prioritize reducing false negatives‚Äîi.e., minimizing the risk of incorrectly approving high-risk loan applicants. By doing so, financial institutions can more effectively mitigate potential losses from defaults, strengthening overall risk management while maintaining a balanced level of loan approvals to support business growth.

<img src="fig3.png" alt="Cost Evaluation" width="750">

The accompanying figure illustrates how the probability decision threshold varies as the difference between class weights changes. Notably, when the class weights are more balanced (closer in value), the model requires a lower threshold to reach high accuracy, meaning it is more lenient in approving loans. Conversely, as the disparity between the class weights increases, the threshold rises, reflecting a more conservative approval stance. This relationship highlights the flexibility lenders have to calibrate their risk tolerance and operational priorities through class weighting and threshold selection, tailoring the automated decision-making process to optimize the trade-off between maximizing revenue and minimizing credit risk.

---

## üèÅ Conclusion and Recommendations

### Conclusion

This study demonstrates that integrating machine learning models, particularly the Light Gradient Boosting Machine (LGBM) Classifier, can significantly transform the loan approval process by enhancing both efficiency and predictive accuracy. The LGBM model outperformed Random Forest and XGBoost by delivering the best balance between precision and recall, which is critical for minimizing both false approvals and false rejections. This balance directly translates into improved risk management and better customer experience, as valid applicants are approved more reliably while high-risk applications are effectively filtered out.

Automating lending decisions with machine learning also offers substantial cost benefits. Our analysis shows that using LGBM can reduce labor-related processing costs by up to 23%, providing a scalable solution that can handle increasing application volumes without proportional increases in staffing. This cost saving not only boosts profitability but also accelerates turnaround times, allowing financial institutions to remain competitive in a fast-paced market where customer expectations for swift service are rising.

The study further highlights the importance of tuning model parameters and decision thresholds to align with business priorities. Adjusting class weights in the LGBM model to {0: 1, 1: 2} ‚Äî effectively weighting loan rejection errors more heavily ‚Äî resulted in optimal accuracy, reflecting a prudent risk management approach. This ability to calibrate the model provides lenders with the flexibility to tailor their decision systems according to risk appetite and regulatory requirements.

### Recommendations

Based on the findings, it is highly recommended to adopt the LGBM Classifier as the primary engine for automated loan decision-making. Its superior accuracy, efficiency, and cost-saving potential make it the most effective choice among the evaluated models. Implementing LGBM can streamline loan processing workflows, reduce dependency on manual labor, and ultimately enhance the institution‚Äôs operational resilience.

To maximize these benefits, continuous efforts should be made to optimize model performance through advanced data engineering practices and regular retraining on fresh data. Reducing incorrect approvals and rejections further mitigates financial risks and protects brand reputation by maintaining customer trust.

Finally, while XGBoost showed comparatively lower performance in this evaluation, it remains a valuable candidate for future exploration. Additional hyperparameter tuning, feature engineering, or even hybrid ensemble approaches could improve its predictive power. Exploring alternative models and incorporating business feedback loops will ensure the automated lending system remains adaptive and robust in evolving market conditions.

## üì¨ Contact

For further questions or collaborations, feel free to reach out:

**Vinylango25**  
[GitHub Profile](https://github.com/Vinylango25)



# Customer_Churn_Analysis_And_Prediction


# Customer Survival Analysis and Churn Prediction

Customer attrition, also known as customer churn, customer turnover, or customer defection, is the loss of clients or customers.

Telephone service companies, Internet service providers, pay TV companies, insurance firms, and alarm monitoring services, often use customer attrition analysis and customer attrition rates as one of their key business metrics because the cost of retaining an existing customer is far less than acquiring a new one. Companies from these sectors often have customer service branches which attempt to win back defecting clients, because recovered long-term customers can be worth much more to a company than newly recruited clients.

Predictive analytics use churn prediction models that predict customer churn by assessing their propensity of risk to churn. Since these models generate a small prioritized list of potential defectors, they are effective at focusing customer retention marketing programs on the subset of the customer base who are most vulnerable to churn.

In this project I aim to perform customer survival analysis and build a model which can predict customer churn. I also aim to build an app which can be used to understand why a specific customer would stop the service and to know his/her expected lifetime value.  


## Project Organization
```
.
‚îú‚îÄ‚îÄ Images/                             : contains images
‚îú‚îÄ‚îÄ static/                             : plots to show gauge chart, hazard and survival curve, shap values in Flask App 
‚îÇ   ‚îî‚îÄ‚îÄ images/
‚îÇ       ‚îú‚îÄ‚îÄ hazard.png
‚îÇ       ‚îú‚îÄ‚îÄ surv.png
‚îÇ       ‚îú‚îÄ‚îÄ shap.png
‚îÇ       ‚îî‚îÄ‚îÄ new_plot.png
‚îú‚îÄ‚îÄ templates/                          : contains html template for flask app
‚îÇ   ‚îî‚îÄ‚îÄ index.html
‚îú‚îÄ‚îÄ Customer Survival Analysis.ipynb    : Survival Analysis kaplan-Meier curve, log-rank test and Cox-proportional Hazard model
‚îú‚îÄ‚îÄ Exploratory Data Analysis.ipynb     : Data Analysis to understand customer data
‚îú‚îÄ‚îÄ Churn Prediction Model.ipynb        : Random Forest model to predict customer churn
‚îú‚îÄ‚îÄ app.py                              : Flask App
‚îú‚îÄ‚îÄ app-pic.png                         : Final App image  
‚îú‚îÄ‚îÄ explainer.bz2                       : Shap Explainer
‚îú‚îÄ‚îÄ model.pkl                           : Random Forest model
‚îú‚îÄ‚îÄ survivemodel.pkl                    : Cox-proportional Hazard model
‚îú‚îÄ‚îÄ requirements.txt                    : requirements to run this model
‚îú‚îÄ‚îÄ Procfile                            : procfile for app deployment
‚îú‚îÄ‚îÄ LICENSE.md                          : MIT License
‚îî‚îÄ‚îÄ README.md                           : Report
```

## Customer Survival Analysis

**Survival Analysis:** 
Survival analysis is generally defined as a set of methods for analyzing data where the outcome variable is the time until the occurrence of an event of interest. The event can be death, occurrence of a disease, marriage, divorce, etc. The time to event or survival time can be measured in days, weeks, years, etc.

For example, if the event of interest is heart attack, then the survival time can be the time in years until a person develops a heart attack.

**Objective:**
The objective of this analysis is to utilize non-parametric and semi-parametric methods of survival analysis to answer the following questions.
- How the likelihood of the customer churn changes over time?
- How we can model the relationship between customer churn, time, and other customer characteristics?
- What are the significant factors that drive customer churn?
- What is the survival and Hazard curve of a specific customer?
- What is the expected lifetime value of a customer?

**Kaplan-Meier Survival Curve:**

<p align="center">
<img src="https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/SurvivalCurve.png" width="400" height="300">
</p>

From above graph, we can say that
- AS expected, for telcom, churn is relatively low. The company was able to retain more than 60% of its customers even after 72 months.
- There is a constant decrease in survival probability probability between 3-60 months.
- After 60 months or 5 years, survival probability decreases with a higher rate. 

**Log-Rank Test:** 

Log-rank test is carried out to analyze churning probabilities group wise and to find if there is statistical significance between groups. The plots show survival curve group wise.

<p align="center">
<img src="https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/gender.png" width="250" height="200"/> 
<img src="https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/Senior%20Citizen.png" width="250" height="200"/>
<img src="https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/partner_1.png" width="250" height="200"/> 
</p>

<p align="center">
<img src="https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/dependents.png" width="250" height="200"/> 
<img src="https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/phoneservice.png" width="250" height="200"/>
<img src="https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/MultipleLines.png" width="250" height="200"/> 
</p>

<p align="center">
<img src="https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/InternetService.png" width="250" height="200"/> 
<img src="https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/OnlineSecurity.png" width="250" height="200"/> 
<img src="https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/OnlineBackup.png" width="250" height="200"/> 
</p>

<p align="center">
<img src="https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/DeviceProtection.png" width="250" height="200"/> 
<img src="https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/TechSupport.png" width="250" height="200"/>
<img src="https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/Contract.png" width="250" height="200"/> 
</p>

<p align="center">
<img src="https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/StreamingMovies.png" width="250" height="200"/>
<img src="https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/paymentmethod.png" width="250" height="200"/> 
<img src="https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/PaperlessBilling.png" width="250" height="200"/>
</p>

From above graphs we can conclude following:
- Customer's Gender and the phone service type are not indictive features and their p value of log rank test is above threshold value 0.05.
- If customer is young and has a family, he or she is less likely to churn. The reason might be the busy life, more money or another factors.
- If customer is not enrolled in services like online backup, online security, device protection, tech support, streaming Tv and streaming movies even though having active internet service, the survival probability is less.
- The company should traget customers who opt for internet service as their survival probability constantly descreases. Also, Fiber Optilc type of Internet Service is costly and fast compared to DSL and this might be the reason of higher customer churning. 
- More offers should be given to customers who opt for month-to-month contract and company should target customers to subscribe for long-term service. 
- If customer's paying method is automatic, he or she is less likely to churn. The reason is in the case of electronic check and mailed check, a customer has to make an effort to pay and it takes time.

**Survival Regression:**
I use cox-proportional hazard model to perform survival regression analysis on customer data. This model is used to relate several risk factors or exposures simultaneously to survival time. In a Cox proportional hazards regression model, the measure of effect is the hazard rate, which is the risk or probability of suffering the event of interest given that the participant has survived up to a specific time. The model fits the data well and the coefficients are shown below.

<p align="center">
<img src="https://github.com/archd3sai/Customer-Survival-Analysis-and-Churn-Prediction/blob/master/Images/Survival-analysis.png" width="750" height="500"/>
</p>

Using this model we can calculate the survival curve and hazard curve of any customer as shown below. These plots are useful to know the remaining life of a customer. 

<p align="center">
<img src="https://github.com/archd3sai/Customer-Survival-Analysis-and-Churn-Prediction/blob/master/Images/survival.png" width="400" height="300"/>
<img src="https://github.com/archd3sai/Customer-Survival-Analysis-and-Churn-Prediction/blob/master/Images/hazard.png" width="400" height="300"/>
</p>

**Customer Lifetime Value:**

To calculate customer lifetime value, I would multiply the Monthly charges the customer is paying to Telcom and the expected life time of the customer.

I utilize the survival function of a customer to calculate its expected life time. I would like to be little bit conservative and consider the customer is churned when the survival probability of him is 10%.

## Customer Churn Prediction
I aim to implement a machine learning model to accurately predict if the customer will churn or not.

### Analysis

**Churn and Tenure Relationship:**

<p align="center">
<img src="https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/tenure-churn.png" width="600" height="300"/>
</p>

- As we can see the higher the tenure, the lesser the churn rate. This tells us that the customer becomes loyal with the tenure.

<br />

**Tenure Distrbution by Various Services:**

<p align="center">
<img src="https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/tenure-dist.png" width="340" height="250"/>
</p>

- When the customers are new they do not opt for various services and their churning rate is very high. This can be seen in above plot for Streaming Movies and this holds true for all various services.

<br />

**Internet Service By Contract Type:**

<p align="center">
<img src="https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/internetservice-contract.png" width="360" height="250"/>
</p>

- Many of the people of who opt for month-to-month Contract choose Fiber optic as Internet service and this is the reason for higher churn rate for fiber optic Internet service type.

<br />

**Payment method By Contract Type:**

<p align="center">
<img src="https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/payment-contract.png" width="500" height="250"/>
</p>

- People having month-to-month contract prefer paying by Electronic Check mostly or mailed check. The reason might be short subscription cancellation process compared to automatic payment.

<br />

**Monthly Charges:**

<p align="center">
<img src="https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/monthlycharges.png" width="300" height="220"/>
</p>

- As we can see the customers paying high monthly fees churn more.

<br />

### Modelling

For the modelling, I will use tress based Ensemble method as we do not have linearity in this classification problem. Also, we have a class imbalance of 1:3 and to combat it I will assign class weightage of 1:3 which means false negatives are 3 times costlier than false positives. I built a model on 80% of data and validated model on remaining 20% of data keeping in mind that I do not have data leakage. The random forest model has many hyperparameters and I tuned them using Grid Search Cross Validation while making sure that I do not overfit.

The final model resulted in 0.62 F1 score and 0.85 ROC-AUC. The resulting plots can be seen below.

<p align="center">
<img src="https://github.com/archd3sai/Customer-Survival-Analysis-and-Churn-Prediction/blob/master/Images/model_1.png" width="600" height="300"/>
<img src="https://github.com/archd3sai/Customer-Survival-Analysis-and-Churn-Prediction/blob/master/Images/model_feat_imp.png" width="600" height="400"/>

</p>

From the feature importance plot, we can see which features govern the customer churn.

### Explainability

We can explain and understand the Random forest model using explainable AI modules such as Permutation Importance, Partial Dependence plots and Shap values.

1. Permutation Importance shows feature importance by randomly shuffling feature values and measuring how much it degrades our performance.

<p align="center">
<img src=https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/eli51.png height=250 width=200>
<img src=https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/eli52.png height=130 width=200> 
</p>

2. Partial dependence plot is used to see how churning probability changes across the range of particular feature. For example, in below graph of tenure group, the churn probability decreases at a higher rate if a person is in tenure group 2 compared to 1.

<p align="center">
<img src=https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/pdp_tenure.png height=250 width=400>
<img src=https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/pdp_contract.png height=250 width=400> 
</p>

<p align="center">
<img src=https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/pdp_monthly_charges.png height=250 width=400>
<img src=https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/pdp_total_charges.png height=250 width=400> 
</p>

3. Shap values (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model. In below plot we can see that why a particual customer's churning probability is less than baseline value and which features are causing them.

![](https://github.com/archd3sai/Customer-Churn-Analysis-and-Prediction/blob/master/Images/shap.png)

## Flask App

I saved the final tuned Random Forest model and deployed it using Flask web app. Flask is a micro web framework written in Python.  It is designed to make getting started quick and easy, with the ability to scale up to complex applications. I saved the shap value explainer tuned using random forest model to show shap plots in app. I have also utilized the cox-proportional hazard model to show survival curve and hazard curve, and to calculate expected customer lifetime value. 

The final app shows churning probability, gauge chart of how severe a customer is and shap values based on customer's data. The final app layout can be seen above.  






# Healthcare-Accessibility-in-Nairobi


# Enhancing Healthcare Accessibility in Nairobi, Kenya

> This project presents a comprehensive, data-driven narrative that delves into the current state of Nairobi‚Äôs healthcare infrastructure. By leveraging multiple datasets, including population demographics and health facility distributions, it evaluates how well the city is positioned to meet the healthcare needs of its diverse population. The analysis is anchored within the framework of the United Nations Sustainable Development Goal 3, which aims to ensure healthy lives and promote well-being for all at all ages. Through detailed examination of healthcare accessibility, service availability, and facility operational hours, this study identifies critical gaps and opportunities for improvement. Ultimately, the findings serve as a foundation for informed policy-making and strategic interventions designed to advance Nairobi‚Äôs journey towards achieving SDG 3.



## Introduction

# Enhancing Healthcare Accessibility in Nairobi, Kenya

## Introduction

In 2015, the United Nations set forth a bold and transformative vision with the launch of the Sustainable Development Goals (SDGs), a global agenda designed to address some of the world‚Äôs most pressing challenges. These 17 goals encompass a wide range of issues, from eradicating poverty and hunger to combating climate change and ensuring access to clean water. Among these, **SDG 3: Good Health and Wellbeing** is particularly significant as it serves as a foundational pillar that supports the achievement of many other goals. Good health is intrinsically linked to social and economic development, influencing factors such as workforce productivity, educational outcomes, and societal stability. The goal‚Äôs comprehensive scope‚Äîfrom reducing maternal and child mortality to combating epidemics and promoting mental health‚Äîreflects the critical role health plays in building sustainable and resilient communities worldwide.

In Kenya, the national response to this global agenda is embedded within its **Vision 2030** framework, a blueprint aimed at transforming the country into a middle-income economy with improved standards of living for all citizens. Central to Vision 2030 is the commitment to develop an inclusive and efficient healthcare system that provides equitable access to quality services. As the country‚Äôs capital and largest urban center, Nairobi holds a unique position in this vision. It acts as the economic heartbeat of Kenya and a melting pot of diverse populations, including some of the most vulnerable groups such as residents of informal settlements. Nairobi‚Äôs healthcare infrastructure and policies must therefore reflect the complexities of urban health challenges while pioneering innovations that can be scaled nationwide.

Over the past decade, Kenya has made commendable strides in enhancing its healthcare system through legislative reforms and institutional developments. The establishment of the **Social Health Authority (SHA)**, for instance, marks a critical step towards achieving Universal Health Coverage (UHC), aiming to reduce financial barriers and improve service quality. Despite these advancements, Nairobi continues to face significant disparities in healthcare accessibility and quality. Challenges such as uneven distribution of facilities, limited operational hours, and inadequate availability of specialized services disproportionately affect low-income and high-density areas. These gaps highlight the urgent need for data-driven approaches to better understand the dynamics at play and to inform targeted interventions.

This report harnesses a rich dataset that includes demographic statistics, health facility locations, and service availability across Nairobi‚Äôs sub-counties to provide an in-depth assessment of the city‚Äôs healthcare landscape. By integrating quantitative analysis with contextual understanding, the study not only uncovers critical insights into who is served and who remains underserved but also examines the operational challenges faced by healthcare providers. The results reveal a nuanced picture of progress and persistent barriers, drawing attention to areas requiring urgent attention such as maternal health, child healthcare, and 24-hour service availability in a city that never sleeps.

Building on these findings, the report offers actionable recommendations aligned with the specific targets of **SDG 3**. These include strategies to expand healthcare infrastructure, diversify services to meet a broad range of health needs, and enhance accessibility for vulnerable populations. The ultimate goal is to support policymakers, health practitioners, and stakeholders in Nairobi to make informed decisions that will accelerate progress toward good health and well-being for all residents. This data-driven approach is not only crucial for Nairobi‚Äôs local development but also serves as a model for other rapidly growing urban centers facing similar health challenges across the globe.

---

## üõ†Ô∏è Tools and Technologies Used

This project was powered by a suite of modern data analysis tools and open-source technologies, enabling detailed exploration, visualization, and storytelling with data:

- **Python**: Core programming language for data preprocessing, analysis, and visualization.
- **Pandas & NumPy**: Efficient data structures and operations for cleaning, transforming, and manipulating healthcare and demographic datasets.
- **Matplotlib & Seaborn**: Static plotting libraries used for bar charts, line plots, and categorical summaries.
- **Plotly & GeoPandas**: Interactive mapping and choropleth tools for spatial exploration of healthcare availability across Nairobi‚Äôs sub-counties.
- **Jupyter Notebook**: A dynamic environment for exploratory data analysis, code execution, and inline visualization.
- **Markdown & LaTeX**: Used to create structured documentation, section headers, annotated captions, and math/statistics notation.

These tools together enabled the creation of a reproducible, transparent, and visually compelling data story that can be shared, extended, or integrated into policy briefs and urban health dashboards.

## Nairobi's Demographic Landscape

The first step in enhancing healthcare is gaining a deep understanding of the population it aims to serve. Effective health planning and resource allocation require accurate and comprehensive demographic data to identify the needs and characteristics of the community. In Nairobi, where the population is diverse and rapidly growing, such insights are especially critical. By analyzing demographic profiles, health authorities can tailor interventions that address the unique health challenges faced by different groups, ensuring that no segment of the population is left behind.

According to the **2019 Kenya National Population Census**, Nairobi‚Äôs population stands at **4.397 million** people. This population is almost evenly divided by gender, with **49% male** and **51% female** residents. Understanding this near-equal gender distribution is essential, as it informs health service providers about potential differences in healthcare needs, utilization patterns, and risk factors between males and females. Such demographic knowledge lays the foundation for developing inclusive health strategies that promote equitable access and improved outcomes for all Nairobi residents.


![Figure 1: Age distribution](figures/fig1.png)

Figure 1 reveals a striking demographic feature of Nairobi‚Äôs population: a pronounced youth bulge. More than **70%** of Nairobi‚Äôs residents are under the age of 35, highlighting a predominantly young population that shapes the city‚Äôs social and economic fabric. This youthful majority represents a vital segment of the population that has the potential to drive innovation, productivity, and development if their health and wellbeing are adequately supported. At the same time, this demographic trend brings unique challenges, as young people often face specific health risks and require targeted services to address their needs effectively.

Among this youthful population, children under the age of five constitute approximately **12%**, signaling the critical importance of early childhood healthcare. This group is particularly vulnerable to preventable diseases such as pneumonia, diarrhea, and malaria, which remain leading causes of child mortality in many low- and middle-income countries. The high proportion of young children necessitates robust maternal and pediatric healthcare systems that ensure timely access to essential services such as immunization, nutrition programs, and growth monitoring to reduce infant and child mortality rates and promote healthy development.

Young people, broadly defined as those aged 15 to 35, face a distinct set of health challenges that extend beyond childhood illnesses. Sexual and reproductive health services become crucial during adolescence and early adulthood, as young people navigate issues related to family planning, sexually transmitted infections, and adolescent pregnancy. Mental health also emerges as a pressing concern, with rising cases of depression, anxiety, and substance abuse among youth worldwide, including Nairobi. Preventive and counseling services, as well as community-based interventions, are essential to address these complex needs.

Given this demographic landscape, healthcare interventions in Nairobi must prioritize youth and child-centric services to be effective. This includes expanding immunization programs, enhancing antenatal and postnatal care, and integrating mental health counseling into primary healthcare. School-based health education programs offer a strategic platform to promote healthy behaviors, raise awareness, and reduce risk factors among children and adolescents. By focusing on these critical services, Nairobi can leverage its youthful population as a powerful driver of sustainable development aligned with SDG Goal 3.


![Figure 2: Population density by sub-county](figures/fig2.png)

Beyond age, population density in Nairobi varies significantly across its sub-counties, revealing stark contrasts in living conditions and health challenges. High-density areas such as **Mathare**, **Kamukunji**, **Makadara**, and **Kibra** are characterized by densely populated informal settlements where thousands of residents live in cramped spaces. These neighborhoods often lack basic amenities like clean water, sanitation facilities, and waste management systems, creating an environment ripe for the spread of infectious diseases such as cholera, typhoid, and respiratory infections. The overcrowding not only exacerbates health risks but also complicates efforts to provide timely and effective healthcare.

The concentration of people in these informal settlements places immense pressure on the existing healthcare infrastructure, which is often under-resourced and ill-equipped to handle the volume and complexity of health needs. Public health systems in high-density areas face challenges such as long patient queues, inadequate medical supplies, and a shortage of trained healthcare professionals. These limitations hinder the delivery of essential services including maternal and child health, immunizations, and treatment for chronic diseases, disproportionately affecting the most vulnerable populations.

Addressing these challenges requires innovative and decentralized healthcare approaches that bring services closer to the community level. Community health workers, mobile clinics, and local health outreaches have emerged as vital strategies to overcome barriers related to accessibility and mobility within crowded urban environments. By empowering communities and enhancing local healthcare capacity, Nairobi can mitigate the adverse effects of population density on health outcomes and move closer to achieving the targets outlined in SDG 3 for Good Health and Wellbeing.

---

## Distribution of Healthcare Facilities

While Nairobi hosts a considerable number of health facilities, their distribution across the city does not adequately reflect the varying healthcare needs of its diverse population. Many health centers and hospitals are concentrated in relatively affluent or centrally located sub-counties, leaving densely populated and low-income areas underserved. This imbalance means that residents in high-need regions such as informal settlements often face longer travel distances, increased waiting times, and limited access to specialized medical services. The disparity in facility distribution highlights systemic inequalities that undermine efforts to provide equitable healthcare access to all Nairobi residents.

Moreover, the mismatch between healthcare infrastructure and population demand exacerbates the strain on existing facilities, leading to overcrowded clinics and stretched resources in areas where services are available. This overburdening compromises the quality of care, reduces patient satisfaction, and can lead to adverse health outcomes, particularly for vulnerable groups like children, the elderly, and people with chronic illnesses. Addressing this issue calls for strategic investment in healthcare infrastructure, focusing on building new facilities and upgrading existing ones in underserved sub-counties, as well as optimizing resource allocation to ensure efficient service delivery aligned with population needs.

![Figure 3: Health facilities per sub-county and healthcare density](figures/fig3.png)

Sub-counties like **Embakasi** and **Starehe** stand out as having the highest number of health facilities within Nairobi, suggesting at first glance a robust healthcare infrastructure. However, these raw numbers can be deceptive when not considered alongside population size and distribution. **Embakasi**, for instance, despite boasting numerous clinics, hospitals, and dispensaries, is also one of the most populous sub-counties in Nairobi. This vast population base dilutes the impact of facility numbers, resulting in a **low healthcare facility density** ‚Äî a critical measure indicating how many facilities serve each segment of the population. Consequently, residents may face overcrowded facilities and longer wait times, reducing the overall effectiveness of healthcare delivery despite facility availability.

Conversely, sub-counties such as **Mathare** and **Kasarani** suffer from a dual challenge: they not only have fewer health facilities overall but also experience low facility density relative to their population size. This compounded deficit significantly exacerbates the healthcare gap in these areas, especially considering that Mathare is home to some of Nairobi‚Äôs largest informal settlements. The scarcity of health facilities forces many residents to seek care far from home or forego treatment altogether, contributing to worsening health outcomes and perpetuating cycles of poverty and ill health. This shortage highlights the urgent need for targeted investment and strategic planning to expand healthcare infrastructure in underserved neighborhoods.

Beyond the mere presence of healthcare facilities, accessibility must also account for geographic and financial barriers faced by Nairobi‚Äôs residents. For many living in informal settlements, the closest health center might be several kilometers away, requiring hours of walking or costly transport that many cannot afford. This physical distance, combined with the financial strain of medical fees and transportation costs, often results in delayed or missed healthcare visits. Therefore, improving healthcare accessibility demands a comprehensive approach that goes beyond increasing the number of facilities to include enhancing transportation options, subsidizing care costs, and decentralizing services closer to communities in need. Only by addressing these multifaceted challenges can Nairobi move closer to achieving equitable healthcare for all its citizens.

---

## Facility Operational Hours and Service Availability

Availability of healthcare services is not solely determined by the number or location of health facilities; operational hours play a crucial role in defining when and how people can access care. For many Nairobi residents, especially those working irregular hours or multiple jobs, clinics and hospitals that operate only during standard business hours fail to meet their needs. This limited availability can result in delayed treatment, worsening of illnesses, and increased reliance on emergency services for conditions that could have been managed earlier with timely care. Furthermore, extended or after-hours services are often concentrated in private facilities, which may be prohibitively expensive for the majority of Nairobi‚Äôs population. This mismatch between health service availability and community needs creates significant barriers to effective healthcare delivery.

Moreover, in densely populated and underserved areas, the scarcity of 24/7 or weekend healthcare options compounds accessibility issues. When primary healthcare centers close after hours, patients are forced to travel longer distances to reach hospitals with emergency services, incurring additional time and costs that many cannot bear. This gap disproportionately affects vulnerable populations such as the elderly, children, and those with chronic conditions who require regular or urgent care outside typical hours. To address these challenges, Nairobi‚Äôs health system must prioritize extending operational hours and implementing flexible service models, including mobile clinics and telemedicine, that can reach people beyond the confines of traditional schedules. Doing so will be instrumental in closing service gaps and improving health outcomes in line with the goals of SDG 3.

![Figure 4: Healthcare facility operational status](figures/fig4.png)

Though it is encouraging that more than half of Nairobi‚Äôs health facilities remain open during weekends, the stark reality is that fewer than 30% offer round-the-clock, 24-hour services. This limitation disproportionately affects critical sub-counties like Starehe, which encompasses the bustling Nairobi Central Business District (CBD). In this area, a significant portion of the population works outside typical 9-to-5 hours, including night shifts, part-time jobs, and informal sector employment. For these residents, access to healthcare during non-traditional hours is essential but often unattainable, creating a critical gap in the health system‚Äôs ability to serve its people effectively.

The lack of adequate 24/7 healthcare services has severe consequences. Patients with urgent medical needs, such as mothers in labor, trauma victims, or those experiencing acute illnesses, face delays that can worsen their conditions or even result in fatalities. Emergency rooms become overcrowded during off-hours as patients seek care in hospitals that are open, increasing wait times and straining limited resources. This situation not only endangers patients but also puts immense pressure on healthcare workers and infrastructure, reducing the overall quality and efficiency of care. In turn, this affects public trust in the healthcare system and undermines efforts to improve population health outcomes.

Extending operational hours of health facilities is therefore not simply a matter of convenience or customer service‚Äîit is a critical intervention that can save lives and improve the resilience of Nairobi‚Äôs health system. To truly align with SDG 3‚Äôs vision of ensuring healthy lives and promoting well-being for all, policymakers and healthcare providers must invest in expanding 24-hour service availability, particularly in high-need areas like Starehe. Innovative approaches such as staggered shifts for health workers, mobile emergency units, and partnerships with private sector clinics could help bridge this gap. Ultimately, enhancing access to timely care regardless of the hour is a foundational step toward equitable, effective, and compassionate healthcare for Nairobi‚Äôs diverse population.

---

## Comprehensive Service Coverage

Availability of essential health services forms the backbone of a resilient and effective healthcare system. Without reliable access to key interventions, even the most well-equipped facilities fall short in delivering comprehensive care to the communities they serve. To assess Nairobi‚Äôs readiness in this regard, we analyzed the extent to which health centers across the city provide a suite of vital services that address some of the most pressing health challenges.

Our focus was on critical programs such as **Antiretroviral Therapy (ART)**, which is essential for managing HIV/AIDS ‚Äî a major public health concern in Kenya. Equally important is **Antenatal Care (ANC)**, ensuring pregnant women receive the monitoring and support necessary to promote safe pregnancies and deliveries. The **Prevention of Mother-to-Child Transmission (PMTCT)** of HIV is closely linked, aimed at reducing the risk of HIV transmission during pregnancy, childbirth, or breastfeeding, thereby safeguarding the next generation.

We also examined the availability of **Tuberculosis (TB) Diagnostics**, as TB remains a leading cause of morbidity and mortality in the region. Early and accurate diagnosis is fundamental to controlling this infectious disease. The **Expanded Program on Immunization (EPI)** was another focal point, highlighting Nairobi‚Äôs efforts to protect children from vaccine-preventable diseases through systematic immunization schedules. Lastly, we evaluated the provision of both **Basic and Comprehensive Emergency Obstetric Care**, which are indispensable for addressing complications during childbirth, reducing maternal and neonatal mortality rates.

Together, these services paint a holistic picture of Nairobi‚Äôs healthcare capacity to meet both routine and emergency needs. Their availability, distribution, and operational status directly influence the city‚Äôs progress toward achieving Sustainable Development Goal 3, emphasizing good health and well-being for all.
![Figure 5: Availability of healthcare services](figures/fig5.png)

While ART and Family Planning services are moderately distributed, many critical services are almost absent. There are sub-counties with no **ANC**, **PMTCT**, or **obstetric emergency services**. The lack of **TB diagnostics**, **X-ray** facilities, and immunization infrastructure puts the city at risk of disease outbreaks and worsens chronic health disparities.

These service gaps directly compromise Kenya‚Äôs ability to meet:
- **SDG 3.1**: Reduce maternal mortality
- **SDG 3.2**: End preventable child deaths
- **SDG 3.3**: Combat HIV/AIDS, TB, and other diseases
- **SDG 3.4**: Reduce premature deaths from NCDs

---

## Pediatric Care: C-IMCI Accessibility

![Figure 6: Children's accessibility to C-IMCI services](figures/fig6.png)

**C-IMCI (Community Integrated Management of Childhood Illnesses)** is a well-established and effective strategy designed to reduce child mortality by improving the diagnosis and treatment of the most common and life-threatening childhood illnesses. These include pneumonia, diarrhea, malaria, and malnutrition, which together account for a large proportion of deaths among children under five. By empowering community health workers with training and resources to manage these conditions promptly and effectively, C-IMCI extends critical healthcare services beyond formal facilities and into the communities where children live. This approach not only improves survival rates but also strengthens overall health system resilience at the grassroots level.

In Nairobi, access to C-IMCI services varies widely between sub-counties. Areas like **Lang‚Äôata** demonstrate relatively better coverage, where community outreach and facility-based programs collaborate effectively to reach vulnerable children. In contrast, high-density informal settlements such as **Mathare** and **Kasarani** show significant gaps in service availability. This disparity poses a serious concern, as children living in underserved areas are at greater risk of preventable illnesses and death. The uneven distribution of C-IMCI highlights the urgent need to address healthcare inequities and ensure that lifesaving interventions reach all children regardless of their geographic or socio-economic status.

Given that children under the age of five make up approximately **12% of Nairobi‚Äôs population**, prioritizing child survival strategies like C-IMCI is critical for the city‚Äôs public health agenda. Expanding the coverage and quality of C-IMCI services will contribute directly to reducing child mortality rates and achieving **Sustainable Development Goal 3.2**, which aims to end preventable deaths of newborns and children under five by 2030. Strengthening these efforts not only saves lives but also supports broader goals of health equity, social justice, and sustainable development across Nairobi.

## Conclusion and Key Recommendations

This data-driven analysis reveals a sobering but actionable reality: while Nairobi boasts substantial healthcare infrastructure in select sub-counties like **Westlands**, **Starehe**, and **Lang‚Äôata**, many areas remain critically underserved. **Mathare**, **Embakasi**, and **Kasarani**, characterized by high population densities and informal settlements, exhibit significant gaps in healthcare availability, operational hours, and essential service provision. If Nairobi is to lead Kenya in achieving **Sustainable Development Goal 3: Good Health and Wellbeing**, targeted interventions must bridge these divides with urgency and precision.

A responsive healthcare system must reflect the demographic and geographic needs of its population. The high proportion of youth and children, coupled with the reality of urban poverty and informal living conditions, demands a holistic, inclusive, and forward-thinking strategy. The following recommendations are grounded in the data and aligned with the SDG 3 targets to ensure Nairobi advances toward universal health coverage, reduced mortality, and equitable access to care.

### Key Recommendations

1. **Design Youth-Oriented Health Programs**  
   With over 70% of Nairobi‚Äôs population under 35, there is an urgent need for youth-centric health services. These should include accessible mental health counseling, comprehensive sexual and reproductive health education, and regular screenings for lifestyle diseases. Equipping youth with the tools and knowledge to manage their health promotes long-term societal wellbeing and productivity.

2. **Prioritize Informal Settlements**  
   Sub-counties like Mathare and Kibra, home to thousands in low-income communities, must receive focused attention. Expanding mobile clinics, deploying community health volunteers, and investing in localized health education campaigns can dramatically improve health outcomes in these vulnerable zones.

3. **Establish 24-Hour Health Centers**  
   Nairobi‚Äôs identity as a 24-hour economy requires a healthcare system that matches its rhythm. Emergency obstetric care, accident response, and night-time healthcare services must be available round-the-clock, especially in areas with high traffic, crime, or industrial activity like Starehe and Embakasi.

4. **Ensure Service Completeness**  
   Healthcare is not just about buildings, but about the services they offer. Facilities must be equipped with comprehensive services including antenatal care, immunizations, diagnostic testing (like TB and HIV), and emergency obstetrics. This ensures continuity of care and reduces the need for referrals, which often delays treatment.

5. **Enhance Pediatric Coverage**  
   With 12% of the population under five years of age, the city must invest heavily in child-specific health programs. Expanding the reach and quality of **C-IMCI** services, along with vaccination campaigns and nutritional support, is critical in lowering child mortality and ensuring a healthy next generation.

6. **Public-Private Partnerships (PPP)**  
   The private sector holds untapped potential in bridging healthcare gaps. Strategic collaborations with private hospitals, diagnostic labs, and telemedicine providers can rapidly expand reach and improve quality‚Äîespecially in diagnostic imaging, specialist services, and emergency care infrastructure.

---

## üß† Insights and Implications

This project underscores the urgent need for data-driven policymaking in Nairobi‚Äôs healthcare planning. As the population continues to grow and urbanize, Nairobi faces mounting pressure to deliver equitable, timely, and effective healthcare to all its residents. The disparities uncovered in this analysis are not just statistical‚Äîthey translate to real lives affected by preventable illnesses, delayed treatments, and missed opportunities for early intervention.

By highlighting spatial and service-related inequalities across sub-counties, this work emphasizes the importance of aligning health infrastructure with demographic realities. A significant portion of Nairobi‚Äôs population is young, densely concentrated, and economically vulnerable, necessitating tailored strategies that address their unique health needs. The insights derived from this data can directly inform policy, budget allocation, and public health interventions, ultimately helping to reduce mortality, decongest emergency services, and improve health system resilience.

In an age where data is increasingly pivotal to development, integrating such granular, evidence-based insights into local governance frameworks offers a path forward. The goal is not only to meet Sustainable Development Goal 3 (Good Health and Wellbeing) by 2030 but to build a system that can adapt, evolve, and respond equitably to its population‚Äôs needs far beyond that milestone.

---

## Project Structure

```bash
Healthcare-Accessibility-in-Nairobi/
‚îú‚îÄ‚îÄ data/                # Datasets (Health facilities, Population census)
‚îú‚îÄ‚îÄ figures/             # Figures used in analysis (fig1.png to fig6.png)
‚îú‚îÄ‚îÄ notebooks/           # Jupyter notebooks for full data analysis
‚îú‚îÄ‚îÄ README.md            # Detailed project overview and documentation
‚îî‚îÄ‚îÄ requirements.txt     # Required Python libraries


# Machine_Learning_Loan_Application_Web_App


# Evaluating Machine Learning Algorithms to Build Web-Based Predictive Loan Approval System Based on Credit Score
## Abstract
In the financial sector, the accuracy and efficiency of loan approval processes are critical for minimizing default risk and ensuring that only eligible applicants receive loans. This research project focuses on applying machine learning algorithms to predict loan approval based on credit scores and other relevant factors. The study employed a comprehensive methodology, including data collection, preprocessing, exploratory data analysis, model development, and evaluation. A detailed analysis and comparison of five widely used classification algorithms: Logistic Regression, Extreme Gradient Boosting (XGBoost), K-Nearest Neighbors (KNN), Multilayer Perceptron (MLP) Classifier, and Support Vector Machine (SVM) was conducted. The results demonstrate that Extreme Gradient Boosting outperforms other models' accuracy, precision, recall, F1-score, and the Area Under the Receiver Operating Characteristic Curve (ROC-AUC) score. A web-based application was developed to integrate the best-performing model for real-time loan approval predictions using HTML, CSS, JavaScript, and Flask. In the financial sector, the accuracy and efficiency of loan approval processes are critical for minimizing default risk and ensuring that eligible applicants receive loans. This project highlights the potential of machine learning algorithms in transforming traditional credit scoring methods, addressing limitations of subjective assessments, and improving the overall reliability and accuracy of loan approval systems.
Keywords: machine learning, loan approval, credit score, predictive modeling, web-based application.

## Usability
* Unzip the zipped folder on your machine.
* Import the project in Visual Studio Code.  
* Install the dependencies from requirements.txt
* Run the loanApprovalPrediction-XGB_Model.py to train the model.
* After the training is complete run the app.py to start web application.


>app.py: Main Flask application file.

>loanApprovalPrediction-XGB_Model.py: Contains the XGBoost model code.

>/data: Folder containing data files (loan application data).

>/templates: HTML templates for the web interface.

>/static: CSS and other static files.

>model.pkl: Pre-trained xg boost model.


## Libraries Required:
* Flask==3.0.3
* numpy==1.24.3
* pandas==2.0.1
* scikit-learn==1.2.2
* xgboost==2.1.1



# Next-js-Boilerplate
üöÄüéâüìö Boilerplate and Starter for Next.js 15 with App Router and Page Router support, Tailwind CSS 4 and TypeScript ‚ö°Ô∏è Made with developer experience first: Next.js + TypeScript + ESLint + Prettier + Drizzle ORM + Husky + Lint-Staged + Vitest + Testing Library + Playwright + Storybook + Commitlint + VSCode + Netlify + PostCSS + Tailwind CSS ‚ú®

# Boilerplate and Starter for Next.js 15+, Tailwind CSS 4, and TypeScript.

<p align="center">
  <a href="https://demo.nextjs-boilerplate.com">
    <img
      src="public/assets/images/nextjs-starter-banner.png?raw=true"
      alt="Next js starter banner"
      style="max-width: 100%; height: auto;"
    />
  </a>
</p>

üöÄ Boilerplate and Starter for Next.js with App Router, Tailwind CSS, and TypeScript ‚ö°Ô∏è Prioritizing developer experience first: Next.js, TypeScript, ESLint, Prettier, Lefthook (replacing Husky), Lint-Staged, Vitest (replacing Jest), Testing Library, Playwright, Commitlint, VSCode, Tailwind CSS, Authentication with [Clerk](https://clerk.com?utm_source=github&utm_medium=sponsorship&utm_campaign=nextjs-boilerplate), Database with DrizzleORM (PostgreSQL, SQLite, and MySQL), Error Monitoring with [Sentry](https://sentry.io/for/nextjs/?utm_source=github&utm_medium=paid-community&utm_campaign=general-fy25q1-nextjs&utm_content=github-banner-nextjsboilerplate-logo), Logging with Pino.js and Log Management, Monitoring as Code, Storybook, Multi-language (i18n), AI-powered code reviews with [CodeRabbit](https://www.coderabbit.ai?utm_source=next_js_starter&utm_medium=github&utm_campaign=next_js_starter_oss_2025), Secure with [Arcjet](https://launch.arcjet.com/Q6eLbRE) (Bot detection, Rate limiting, Attack protection, etc.) and more.

Clone this project and use it to create your own Next.js project. You can check out the live demo at [Next.js Boilerplate](https://demo.nextjs-boilerplate.com), which includes a working authentication system.

## Sponsors

<table width="100%">
  <tr height="187px">
    <td align="center" width="33%">
      <a href="https://clerk.com?utm_source=github&utm_medium=sponsorship&utm_campaign=nextjs-boilerplate">
        <picture>
          <source media="(prefers-color-scheme: dark)" srcset="https://github.com/ixartz/SaaS-Boilerplate/assets/1328388/6fb61971-3bf1-4580-98a0-10bd3f1040a2">
          <source media="(prefers-color-scheme: light)" srcset="https://github.com/ixartz/SaaS-Boilerplate/assets/1328388/f80a8bb5-66da-4772-ad36-5fabc5b02c60">
          <img alt="Clerk ‚Äì Authentication & User Management for Next.js" src="https://github.com/ixartz/SaaS-Boilerplate/assets/1328388/f80a8bb5-66da-4772-ad36-5fabc5b02c60">
        </picture>
      </a>
    </td>
    <td align="center" width="33%">
      <a href="https://www.coderabbit.ai?utm_source=next_js_starter&utm_medium=github&utm_campaign=next_js_starter_oss_2025">
        <picture>
          <source media="(prefers-color-scheme: dark)" srcset="public/assets/images/coderabbit-logo-dark.svg?raw=true">
          <source media="(prefers-color-scheme: light)" srcset="public/assets/images/coderabbit-logo-light.svg?raw=true">
          <img alt="CodeRabbit" src="public/assets/images/coderabbit-logo-light.svg?raw=true">
        </picture>
      </a>
    </td>
    <td align="center" width="33%">
      <a href="https://sentry.io/for/nextjs/?utm_source=github&utm_medium=paid-community&utm_campaign=general-fy25q1-nextjs&utm_content=github-banner-nextjsboilerplate-logo">
        <picture>
          <source media="(prefers-color-scheme: dark)" srcset="public/assets/images/sentry-white.png?raw=true">
          <source media="(prefers-color-scheme: light)" srcset="public/assets/images/sentry-dark.png?raw=true">
          <img alt="Sentry" src="public/assets/images/sentry-dark.png?raw=true">
        </picture>
      </a>
      <a href="https://about.codecov.io/codecov-free-trial/?utm_source=github&utm_medium=paid-community&utm_campaign=general-fy25q1-nextjs&utm_content=github-banner-nextjsboilerplate-logo">
        <picture>
          <source media="(prefers-color-scheme: dark)" srcset="public/assets/images/codecov-white.svg?raw=true">
          <source media="(prefers-color-scheme: light)" srcset="public/assets/images/codecov-dark.svg?raw=true">
          <img alt="Codecov" src="public/assets/images/codecov-dark.svg?raw=true">
        </picture>
      </a>
    </td>
  </tr>
  <tr height="187px">
    <td align="center" width="33%">
      <a href="https://launch.arcjet.com/Q6eLbRE">
        <picture>
          <source media="(prefers-color-scheme: dark)" srcset="public/assets/images/arcjet-dark.svg?raw=true">
          <source media="(prefers-color-scheme: light)" srcset="public/assets/images/arcjet-light.svg?raw=true">
          <img alt="Arcjet" src="public/assets/images/arcjet-light.svg?raw=true">
        </picture>
      </a>
    </td>
    <td align="center" width="33%">
      <a href="https://sevalla.com/">
        <picture>
          <source media="(prefers-color-scheme: dark)" srcset="public/assets/images/sevalla-dark.png">
          <source media="(prefers-color-scheme: light)" srcset="public/assets/images/sevalla-light.png">
          <img alt="Sevalla" src="public/assets/images/sevalla-light.png">
        </picture>
      </a>
    </td>
    <td align="center" width="33%">
      <a href="https://l.crowdin.com/next-js">
        <picture>
          <source media="(prefers-color-scheme: dark)" srcset="public/assets/images/crowdin-white.png?raw=true">
          <source media="(prefers-color-scheme: light)" srcset="public/assets/images/crowdin-dark.png?raw=true">
          <img alt="Crowdin" src="public/assets/images/crowdin-dark.png?raw=true">
        </picture>
      </a>
    </td>
  </tr>
  <tr height="187px">
    <td align="center" width="33%">
      <a href="https://posthog.com/?utm_source=github&utm_medium=sponsorship&utm_campaign=next-js-boilerplate">
        <picture>
          <source media="(prefers-color-scheme: dark)" srcset="https://posthog.com/brand/posthog-logo-white.svg">
          <source media="(prefers-color-scheme: light)" srcset="https://posthog.com/brand/posthog-logo.svg">
          <img alt="PostHog" src="https://posthog.com/brand/posthog-logo.svg">
        </picture>
      </a>
    </td>
    <td align="center" width="33%">
      <a href="https://betterstack.com/?utm_source=github&utm_medium=sponsorship&utm_campaign=next-js-boilerplate">
        <picture>
          <source media="(prefers-color-scheme: dark)" srcset="public/assets/images/better-stack-white.png?raw=true">
          <source media="(prefers-color-scheme: light)" srcset="public/assets/images/better-stack-dark.png?raw=true">
          <img alt="Better Stack" src="public/assets/images/better-stack-dark.png?raw=true">
        </picture>
      </a>
    </td>
    <td align="center" width="33%">
      <a href="https://www.checklyhq.com/?utm_source=github&utm_medium=sponsorship&utm_campaign=next-js-boilerplate">
        <picture>
          <source media="(prefers-color-scheme: dark)" srcset="public/assets/images/checkly-logo-dark.png?raw=true">
          <source media="(prefers-color-scheme: light)" srcset="public/assets/images/checkly-logo-light.png?raw=true">
          <img alt="Checkly" src="public/assets/images/checkly-logo-light.png?raw=true">
        </picture>
      </a>
    </td>
  </tr>
  <tr height="187px">
    <td align="center" style=width="33%">
      <a href="https://nextjs-boilerplate.com/pro-saas-starter-kit">
        <img src="public/assets/images/nextjs-boilerplate-saas.png?raw=true" alt="Next.js SaaS Boilerplate with React" />
      </a>
    </td>
    <td align="center" width="33%">
      <a href="mailto:contact@creativedesignsguru.com">
        Add your logo here
      </a>
    </td>
  </tr>
</table>

### Demo

**Live demo: [Next.js Boilerplate](https://demo.nextjs-boilerplate.com)**

| Sign Up | Sign In |
| --- | --- |
| [![Next.js Boilerplate SaaS Sign Up](public/assets/images/nextjs-boilerplate-sign-in.png)](https://demo.nextjs-boilerplate.com/sign-up) | [![Next.js Boilerplate SaaS Sign In](public/assets/images/nextjs-boilerplate-sign-in.png)](https://demo.nextjs-boilerplate.com/sign-in) |

### Features

Developer experience first, extremely flexible code structure and only keep what you need:

- ‚ö° [Next.js](https://nextjs.org) with App Router support
- üî• Type checking [TypeScript](https://www.typescriptlang.org)
- üíé Integrate with [Tailwind CSS](https://tailwindcss.com)
- ‚úÖ Strict Mode for TypeScript and React 19
- üîí Authentication with [Clerk](https://clerk.com?utm_source=github&utm_medium=sponsorship&utm_campaign=nextjs-boilerplate): Sign up, Sign in, Sign out, Forgot password, Reset password, and more.
- üë§ Passwordless Authentication with Magic Links, Multi-Factor Auth (MFA), Social Auth (Google, Facebook, Twitter, GitHub, Apple, and more), Passwordless login with Passkeys, User Impersonation
- üì¶ Type-safe ORM with DrizzleORM, compatible with PostgreSQL, SQLite, and MySQL
- üíΩ Offline and local development database with PGlite
- üåê Multi-language (i18n) with [next-intl](https://next-intl-docs.vercel.app/) and [Crowdin](https://l.crowdin.com/next-js)
- ‚ôªÔ∏è Type-safe environment variables with T3 Env
- ‚å®Ô∏è Form handling with React Hook Form
- üî¥ Validation library with Zod
- üìè Linter with [ESLint](https://eslint.org) (default Next.js, Next.js Core Web Vitals, Tailwind CSS and Antfu configuration)
- üíñ Code Formatter with [Prettier](https://prettier.io)
- ü¶ä Husky for Git Hooks (replaced by Lefthook)
- üö´ Lint-staged for running linters on Git staged files
- üöì Lint git commit with Commitlint
- üìì Write standard compliant commit messages with Commitizen
- ü¶∫ Unit Testing with Vitest and Browser mode (replacing React Testing Library)
- üß™ Integration and E2E Testing with Playwright
- üë∑ Run tests on pull request with GitHub Actions
- üéâ Storybook for UI development
- üê∞ AI-powered code reviews with [CodeRabbit](https://www.coderabbit.ai?utm_source=next_js_starter&utm_medium=github&utm_campaign=next_js_starter_oss_2025)
- üö® Error Monitoring with [Sentry](https://sentry.io/for/nextjs/?utm_source=github&utm_medium=paid-community&utm_campaign=general-fy25q1-nextjs&utm_content=github-banner-nextjsboilerplate-logo)
- ‚òÇÔ∏è Code coverage with [Codecov](https://about.codecov.io/codecov-free-trial/?utm_source=github&utm_medium=paid-community&utm_campaign=general-fy25q1-nextjs&utm_content=github-banner-nextjsboilerplate-logo)
- üìù Logging with Pino.js and Log Management with [Better Stack](https://betterstack.com/?utm_source=github&utm_medium=sponsorship&utm_campaign=next-js-boilerplate)
- üñ•Ô∏è Monitoring as Code with [Checkly](https://www.checklyhq.com/?utm_source=github&utm_medium=sponsorship&utm_campaign=next-js-boilerplate)
- üîê Security and bot protection ([Arcjet](https://launch.arcjet.com/Q6eLbRE))
- üìä Analytics with PostHog
- üéÅ Automatic changelog generation with Semantic Release
- üîç Visual regression testing
- üí° Absolute Imports using `@` prefix
- üóÇ VSCode configuration: Debug, Settings, Tasks and Extensions
- ü§ñ SEO metadata, JSON-LD and Open Graph tags
- üó∫Ô∏è Sitemap.xml and robots.txt
- üë∑ Automatic dependency updates with Dependabot
- ‚åò Database exploration with Drizzle Studio and CLI migration tool with Drizzle Kit
- ‚öôÔ∏è [Bundler Analyzer](https://www.npmjs.com/package/@next/bundle-analyzer)
- üåà Include a FREE minimalist theme
- üíØ Maximize lighthouse score

Built-in feature from Next.js:

- ‚òï Minify HTML & CSS
- üí® Live reload
- ‚úÖ Cache busting

### Philosophy

- Nothing is hidden from you, allowing you to make any necessary adjustments to suit your requirements and preferences.
- Dependencies are regularly updated on a monthly basis
- Start for free without upfront costs
- Easy to customize
- Minimal code
- Unstyled template
- SEO-friendly
- üöÄ Production-ready

### Requirements

- Node.js 22+ and npm

### Getting started

Run the following command on your local environment:

```shell
git clone --depth=1 https://github.com/ixartz/Next-js-Boilerplate.git my-project-name
cd my-project-name
npm install
```

For your information, all dependencies are updated every month.

Then, you can run the project locally in development mode with live reload by executing:

```shell
npm run dev
```

Open http://localhost:3000 with your favorite browser to see your project.

### Set up authentication

To get started, you will need to create a Clerk account at [Clerk.com](https://clerk.com?utm_source=github&utm_medium=sponsorship&utm_campaign=nextjs-boilerplate) and create a new application in the Clerk Dashboard. Once you have done that, copy the `NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY` and `CLERK_SECRET_KEY` values and add them to the `.env.local` file (not tracked by Git):

```shell
NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY=your_clerk_pub_key
CLERK_SECRET_KEY=your_clerk_secret_key
```

Now you have a fully functional authentication system with Next.js, including features such as sign up, sign in, sign out, forgot password, reset password, update profile, update password, update email, delete account, and more.

### Set up remote database

The project uses DrizzleORM, a type-safe ORM that is compatible with PostgreSQL, SQLite, and MySQL databases. By default, the project is configured to seamlessly work with PostgreSQL, and you have the flexibility to choose any PostgreSQL database provider of your choice.

### Translation (i18n) setup

For translation, the project uses `next-intl` combined with [Crowdin](https://l.crowdin.com/next-js). As a developer, you only need to take care of the English (or another default language) version. Translations for other languages are automatically generated and handled by Crowdin. You can use Crowdin to collaborate with your translation team or translate the messages yourself with the help of machine translation.

To set up translation (i18n), create an account at [Crowdin.com](https://l.crowdin.com/next-js) and create a new project. In the newly created project, you will be able to find the project ID. You will also need to create a new Personal Access Token by going to Account Settings > API. Then, in your GitHub Actions, you need to define the following environment variables: `CROWDIN_PROJECT_ID` and `CROWDIN_PERSONAL_TOKEN`.

After defining the environment variables in your GitHub Actions, your localization files will be synchronized with Crowdin every time you push a new commit to the `main` branch.

### Project structure

```shell
.
‚îú‚îÄ‚îÄ README.md                       # README file
‚îú‚îÄ‚îÄ .github                         # GitHub folder
‚îú‚îÄ‚îÄ .storybook                      # Storybook folder
‚îú‚îÄ‚îÄ .vscode                         # VSCode configuration
‚îú‚îÄ‚îÄ migrations                      # Database migrations
‚îú‚îÄ‚îÄ public                          # Public assets folder
‚îú‚îÄ‚îÄ src
‚îÇ   ‚îú‚îÄ‚îÄ app                         # Next JS App (App Router)
‚îÇ   ‚îú‚îÄ‚îÄ components                  # React components
‚îÇ   ‚îú‚îÄ‚îÄ libs                        # 3rd party libraries configuration
‚îÇ   ‚îú‚îÄ‚îÄ locales                     # Locales folder (i18n messages)
‚îÇ   ‚îú‚îÄ‚îÄ models                      # Database models
‚îÇ   ‚îú‚îÄ‚îÄ styles                      # Styles folder
‚îÇ   ‚îú‚îÄ‚îÄ templates                   # Templates folder
‚îÇ   ‚îú‚îÄ‚îÄ types                       # Type definitions
‚îÇ   ‚îú‚îÄ‚îÄ utils                       # Utilities folder
‚îÇ   ‚îî‚îÄ‚îÄ validations                 # Validation schemas
‚îú‚îÄ‚îÄ tests
‚îÇ   ‚îú‚îÄ‚îÄ e2e                         # E2E tests, also includes Monitoring as Code
‚îÇ   ‚îî‚îÄ‚îÄ integration                 # Integration tests
‚îî‚îÄ‚îÄ tsconfig.json                   # TypeScript configuration
```

### Customization

You can easily configure Next js Boilerplate by searching the entire project for `FIXME:` to make quick customizations. Here are some of the most important files to customize:

- `public/apple-touch-icon.png`, `public/favicon.ico`, `public/favicon-16x16.png` and `public/favicon-32x32.png`: your website favicon
- `src/utils/AppConfig.ts`: configuration file
- `src/templates/BaseTemplate.tsx`: default theme
- `next.config.ts`: Next.js configuration
- `.env`: default environment variables

You have full access to the source code for further customization. The provided code is just an example to help you start your project. The sky's the limit üöÄ.

### Change database schema

To modify the database schema in the project, you can update the schema file located at `./src/models/Schema.ts`. This file defines the structure of your database tables using the Drizzle ORM library.

After making changes to the schema, generate a migration by running the following command:

```shell
npm run db:generate
```

This will create a migration file that reflects your schema changes. The migration is automatically applied during the next database interaction, so there is no need to run it manually or restart the Next.js server.

### Commit Message Format

The project follows the [Conventional Commits](https://www.conventionalcommits.org/) specification, meaning all commit messages must be formatted accordingly. To help you write commit messages, the project uses [Commitizen](https://github.com/commitizen/cz-cli), an interactive CLI that guides you through the commit process. To use it, run the following command:

```shell
npm run commit
```

One of the benefits of using Conventional Commits is the ability to automatically generate a `CHANGELOG` file. It also allows us to automatically determine the next version number based on the types of commits that are included in a release.

### CodeRabbit AI Code Reviews

The project uses [CodeRabbit](https://www.coderabbit.ai?utm_source=next_js_starter&utm_medium=github&utm_campaign=next_js_starter_oss_2025), an AI-powered code reviewer. CodeRabbit monitors your repository and automatically provides intelligent code reviews on all new pull requests using its powerful AI engine.

Setting up CodeRabbit is simple, visit [coderabbit.ai](https://www.coderabbit.ai?utm_source=next_js_starter&utm_medium=github&utm_campaign=next_js_starter_oss_2025), sign in with your GitHub account, and add your repository from the dashboard. That's it!

### Testing

All unit tests are located alongside the source code in the same directory, making them easier to find. The project uses Vitest and React Testing Library for unit testing. You can run the tests with the following command:

```shell
npm run test
```

### Integration & E2E Testing

The project uses Playwright for integration and end-to-end (E2E) testing. You can run the tests with the following commands:

```shell
npx playwright install # Only for the first time in a new environment
npm run test:e2e
```

### Deploy to production

During the build process, database migrations are automatically executed, so there's no need to run them manually. However, you must define `DATABASE_URL` in your environment variables.

Then, you can generate a production build with:

```shell
$ npm run build
```

It generates an optimized production build of the boilerplate. To test the generated build, run:

```shell
$ npm run start
```

You also need to defined the environment variables `CLERK_SECRET_KEY` using your own key.

This command starts a local server using the production build. You can now open http://localhost:3000 in your preferred browser to see the result.

### Deploy to Sevalla

You can deploy a Next.js application along with its database on a single platform. First, create an account on [Sevalla](https://sevalla.com).

After registration, you will be redirected to the dashboard. From there, navigate to `Database > Create a database`. Select PostgreSQL and and use the default settings for a quick setup. For advanced users, you can customize the database location and resource size. Finally, click on `Create` to complete the process.

Once the database is created and ready, return to the dashboard and click `Application > Create an App`. After connecting your GitHub account, select the repository you want to deploy. Keep the default settings for the remaining options, then click `Create`.

Next, connect your database to your application by going to `Networking > Connected services > Add connection` and select the database you just created. You also need to enable the `Add environment variables to the application` option, and rename `DB_URL` to `DATABASE_URL`. Then, click `Add connection`.

Go to `Environment variables > Add environment variable`, and define the environment variables `NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY` and `CLERK_SECRET_KEY` from your Clerk account. Click `Save`.

Finally, initiate a new deployment by clicking `Overview > Latest deployments > Deploy now`. If everything is set up correctly, your application will be deployed successfully with a working database.

### Error Monitoring

The project uses [Sentry](https://sentry.io/for/nextjs/?utm_source=github&utm_medium=paid-community&utm_campaign=general-fy25q1-nextjs&utm_content=github-banner-nextjsboilerplate-logo) to monitor errors. In the development environment, no additional setup is needed: Next.js Boilerplate is pre-configured to use Sentry and Spotlight (Sentry for Development). All errors will automatically be sent to your local Spotlight instance, allowing you to experience Sentry locally.

For production environment, you'll need to create a Sentry account and a new project. Then, in `next.config.mjs`, you need to update the `org` and `project` attributes in `withSentryConfig` function. Additionally, add your Sentry DSN to `sentry.client.config.ts`, `sentry.edge.config.ts` and `sentry.server.config.ts`.

### Code coverage

Next.js Boilerplate relies on [Codecov](https://about.codecov.io/codecov-free-trial/?utm_source=github&utm_medium=paid-community&utm_campaign=general-fy25q1-nextjs&utm_content=github-banner-nextjsboilerplate-logo) for code coverage reporting solution. To enable Codecov, create a Codecov account and connect it to your GitHub account. Your repositories should appear on your Codecov dashboard. Select the desired repository and copy the token. In GitHub Actions, define the `CODECOV_TOKEN` environment variable and paste the token.

Make sure to create `CODECOV_TOKEN` as a GitHub Actions secret, do not paste it directly into your source code.

### Logging

The project uses Pino.js for logging. In the development environment, logs are displayed in the console by default.

For production, the project is already integrated with [Better Stack](https://betterstack.com/?utm_source=github&utm_medium=sponsorship&utm_campaign=next-js-boilerplate) to manage and query your logs using SQL. To use Better Stack, you need to create a [Better Stack](https://betterstack.com/?utm_source=github&utm_medium=sponsorship&utm_campaign=next-js-boilerplate) account and create a new source: go to your Better Stack Logs Dashboard > Sources > Connect source. Then, you need to give a name to your source and select Node.js as the platform.

After creating the source, you will be able to view and copy your source token. In your environment variables, paste the token into the `LOGTAIL_SOURCE_TOKEN` variable. Now, all logs will automatically be sent to and ingested by Better Stack.

### Checkly monitoring

The project uses [Checkly](https://www.checklyhq.com/?utm_source=github&utm_medium=sponsorship&utm_campaign=next-js-boilerplate) to ensure that your production environment is always up and running. At regular intervals, Checkly runs the tests ending with `*.check.e2e.ts` extension and notifies you if any of the tests fail. Additionally, you have the flexibility to execute tests from multiple locations to ensure that your application is available worldwide.

To use Checkly, you must first create an account on [their website](https://www.checklyhq.com/?utm_source=github&utm_medium=sponsorship&utm_campaign=next-js-boilerplate). After creating an account, generate a new API key in the Checkly Dashboard and set the `CHECKLY_API_KEY` environment variable in GitHub Actions. Additionally, you will need to define the `CHECKLY_ACCOUNT_ID`, which can also be found in your Checkly Dashboard under User Settings > General.

To complete the setup, update the `checkly.config.ts` file with your own email address and production URL.

### Arcjet security and bot protection

The project uses [Arcjet](https://launch.arcjet.com/Q6eLbRE), a security as code product that includes several features that can be used individually or combined to provide defense in depth for your site.

To set up Arcjet, [create a free account](https://launch.arcjet.com/Q6eLbRE) and get your API key. Then add it to the `ARCJET_KEY` environment variable.

Arcjet is configured with two main features: bot detection and the Arcjet Shield WAF:

- [Bot detection](https://docs.arcjet.com/bot-protection/concepts) is configured to allow search engines, preview link generators e.g. Slack and Twitter previews, and to allow common uptime monitoring services. All other bots, such as scrapers and AI crawlers, will be blocked. You can [configure additional bot types](https://docs.arcjet.com/bot-protection/identifying-bots) to allow or block.
- [Arcjet Shield WAF](https://docs.arcjet.com/shield/concepts) will detect and block common attacks such as SQL injection, cross-site scripting, and other OWASP Top 10 vulnerabilities.

Arcjet is configured with a central client at `src/libs/Arcjet.ts` that includes the Shield WAF rules. Additional rules are applied when Arcjet is called in `middleware.ts`.

### Useful commands

#### Bundle Analyzer

Next.js Boilerplate includes a built-in bundle analyzer. It can be used to analyze the size of your JavaScript bundles. To begin, run the following command:

```shell
npm run build-stats
```

By running the command, it'll automatically open a new browser window with the results.

#### Database Studio

The project is already configured with Drizzle Studio to explore the database. You can run the following command to open the database studio:

```shell
npm run db:studio
```

Then, you can open https://local.drizzle.studio with your favorite browser to explore your database.

### VSCode information (optional)

If you are VSCode user, you can have a better integration with VSCode by installing the suggested extension in `.vscode/extension.json`. The starter code comes up with Settings for a seamless integration with VSCode. The Debug configuration is also provided for frontend and backend debugging experience.

With the plugins installed in your VSCode, ESLint and Prettier can automatically fix the code and display errors. The same applies to testing: you can install the VSCode Vitest extension to automatically run your tests, and it also shows the code coverage in context.

Pro tips: if you need a project wide-type checking with TypeScript, you can run a build with <kbd>Cmd</kbd> + <kbd>Shift</kbd> + <kbd>B</kbd> on Mac.

### Contributions

Everyone is welcome to contribute to this project. Feel free to open an issue if you have any questions or find a bug. Totally open to suggestions and improvements.

### License

Licensed under the MIT License, Copyright ¬© 2025

See [LICENSE](LICENSE) for more information.

## Sponsors

<table width="100%">
  <tr height="187px">
    <td align="center" width="33%">
      <a href="https://clerk.com?utm_source=github&utm_medium=sponsorship&utm_campaign=nextjs-boilerplate">
        <picture>
          <source media="(prefers-color-scheme: dark)" srcset="https://github.com/ixartz/SaaS-Boilerplate/assets/1328388/6fb61971-3bf1-4580-98a0-10bd3f1040a2">
          <source media="(prefers-color-scheme: light)" srcset="https://github.com/ixartz/SaaS-Boilerplate/assets/1328388/f80a8bb5-66da-4772-ad36-5fabc5b02c60">
          <img alt="Clerk ‚Äì Authentication & User Management for Next.js" src="https://github.com/ixartz/SaaS-Boilerplate/assets/1328388/f80a8bb5-66da-4772-ad36-5fabc5b02c60">
        </picture>
      </a>
    </td>
    <td align="center" width="33%">
      <a href="https://www.coderabbit.ai?utm_source=next_js_starter&utm_medium=github&utm_campaign=next_js_starter_oss_2025">
        <picture>
          <source media="(prefers-color-scheme: dark)" srcset="public/assets/images/coderabbit-logo-dark.svg?raw=true">
          <source media="(prefers-color-scheme: light)" srcset="public/assets/images/coderabbit-logo-light.svg?raw=true">
          <img alt="CodeRabbit" src="public/assets/images/coderabbit-logo-light.svg?raw=true">
        </picture>
      </a>
    </td>
    <td align="center" width="33%">
      <a href="https://sentry.io/for/nextjs/?utm_source=github&utm_medium=paid-community&utm_campaign=general-fy25q1-nextjs&utm_content=github-banner-nextjsboilerplate-logo">
        <picture>
          <source media="(prefers-color-scheme: dark)" srcset="public/assets/images/sentry-white.png?raw=true">
          <source media="(prefers-color-scheme: light)" srcset="public/assets/images/sentry-dark.png?raw=true">
          <img alt="Sentry" src="public/assets/images/sentry-dark.png?raw=true">
        </picture>
      </a>
      <a href="https://about.codecov.io/codecov-free-trial/?utm_source=github&utm_medium=paid-community&utm_campaign=general-fy25q1-nextjs&utm_content=github-banner-nextjsboilerplate-logo">
        <picture>
          <source media="(prefers-color-scheme: dark)" srcset="public/assets/images/codecov-white.svg?raw=true">
          <source media="(prefers-color-scheme: light)" srcset="public/assets/images/codecov-dark.svg?raw=true">
          <img alt="Codecov" src="public/assets/images/codecov-dark.svg?raw=true">
        </picture>
      </a>
    </td>
  </tr>
  <tr height="187px">
    <td align="center" width="33%">
      <a href="https://launch.arcjet.com/Q6eLbRE">
        <picture>
          <source media="(prefers-color-scheme: dark)" srcset="public/assets/images/arcjet-dark.svg?raw=true">
          <source media="(prefers-color-scheme: light)" srcset="public/assets/images/arcjet-light.svg?raw=true">
          <img alt="Arcjet" src="public/assets/images/arcjet-light.svg?raw=true">
        </picture>
      </a>
    </td>
    <td align="center" width="33%">
      <a href="https://sevalla.com/">
        <picture>
          <source media="(prefers-color-scheme: dark)" srcset="public/assets/images/sevalla-dark.png">
          <source media="(prefers-color-scheme: light)" srcset="public/assets/images/sevalla-light.png">
          <img alt="Sevalla" src="public/assets/images/sevalla-light.png">
        </picture>
      </a>
    </td>
    <td align="center" width="33%">
      <a href="https://l.crowdin.com/next-js">
        <picture>
          <source media="(prefers-color-scheme: dark)" srcset="public/assets/images/crowdin-white.png?raw=true">
          <source media="(prefers-color-scheme: light)" srcset="public/assets/images/crowdin-dark.png?raw=true">
          <img alt="Crowdin" src="public/assets/images/crowdin-dark.png?raw=true">
        </picture>
      </a>
    </td>
  </tr>
  <tr height="187px">
    <td align="center" width="33%">
      <a href="https://posthog.com/?utm_source=github&utm_medium=sponsorship&utm_campaign=next-js-boilerplate">
        <picture>
          <source media="(prefers-color-scheme: dark)" srcset="https://posthog.com/brand/posthog-logo-white.svg">
          <source media="(prefers-color-scheme: light)" srcset="https://posthog.com/brand/posthog-logo.svg">
          <img alt="PostHog" src="https://posthog.com/brand/posthog-logo.svg">
        </picture>
      </a>
    </td>
    <td align="center" width="33%">
      <a href="https://betterstack.com/?utm_source=github&utm_medium=sponsorship&utm_campaign=next-js-boilerplate">
        <picture>
          <source media="(prefers-color-scheme: dark)" srcset="public/assets/images/better-stack-white.png?raw=true">
          <source media="(prefers-color-scheme: light)" srcset="public/assets/images/better-stack-dark.png?raw=true">
          <img alt="Better Stack" src="public/assets/images/better-stack-dark.png?raw=true">
        </picture>
      </a>
    </td>
    <td align="center" width="33%">
      <a href="https://www.checklyhq.com/?utm_source=github&utm_medium=sponsorship&utm_campaign=next-js-boilerplate">
        <picture>
          <source media="(prefers-color-scheme: dark)" srcset="public/assets/images/checkly-logo-dark.png?raw=true">
          <source media="(prefers-color-scheme: light)" srcset="public/assets/images/checkly-logo-light.png?raw=true">
          <img alt="Checkly" src="public/assets/images/checkly-logo-light.png?raw=true">
        </picture>
      </a>
    </td>
  </tr>
  <tr height="187px">
    <td align="center" style=width="33%">
      <a href="https://nextjs-boilerplate.com/pro-saas-starter-kit">
        <img src="public/assets/images/nextjs-boilerplate-saas.png?raw=true" alt="Next.js SaaS Boilerplate with React" />
      </a>
    </td>
    <td align="center" width="33%">
      <a href="mailto:contact@creativedesignsguru.com">
        Add your logo here
      </a>
    </td>
  </tr>
</table>

---

Made with ‚ô• by [CreativeDesignsGuru](https://creativedesignsguru.com) [![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Follow%20%40Ixartz)](https://twitter.com/ixartz)

Looking for a custom boilerplate to kick off your project? I'd be glad to discuss how I can help you build one. Feel free to reach out anytime at contact@creativedesignsguru.com!

[![Sponsor Next JS Boilerplate](https://cdn.buymeacoffee.com/buttons/default-red.png)](https://github.com/sponsors/ixartz)


# Portfolio


### üí´ Hi there! üëã I'm Kipkemoi Vincent  

I am a passionate Data Scientist, ML/AI Engineer & Data Analyst with expertise in transforming raw data into actionable insights and developing scalable data science/ ML solutions in production environments. With experience across fintech, environmental science, healthcare, and agri-tech, I build machine learning models, predictive systems, and analytical solutions that drive growth and optimize decision-making. Currently based in **Nairobi, Kenya**, open to opportunities **worldwide**.



### üéì Education
- MPhil in Environmental Science: The Cyprus Institute, Nicosia Cyprus; through the **Cyprus Institute Merit Scholarship**.
- MSc in Mathematical Sciences(Data Science) : University of Western Cape/AIMS, Cape Town, South Africa; The program made possible through **MasterCard Foundation scholarship**.
- BSc. Mathematics: University of Nairobi, Nairobi, Kenya; through sponsorship by **Finlays Undergraduate Scholarship**.


# üìã Professional Summary

- 5+ years experience delivering Data Science and ML solutions across sectors (Fintech, Climate, Healthcare).
- Skilled in data analytics, predictive analytics,credit risk analysis, anomaly & fraud detection, credit scoring,
 time series forecasting, and scalable ML model deployment.
- Proficient in Python, R, SQL, Scikit-learn, CNN, OpenCV, TensorFlow, RF, LSTM,Catboost, XGBoost, LightGBM,PyOD models and AWS.
- Experienced with MLOps workflows: streamlit, Docker, Kubernetes, MLflow, FastAPI, and CI/CD pipelines.
- Committed to leveraging advanced analytics for risk minimization, business growth, and social impact.

## üåç Connect with Me  
<img src="https://cdn-icons-png.flaticon.com/512/174/174857.png" width="20" /> **Linkedln:** [kipkemoi-vincent-19307a94/](https://www.linkedin.com/in/kipkemoi-vincent-19307a94/)  
üåê **Website:** [vinylango25.github.io](https://vinylango25.github.io/)<br>
![GitHub](https://img.shields.io/badge/GitHub-000?style=flat&logo=github)[github.com/Vinylango25](https://github.com/Vinylango25?tab=repositories)<br>
 üì´ **Email:** vincentl@aims.ac.za / vinylango90@gmail.com



## üíº Featured projects

| Project | Description | Tools Used |
|--------|-------------|------------|
| [üõ°Ô∏è Anomaly and Fraud Detection in Finance](https://github.com/Vinylango25/Anomaly-Fraud-detection-in-Finance) | This project applies PyOD and Microsoft AutoML (FLAML) to detect anomalies in credit card transactions using a highly imbalanced dataset. A variety of algorithms‚Äîincluding Isolation Forest and Autoencoders‚Äîwere tested for their ability to flag suspicious activity. To address imbalance, techniques like undersampling, oversampling, and SMOTE were applied. Evaluation focused on metrics like Precision, Recall, and ROC-AUC for a robust assessment. The outcome is a high-precision fraud detection pipeline that enhances financial risk management. üëâ Read the full project on [Github](https://github.com/Vinylango25/Anomaly-Fraud-detection-in-Finance) or  [Medium](https://medium.com/@vinylango90/anomaly-detection-in-credit-card-transactions-using-pyod-and-microsoft-automl-flaml-727a091487bd)| Python, Scikit-learn, XGBoost, RF, Optuna, LightGBM, FLAML, PyOD, LIME, SHAP |
| [üè• Enhancing Healthcare Accessibility in Nairobi](https://github.com/Vinylango25/Healthcare-Accessibility-in-Nairobi) | This project presents a comprehensive, data-driven narrative that delves into the current state of Nairobi‚Äôs healthcare infrastructure. By leveraging multiple datasets, including population demographics and health facility distributions, it evaluates how well the city is positioned to meet the healthcare needs of its diverse population. The analysis is anchored within the framework of the United Nations Sustainable Development Goal 3, which aims to ensure healthy lives and promote well-being for all at all ages. Through detailed examination of healthcare accessibility, service availability, and facility operational hours, this study identifies critical gaps and opportunities for improvement. Ultimately, the findings serve as a foundation for informed policy-making and strategic interventions designed to advance Nairobi‚Äôs journey toward achieving SDG 3. üëâ Read the full project on [GitHub](https://github.com/Vinylango25/Healthcare-Accessibility-in-Nairobi) or [Medium](https://medium.com/@vinylango90/enhancing-healthcare-accessibility-in-nairobi-kenya-21dfff0a2ac8) | Python, Matplotlib, Seaborn, Plotly, GeoPandas, OSRM, Folium, Pandas, QGIS, Spatial Analysis |
| [ü§ñ Lending Automation - ML for Credit Scoring](https://github.com/Vinylango25/Credit-Scoring-Lending-automation) | This project builds an end-to-end loan approval system using machine learning algorithms like Random Forest, XGBoost, and LightGBM. It replaces manual decision-making with faster, scalable, and data-driven processes for improved credit scoring. Key tasks include data cleaning, feature engineering, and model optimization using real-world loan data. Evaluation metrics ensure accuracy and fairness, reducing false approvals and rejections. The system supports personalized lending and dynamic pricing for better customer experience. üëâ Read the full project on [Github](https://github.com/Vinylango25/Credit-Scoring-Lending-automation) or [Medium](https://medium.com/@vinylango90/credit-scoring-lending-automation-6dabe30d53da)| Python, Scikit-learn, RF, XGBoost, LightGBM, Catboost |
| [üåç Air Quality Monitoring in Nicosia, Cyprus](https://github.com/Vinylango25/Air-Quality-in-Nicosia-Cyprus) | To improve urban air quality monitoring, this project calibrates low-cost electrochemical sensors using ML algorithms like XGBoost, Random Forest, and ANN. Raw sensor data, collected over six months, is aligned with reference-grade measurements. The study analyzes calibration frequency, data sampling strategies, and environmental factors like humidity and cross-gas interference. Results show that with proper calibration, LCSs can meet EU and EPA accuracy standards. This opens doors for cost-effective, citywide monitoring networks. üëâ Read the full project on [Github](https://github.com/Vinylango25/Air-Quality-in-Nicosia-Cyprus)  or [Medium](https://medium.com/@vinylango90/sensor-calibration-and-air-quality-monitoring-in-nicosia-cyprus-e5072a4184aa)| Python, Scikit-learn, LR, SVR, ANN, FLAML, XGBoost, Random Forest,Jupyter Notebook |
| [ü©∫ COVID-19 Detection Using CT Scans](https://github.com/Vinylango25/Covid-19-Detection-Deep-Learning) | This project applies deep learning to detect COVID-19 from chest CT scans using convolutional neural networks. Models like ResNet50, DenseNet169, and MobileNetV2 are trained and fine-tuned for accurate image classification, achieving high detection accuracy through transfer learning and ensemble methods. The pipeline includes image preprocessing, augmentation, and evaluation with real-world datasets. The system supports rapid and reliable diagnosis, aiding medical decision-making. üëâ Read the full project on [Github](https://github.com/Vinylango25/Covid-19-Detection-Deep-Learning) or [Medium](https://medium.com/@vinylango90/deep-learning-based-detection-of-covid-19-from-ct-scans-using-convolutional-neural-networks-a09ca1a6f9ce) |Python, TensorFlow, Keras, CNN, ResNet50, DenseNet169, MobileNetV2 |
| [üìâ Customer Churn Analysis and Prediction](https://github.com/Vinylango25/Customer_Churn_Analysis_And_Prediction) |This project predicts customer churn using survival analysis and machine learning to identify clients likely to leave. It focuses on telecom-style use cases where retaining customers is more cost-effective than acquiring new ones. By analyzing historical behavior and risk factors, it enables targeted retention campaigns. An interactive tool is also developed to assess individual churn risk and lifetime value. This helps businesses make data-driven decisions to reduce attrition and improve customer loyalty. üëâ Read the full project on [Github](https://github.com/Vinylango25/Customer_Churn_Analysis_And_Prediction). | Python, Flask, Scikit-learn, SHAP, Cox Proportional Hazards Model, Survival Analysis |
| [üìä Sales Time Series Analysis](https://github.com/Vinylango25/Sales_Time_Series_Analysis) | This project analyzes historical sales data using time series forecasting techniques to uncover trends, seasonality, and patterns that drive business performance. By leveraging models like ARIMA, SARIMA, and Prophet, it enables accurate sales forecasting to support inventory planning, resource allocation, and strategic decision-making. The pipeline includes data preprocessing, stationarity testing, model tuning, and performance evaluation. The insights generated help optimize operations and reduce forecasting errors. üëâ Read the full project on [Github](https://github.com/Vinylango25/Sales_Time_Series_Analysis)| Python, Pandas, Statsmodels, FBProphet, ARIMA, SARIMA |
| [üíª Machine Learning Loan Application Web App](https://github.com/Vinylango25/Machine_Learning_Loan_Application_Web_App) | This project delivers a full-stack web application for automated loan approval using machine learning models. Built with Streamlit and powered by Random Forest and Logistic Regression, the system predicts loan eligibility based on user input. The backend includes data preprocessing, model training, and evaluation, while the frontend ensures a seamless user experience. It streamlines loan applications by replacing manual reviews with instant, data-driven decisions. üëâ Read the full project on [Github](https://github.com/Vinylango25/Machine_Learning_Loan_Application_Web_App) | Python, Scikit-learn, Streamlit, Random Forest, Logistic Regression, Flask|
| [üöÄ Active Learning API (Django + CatBoost)](https://github.com/Vinylango25/active-learning-backend) | This project implements an active learning backend system to optimize data labeling for machine learning tasks. It intelligently selects the most informative samples for annotation, reducing labeling effort while improving model performance. Built with Django and integrated with machine learning models (e.g., CatBoost), it supports iterative learning cycles, model versioning, and anomaly detection. Key features include dataset management, active query strategies, and seamless MLflow tracking. üëâ Read the full project on [Github](https://github.com/Vinylango25/active-learning-backend)| Python, Django, CatBoost, MLflow, Active Learning, REST API, SQLite|
| [üß† Brain Tumor Detection Using Deep Learning](https://github.com/Vinylango25/Brain-Tumor-Detection-Using-Deep-Learning) |his project leverages deep learning to detect brain tumors from MRI scans using advanced convolutional neural networks. It employs architectures like MobileNetV2, DenseNet169, and ResNet50, enhanced through transfer learning and ensemble techniques to achieve up to 99.8% accuracy. The workflow includes image preprocessing, model training, and performance evaluation, enabling fast, accurate, and scalable tumor classification to support early diagnosis and clinical workflows. üëâ Read the full project [Github](https://github.com/Vinylango25/Brain-Tumor-Detection-Using-Deep-Learning) | Python, TensorFlow, Keras, CNN, MobileNetV2, ResNet50, DenseNet169, FLAML|

---


## üíª Tech Stack  
<p align="left">
  <img src="https://img.shields.io/badge/Python-blue?style=flat&logo=python" height="30">
  <img src="https://img.shields.io/badge/R-lightgrey?style=flat&logo=r" height="30">
  <img src="https://img.shields.io/badge/SQL-orange?style=flat&logo=postgresql" height="30">
  <img src="https://img.shields.io/badge/TensorFlow-orange?style=flat&logo=tensorflow" height="30">
  <img src="https://img.shields.io/badge/PyTorch-red?style=flat&logo=pytorch" height="30">
  <img src="https://img.shields.io/badge/Scikit--Learn-yellow?style=flat&logo=scikit-learn" height="30">
  <img src="https://img.shields.io/badge/XGBoost-orange?style=flat&logo=xgboost" height="30">
  <img src="https://img.shields.io/badge/CatBoost-blue?style=flat&logo=catboost" height="30">
  <img src="https://img.shields.io/badge/LightGBM-green?style=flat&logo=lightgbm" height="30">
  <img src="https://img.shields.io/badge/Sktime-blue?style=flat" height="30">
  <img src="https://img.shields.io/badge/LSTM-red?style=flat" height="30">
  <img src="https://img.shields.io/badge/Django-green?style=flat&logo=django" height="30">
  <img src="https://img.shields.io/badge/Flask-black?style=flat&logo=flask" height="30">
  <img src="https://img.shields.io/badge/FastAPI-blue?style=flat&logo=fastapi" height="30">
  <img src="https://img.shields.io/badge/Docker-blue?style=flat&logo=docker" height="30">
  <img src="https://img.shields.io/badge/Kubernetes-darkblue?style=flat&logo=kubernetes" height="30">
  <img src="https://img.shields.io/badge/AWS-yellow?style=flat&logo=amazon-aws" height="30">
  <img src="https://img.shields.io/badge/Gurobi-red?style=flat" height="30">
  <img src="https://img.shields.io/badge/CPLEX-darkblue?style=flat" height="30">
  <img src="https://img.shields.io/badge/Power%20BI-yellow?style=flat&logo=power-bi" height="30">
  <img src="https://img.shields.io/badge/Tableau-blueviolet?style=flat&logo=tableau" height="30">
  <img src="https://img.shields.io/badge/Excel-green?style=flat&logo=microsoft-excel" height="30">
  <img src="https://img.shields.io/badge/STATA-blue?style=flat" height="30">
  <img src="https://img.shields.io/badge/Matlab-orange?style=flat&logo=mathworks" height="30">
  <img src="https://img.shields.io/badge/SPSS-lightblue?style=flat" height="30">
  <img src="https://img.shields.io/badge/Gretl-darkgreen?style=flat" height="30">
  <img src="https://img.shields.io/badge/Singular-purple?style=flat" height="30">
  <img src="https://img.shields.io/badge/Preset-black?style=flat" height="30">
  <img src="https://img.shields.io/badge/DBeaver-darkred?style=flat" height="30">
  <img src="https://img.shields.io/badge/SageMath-darkblue?style=flat" height="30">
  <img src="https://img.shields.io/badge/Git-black?style=flat&logo=git" height="30">
  <img src="https://img.shields.io/badge/LATEX-blue?style=flat&logo=latex" height="30">
  <img src="https://img.shields.io/badge/FLAML-orange?style=flat" height="30">
  <img src="https://img.shields.io/badge/Looker-purple?style=flat&logo=looker" height="30">
</p>

## üèÜ GitHub Trophies

![GitHub Trophies](https://github-profile-trophy.vercel.app/?username=Vinylango25&theme=dracula&width=1200)  
![GitHub Trophies](https://github-profile-trophy.vercel.app/?username=vinylango&theme=dracula&width=1200)

---


## üìä GitHub Stats

<table>
<tr>
<td align="center">
  
### **Vinylango25 Stats**  
 
<img src="https://streak-stats.demolab.com/?user=Vinylango25&theme=dark&hide_border=false" alt="GitHub Streak" />  
<img src="https://github-readme-stats.vercel.app/api?username=Vinylango25&show_icons=true&theme=dark" alt="GitHub Stats" />  
<img src="https://github-readme-stats.vercel.app/api/top-langs/?username=Vinylango25&theme=dark&layout=compact" alt="Top Languages" />

</td>

<td align="center">

### **Vinylango Stats**  

<img src="https://streak-stats.demolab.com/?user=vinylango&theme=dark&hide_border=false" alt="GitHub Streak" />  
<img src="https://github-readme-stats.vercel.app/api?username=vinylango&show_icons=true&theme=dark" alt="GitHub Stats" />  
<img src="https://github-readme-stats.vercel.app/api/top-langs/?username=vinylango&theme=dark&layout=compact" alt="Top Languages" />

</td>
</tr>
</table>




## üìú Random Dev Quote  
![Quote](https://quotes-github-readme.vercel.app/api?type=horizontal)  

---

## üöÄ Let's Collaborate!  
I am open to working on **AI, ML, data science, and fintech projects**.  
üí¨ **Reach out to me for exciting collaborations!**  


# Production_ready_anomaly_detection_streamlit


# üí∞ Financial Anomaly Detector

A lightweight Streamlit app to detect fraudulent or unusual transactions in financial datasets using the Isolation Forest algorithm.

## üöÄ Features

- Anomaly detection using Isolation Forest
- Interactive frontend via Streamlit
- Visual summary of anomalies
- Built using open-source tools

## üìä Dataset

Dataset: [Credit Card Fraud Detection on Kaggle](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)

- **Class** column: 1 = Fraud, 0 = Normal
- Features: V1‚ÄìV28 (PCA-reduced), `Amount`, `Time`

## üõ†Ô∏è Setup

```bash
git clone https://github.com/yourusername/anomaly-detector-finance.git
cd anomaly-detector-finance
pip install -r requirements.txt


# Sales_Time_Series_Analysis


# Time Series Analysis on Sales Data

### Motivation
---

<p align="justify"> Time series prediction problems pose an important role in many domains and multi-series (More than one time series), multivariate (multiple predictors) and multi-step forecasting like stock price prediction of different symbols could help people make better decisions. However, these problems are quite hard to solve. </p>

<p align="justify"> Retail is an important business domain for data science and data mining applications. Sales forecasting is an essential task in retail stores. Being able to estimate the number of products that a store going to sell in future will allow store managers to prepare the inventory, the number of employees needed minimizes over and under stocking thereby minimizing losses and most importantly maximizes sales and customer satisfaction. Therefore forecasting sales taking into account all of the factors becomes essential. </p>

<p align="justify"> Given the background, we would like to analyze and identify the factors that affect the sales and predict patterns in the same for different stores over time. We make use of the sales data of 10 stores over the time period of 2 years and work towards forecasting future sales. </p>

We aim to approach this problem from two different perspectives:

- Treating the prediction of sales purely as a time series analysis and forecast sales using Autoregression Integrated Moving Average (ARIMA), LSTM and see how these two models could increase the forecasting accuracy.

- Viewing the problem as a Supervised Machine Learning problem by taking lags and calculating moving averages both on target and features. we would like to compare the relative performance of XGBoost to above time series models.


<p align="justify"> we would like to design a web application that provides an interface by which data scientists or store managers can use past sales data to forecast it from a selected date. In addition to allowing the user to retrain and tune three different time series models, the application also displays the model performance, past information and forecasted predictions visually. </p>


### Dataset
---

The dataset is sample sales information of 10 different stores over the time of 2 years from DataRobot Inc.

Features Available:

| Feature Name      | Data Type  | Description                                                                |
|-------------------|------------|----------------------------------------------------------------------------|
| Store_Location 	| String     | Different locations of the store [Categorical]							  |
| Date 				| Date/Time  | Year, month and day of the sales of the stores [Quantitative]              |
| Sales 			| Float      | Sales of different stores over time [Quantitative]                         |
| Store_Size 		| Integer    | size of the store [Quantitative]                                           |
| Num_Employees 	| Integer    | Number of employees on that particular date and store [Quantitative].      |
| Returns_Pct		| Float      | Return percentage [Quantitative]                                           |
| Num_Customer		| Integer    | Number of Customers in a store and on a particular Date [Quantitative]     |
| Pct_On_Sale 		| Float      | Percentage on Sale [Quantitative]                                          |
| Marketing			| Text       | Discount Information of the store on a date [Text]                         |
| Near_Xmas			| Integer    | If a particular day is near to Christmas [Categorical]                     |
| Near_BlackFriday	| Integer    | If a particular day is near to Black Friday [Categorical]                  |
| Holiday			| String     | If a particular is a Holiday or not [Categorical]                          |
| Destination_Event	| String     | If there is a destination event on that particular day or not [Categorical]|
| Econ_ChangeGDP	| Float      | Change in economy GDP [Quantitative]                                       |
| Econ_JobsChange	| Float      | Change in jobs [Quantitative]                                              |
| Annualized_CPI 	| Float      | Annual Consumer Price Index (CPI) [Quantitative]                           |


#### Data Preprocessing

The following operations were done on the raw data as a part of the cleanup process:

- Typecasting Features (For Instance: Date).

- Conditional processing of features Holiday and Destination_Event to numeric from string to numeric.

- One-Hot encoding of categorical features.

- Normalizing the quantitative features.

- Character count for text features as a new feature.

- Imputing Missing values for quantitative features Econ_ChangeGDP, Econ_JobsChange, Annualized_CPI using forward filling as these are an economic indicator which will remain same.


### Exploratory Data Analysis
---

We performed profiling of all the variables used in the analysis for the distributions of numeric features, unique values of categorical features, and correlations.

![alt text](images/DatasetInfo.png)

![alt text](images/Corr-Plot.png)

We also brought week as a variable in our analysis from the date and looked at the variation of sales prices over the three years with heatmaps. The anticipated climb in sales during the weeks of November/December can be easily observed here.

![alt text](images/Heatmap-Plot.png)

To be able to study the trend in features like ‚ÄòNum_Employees‚Äô and ‚ÄòNum_Customers‚Äô over the time especially during Black Friday and Christmas for a given store, we have visualized the data using plotly. Below we can see the Number of employees required climbs up during the holidays season to able to help high volume of customers needs.

![alt text](images/NumCustomer-Plot.png)

### Task Analysis
---

|Domain Task | Analytic Task (Low Level) | Search Task (Mid Level) | Analyze Task (High Level) |
|------------|---------------------------|-------------------------|---------------------------|
|Examine if the sales prices will rise or fall | Identify | Lookup | Present |			
|Compare the projected sales per store | Compare |	Lookup | Present/Derive |
|Compare the trained model's performance with actual sales prices | Compare	| Lookup | Derive |
|Compare the rise or fall of sales price from a previously set base price | Compare	| Lookup | Derive |
|Sales predictions of a given store over time (Days/Weeks) | Filter | Lookup | Derive |
|Trend in sales of different stores at a given point of time |	Identify | Lookup | Present |
|Comparision of Sales between of a given stores (Actual/Predicted) | Compare | Lookup | Present/Derive |
|Number of Employee needed at different times of an year given a store | summarize | Lookup | Present |
|Number of Customers at different times of an year given a store | summarize | Lookup | Present |
|Amount of Sales during black friday and christmas (Actual/Predicted) |	Compare | Lookup | Present/Derive |
|Feature contibuting positively for the sales prediction | Identify | Locate | Derive |


### Model Description
---

#### LSTM

<p align="justify"> Long short term memory is a modification of the vanilla recurrent neural networks. LSTMs can process entire sequences of data (such as speech or video). This sequence learning capability is LSTM the reason it became a choice for our time series modeling.</p>

<p align="justify"> Since the size of our dataset is small, walk forward validation has been employed for performance comparison and tuning of our models. The modeling for  LSTM has been conditioned on the store. The sales data of a store between the durations from the beginning date until 2014-01-01, 2014-02-01, 2014-03-01, 2014-04-01, and 2014-05-01 has been used for training and the rest of the data has been used in testing during the walk forward cross-validation. The sequence model with 3 LSTM layers has been able to obtain an average RMSE of 18116.48.</p>

#### ARIMA

<p align="justify"> An autoregressive integrated moving average, or ARIMA, is a statistical analysis model. It is a  forecasting technique that projects the future values of a series based entirely on its own inertia. We have decomposed the dataset into different components like trend, seasonality, residual. Then the data is made stationary using differentiation.</p>

<p align="justify"> We have used the first 18 months as a part of the training data and the next 6 months as a part of the testing data. The RMSE  is calculated for each store and we have obtained an average RMSE of 18565.45  with considering only the Sales as a time-dependent feature.</p>

<p align="justify"> A different approach of using ARIMA  is experimented by considering multiple features of the store but the RMSE did not see any improvement.</p>


#### XGBoost

<p align="justify"> XGBoost is a gradient boosting algorithm. It is also known as regularized boosting which has a good bias-variance tradeoff to reduce overfitting (which lacks in GBM). In order to use the supervised machine learning algorithms on time series data, we have used feature engineering techniques to extract time-related features like dayofweek, quarter, year, month, day. Also to capture the seasonality of the data, we have created Moving Averages and lags of different time windows which are extracted using autocorrelation. At the end of the feature engineering, we have below. </p>

- Time-related Features (DayOftheWeek, WeekOftheYear, Quarter)

- Seasonality Features (Lags, Moving Averages, Difference)

- Static Features (like holiday, destination event, nearXmas) 


<p align="justify"> We used the first 18 months of data as part of training dataset and last six months as the test dataset. For model evaluation, we have used root mean square error (RMSE).  In the initial model design we have considered all the extracted and static features until the present day (i.e., to predict the sales at time ‚Äòt‚Äô we used features until time ‚Äòt‚Äô), which resulted in a better model performance with RMSE value of 6138.18 for XGBoost model. As suggested, we have restricted the features to time step ‚Äòt-1‚Äô (i.e., to predict sales at ‚Äòt‚Äô we gave the features until time step ‚Äòt-1‚Äô as input to model) and observed increase in RMSE value to 6956.57 on test data.</p>

<p align="justify"> To have a better understanding of the model performance variability we have used forward validation (explained more in the next section) where we received an average RMSE value of 6131.95.</p>


### Design Process
---

<p align="justify"> After reviewing the task analysis and from our group discussions, we came to the conclusions that there would be two primary purposes the visualizations we build. The first would be to ‚Äúpresent‚Äù - the predictions of the model and derive insights from them. These would be the primary output of the time series forecasting application. The secondary purpose would be ‚Äúdiscover‚Äù - to ensure the quality of models. </p>

#### Consumers of our visualizations

There would be two types of consumers for our visualizations:

- Data Analyst (s) / Data Scientist(s) - People who can infer the visualization and derive conclusions on the performance of the model.

- Store Managers - People who would be looking at the sales prediction visualizations from the model and optimizing their resources in accordance with them.


####  Iterative Improvement

- Stacked Area Chart

<p align="justify"> The original sketch is to show multiple area plots having a shared x-axis. Instead of having multiple area charts, a stacked area chart is chosen to visualize the historical sales data across all the stores over time .The idea is to see the sales across different stores at the same time. Store being the categorical feature we have encoded each value with different color to show the variations. </p>

- Waterfall Chart 

<p align="justify"> The original sketch demonstrated to plot the daily variation of prices from a base price, in the up or down direction. Instead of this idea, a waterfall model plot will be used. The plot will show the increase or decrease in price from the previous day, keeping the previous day as the baseline. The bars in the plot will be encoded in two colors, showing the rise or fall from the previous day‚Äôs sale. In addition, the plot will also present the percentage of increase in price from the previous day above the bars. </p>

![alt text](images/WaterFall-2.png)

- Validation Chart

<p align="justify"> We have used Forward validation technique to evalute the time series data. The initial design had only the line chart with the validation results using 6 steps of size 30 days. In order to incorporate the store location feature we brought Pie chart and linked with line chart based on the location where each arc of the pie indicates the average sales in that particular store and also changed the number of forward validation steps to 3 steps with step size of 60 days from the forecast date selected by user.  </p>

<p align="justify"> In order to display the average sales value of a store, we changes the pie chart to doughnut chart where the area inside inner circle is used to display the average sales information using onhover functionality. Also we used onclick function to show the model performance of store selected by the user on pie chart. </p>

![alt text](images/ValPlot-Plotly.png)


#### Usability Testing Feedback

- The store location is added in the waterfall chart when a particular store is clicked in the area plot.  And all the legends and labels are made appropriate to the plots. On hover is added to the area chart so that the user can get the exact sales for a day hovered.

- The area chart and the waterfall chart are advised to be kept adjacent and the changes are reflected in the app.


### Final Visualization
---

#### Model Performance Evaluation

![alt text](images/Validation-plot.png)

<p align="justify"> Doughnut chart describes the historical average sales at different stores. As the Store is an categorical variable, each value is encoded using a different color attribute. The arc length of each slice is proportional to the average sales of a particular location. The location lebel on each arc provide initial overview to the user on what each arc represents. On hovering over each location, we get the average sales of that store displayed in the area of inner circle with the location which provides on demand details. </p>

<p align="justify"> Onclick on the hovered store, links to the model performance evalution (Time Series Cross Validation) plot using line charts for that particular store using user selected model. </p>

<p align="justify"> Multiple line charts describe the model performance using forward validation of a particular store. This approach uses the input from user on the initial train end date, until which, data is considered to be as part of training data and next two months as the validation data (validation window period = 2 months). In the next step, we consider data until validation end date in first step to be part of trainig data (training data increased by 2 months) and next two months as validation data. This will be repeated for one more step. This is considered to be a practical approach as we validate and retrain model using the future data in real world. </p>

<p align="justify"> As the Sales and Time are quantitative variables, we have used position as channel which is ranked first in effective channel ranking by datatype for quantitative variables. As we have two attributes Actual and Predicted sales over the Y-axis, we used two different colors to encode the attributes where 'steelblue' represents the actual sales and 'tomato' represents the predicted sales value. This plot uses simple and effective ways to visualize the model performace as it makes use effecitve channels, marks, avoids the extra unnecessary dimension and minimizes the occlusion by using only to attributes at a given time. </p>

 
#### Area Plot for Visualizing Historic Data

![alt text](images/Area-plot.png)

<p align="justify"> The stacked area chart describes about the historical sales data for all the stores. Each store is encoded using a color attribute and it is aggregated month-wise. On Hovering over the Area plot on a particular store, we can get the details of the sales in a store for a particular day. </p>
	
<p align="justify"> An Onclick on the selected store links to the sales forecast for the next 7 days after the forecast start date for a particular selected store. </p>
	
#### Forecasting Predicitons

<p align="justify"> The waterfall plot provides a visualization of the predictions from the models integrated to the applications. The forecast date can be provided as one of the user inputs while the model is being trained/re-trained. </p>

<p align="justify"> The plot shows the variation of prices (up/down) and the percentage change in the prices from the prices from the previous day. An increase in sales is represented by a gray bar whereas a decrease is represented by an yellow bar. </p>



### Conclusion
---

<p align="justify"> Application enables users Data Scientists/Store Managers see the overall trends in the sales across different locations, makes use of machine learning models (both Supervised and Time Series) to forecast the sales from the forecast date selected by the user using D3 Visualizations. We want provide the user a platform to understand the model performance by parameter tunning on three different models rather than taking only the best performing model. In addition to allowing the user to retrain and tune three different machine learning models, the application also displays the model performance, past information and forecasted predictions. </p>



### Future Work
---

The major addons we want to include are:

- Interpretability 

<p align="justify"> The current visualization let's the user know the sales prediction ahead. We want to extend this to let the user know Which features drive the decision of increase/decrease in the sales. This will help the store managers on their Next Best Action (NBA) and manage store effectively. </p>

- Evaluation Metrics

<p align="justify"> Include more evaluation metrics as part of validation plot that will help a Data Scientist to make better decisions on the model performance. </p>

- Comprehensive

<p align="justify"> Enable user to upload different datasets on time series and able to predict the target variable for the regression problems. </p>


### Related Research
---

Related Research

[1] Microsoft Time Series Algorithm: P. Mekala B. Srinivasan. Time series data prediction on shopping mall. In International Journal of Research in Computer Application and Robotics, Aug 2014.

[2] Simon Scheider et. all Maike Krause-Traudes. Spatial data mining for retail sales forecasting. 11th AGILE International Conference on Geographic Information Science, 2008.

[3] Liu Yunpeng ; Hou Di ; Bao Junpeng ; Qi Yong. Multi-step Ahead Time Series Forecasting for Different Data Patterns Based on LSTM. Published in 2017 14th Web Information Systems and Applications Conference (WISA) by IEEE.

[4] A multi-step approach to time series analysis and gene expression clustering. Published in Oxford Academic. https://academic.oup.com/bioinformatics/article/22/5/589/205917

[5] Multi-scale Internet traffic forecasting using neural networks and time series methods.
https://onlinelibrary.wiley.com/doi/full/10.1111/j.1468-0394.2010.00568.x

[6] Kaggle Competition: Forecast sales using store, promotion, and competitor data.
https://www.kaggle.com/c/rossmann-store-sales/kernels



# SQL-in-Python


Integrating SQL queries into a Python Jupyter Notebook using the ipython-sql extension streamlines the process of working with databases and analyzing data. The ipython-sql magic commands allow users to execute SQL queries directly within a notebook cell, blending the flexibility of SQL with the powerful data analysis capabilities of Python. This integration provides a seamless experience for users who need to interact with databases and manipulate data without switching contexts between different tools. By leveraging the %sql magic command, users can execute SQL queries, fetch results, and display them in a format that's easy to work with, all within the interactive notebook environment.

One of the key advantages of using ipython-sql is the ability to harness the strengths of both SQL and Python within a single interface. SQL is renowned for its efficiency in querying and managing relational databases, while Python excels in data analysis and visualization. By running SQL queries in a Jupyter Notebook, users can quickly extract and preprocess data from databases and then apply Python's extensive data manipulation libraries, such as Pandas,numpy and Matplotlib to perform more complex analyses and visualizations. This integration enhances productivity by reducing the need to manually export data between SQL environments and Python scripts.

Additionally, the ipython-sql extension provides opportunities for more interactive and exploratory data analysis. Users can leverage the notebook's interactive features to iteratively refine SQL queries and immediately see the impact on their data. This iterative process is especially useful for data scientists and analysts who need to explore large datasets and develop insights in real-time. Moreover, the ability to document and share notebooks with embedded SQL queries and their results facilitates better collaboration and reproducibility in data-driven projects, making ipython-sql a valuable tool in both educational and professional settings.


# Vinylango25


# üí´ About Me
### üí´ Hi there! üëã I'm Kipkemoi Vincent  

I am a passionate Data Scientist, ML/AI Engineer & Data Analyst with expertise in transforming raw data into actionable insights and developing scalable data science/ ML solutions in production environments. With experience across fintech, environmental science, healthcare, and agri-tech, I build machine learning models, predictive systems, 
and analytical solutions that drive growth and optimize decision-making.

Currently based in **Nairobi, Kenya**, open to opportunities **worldwide**.

---

# üìã Professional Summary

- 5+ years experience delivering Data Science and ML solutions across sectors (Fintech, Climate, Healthcare).
- Skilled in data analytics, predictive analytics,credit risk analysis, anomaly & fraud detection, credit scoring,
 time series forecasting, and scalable ML model deployment.
- Proficient in Python, R, SQL, Scikit-learn, CNN, OpenCV, TensorFlow, RF, LSTM,Catboost, XGBoost, LightGBM,PyOD models and AWS.
- Experienced with MLOps workflows: streamlit, Docker, Kubernetes, MLflow, FastAPI, and CI/CD pipelines.
- Committed to leveraging advanced analytics for risk minimization, business growth, and social impact.

---
## üåç Connect with Me  
<img src="https://cdn-icons-png.flaticon.com/512/174/174857.png" width="20" /> [![LinkedIn](https://img.shields.io/badge/LinkedIn-blue?style=flat&logo=linkedin)](https://www.linkedin.com/in/kipkemoi-vincent-19307a94/)   
üì´ **Email:** vincentl@aims.ac.za/vinylango90@gmail.com

---

## üíº Featured projects

| Project | Description | Tools Used |
|--------|-------------|------------|
| [üõ°Ô∏è Anomaly and Fraud Detection in Finance](https://github.com/Vinylango25/Anomaly-Fraud-detection-in-Finance) | This project applies PyOD and Microsoft AutoML (FLAML) to detect anomalies in credit card transactions using a highly imbalanced dataset. A variety of algorithms‚Äîincluding Isolation Forest and Autoencoders‚Äîwere tested for their ability to flag suspicious activity. To address imbalance, techniques like undersampling, oversampling, and SMOTE were applied. Evaluation focused on metrics like Precision, Recall, and ROC-AUC for a robust assessment. The outcome is a high-precision fraud detection pipeline that enhances financial risk management. üëâ Read the full project on [Github](https://github.com/Vinylango25/Anomaly-Fraud-detection-in-Finance) or  [Medium](https://medium.com/@vinylango90/anomaly-detection-in-credit-card-transactions-using-pyod-and-microsoft-automl-flaml-727a091487bd)| Python, Scikit-learn, LightGBM, FLAML, PyOD, LIME, SHAP |
| [ü§ñ Lending Automation - ML for Credit Scoring](https://github.com/Vinylango25/Credit-Scoring-Lending-automation) | This project builds an end-to-end loan approval system using machine learning algorithms like Random Forest, XGBoost, and LightGBM. It replaces manual decision-making with faster, scalable, and data-driven processes for improved credit scoring. Key tasks include data cleaning, feature engineering, and model optimization using real-world loan data. Evaluation metrics ensure accuracy and fairness, reducing false approvals and rejections. The system supports personalized lending and dynamic pricing for better customer experience. üëâ Read the full project on [Github](https://github.com/Vinylango25/Credit-Scoring-Lending-automation) or [Medium](https://medium.com/@vinylango90/credit-scoring-lending-automation-6dabe30d53da)| Python, Scikit-learn, RF, XGBoost, LightGBM |
| [üåç Air Quality Monitoring in Nicosia, Cyprus](https://github.com/Vinylango25/Air-Quality-in-Nicosia-Cyprus) | To improve urban air quality monitoring, this project calibrates low-cost electrochemical sensors using ML algorithms like XGBoost, Random Forest, and ANN. Raw sensor data, collected over six months, is aligned with reference-grade measurements. The study analyzes calibration frequency, data sampling strategies, and environmental factors like humidity and cross-gas interference. Results show that with proper calibration, LCSs can meet EU and EPA accuracy standards. This opens doors for cost-effective, citywide monitoring networks. üëâ Read the full project on [Github](https://github.com/Vinylango25/Air-Quality-in-Nicosia-Cyprus)  or [Medium](https://medium.com/@vinylango90/sensor-calibration-and-air-quality-monitoring-in-nicosia-cyprus-e5072a4184aa)| Python, Scikit-learn, XGBoost, Random Forest, Data Analysis, Jupyter Notebook |
| [ü©∫ COVID-19 Detection Using CT Scans](https://github.com/Vinylango25/Covid-19-Detection-Deep-Learning) | This project applies deep learning to detect COVID-19 from chest CT scans using convolutional neural networks. Models like ResNet50, DenseNet169, and MobileNetV2 are trained and fine-tuned for accurate image classification, achieving high detection accuracy through transfer learning and ensemble methods. The pipeline includes image preprocessing, augmentation, and evaluation with real-world datasets. The system supports rapid and reliable diagnosis, aiding medical decision-making. üëâ Read the full project on [Github](https://github.com/Vinylango25/Covid-19-Detection-Deep-Learning) or [Medium](https://medium.com/@vinylango90/deep-learning-based-detection-of-covid-19-from-ct-scans-using-convolutional-neural-networks-a09ca1a6f9ce) |Python, TensorFlow, Keras, CNN, ResNet50, DenseNet169, MobileNetV2 |
| [üìâ Customer Churn Analysis and Prediction](https://github.com/Vinylango25/Customer_Churn_Analysis_And_Prediction) |This project predicts customer churn using survival analysis and machine learning to identify clients likely to leave. It focuses on telecom-style use cases where retaining customers is more cost-effective than acquiring new ones. By analyzing historical behavior and risk factors, it enables targeted retention campaigns. An interactive tool is also developed to assess individual churn risk and lifetime value. This helps businesses make data-driven decisions to reduce attrition and improve customer loyalty. üëâ Read the full project on [Github](https://github.com/Vinylango25/Customer_Churn_Analysis_And_Prediction). | Python, Flask, Scikit-learn, SHAP, Cox Proportional Hazards Model, Survival Analysis |
| [üìä Sales Time Series Analysis](https://github.com/Vinylango25/Sales_Time_Series_Analysis) | This project analyzes historical sales data using time series forecasting techniques to uncover trends, seasonality, and patterns that drive business performance. By leveraging models like ARIMA, SARIMA, and Prophet, it enables accurate sales forecasting to support inventory planning, resource allocation, and strategic decision-making. The pipeline includes data preprocessing, stationarity testing, model tuning, and performance evaluation. The insights generated help optimize operations and reduce forecasting errors. üëâ Read the full project on [Github](https://github.com/Vinylango25/Sales_Time_Series_Analysis)| Python, Pandas, Statsmodels, FBProphet, ARIMA, SARIMA |
| [üíª Machine Learning Loan Application Web App](https://github.com/Vinylango25/Machine_Learning_Loan_Application_Web_App) | This project delivers a full-stack web application for automated loan approval using machine learning models. Built with Streamlit and powered by Random Forest and Logistic Regression, the system predicts loan eligibility based on user input. The backend includes data preprocessing, model training, and evaluation, while the frontend ensures a seamless user experience. It streamlines loan applications by replacing manual reviews with instant, data-driven decisions. üëâ Read the full project on [Github](https://github.com/Vinylango25/Machine_Learning_Loan_Application_Web_App) | Python, Scikit-learn, Streamlit, Random Forest, Logistic Regression, Flask|
| [üöÄ Active Learning API (Django + CatBoost)](https://github.com/Vinylango25/active-learning-backend) | This project implements an active learning backend system to optimize data labeling for machine learning tasks. It intelligently selects the most informative samples for annotation, reducing labeling effort while improving model performance. Built with Django and integrated with machine learning models (e.g., CatBoost), it supports iterative learning cycles, model versioning, and anomaly detection. Key features include dataset management, active query strategies, and seamless MLflow tracking. üëâ Read the full project on [Github](https://github.com/Vinylango25/active-learning-backend)| Python, Django, CatBoost, MLflow, Active Learning, REST API, SQLite|
| [üß† Brain Tumor Detection Using Deep Learning](https://github.com/Vinylango25/Brain-Tumor-Detection-Using-Deep-Learning) |his project leverages deep learning to detect brain tumors from MRI scans using advanced convolutional neural networks. It employs architectures like MobileNetV2, DenseNet169, and ResNet50, enhanced through transfer learning and ensemble techniques to achieve up to 99.8% accuracy. The workflow includes image preprocessing, model training, and performance evaluation, enabling fast, accurate, and scalable tumor classification to support early diagnosis and clinical workflows. üëâ Read the full project [Github](https://github.com/Vinylango25/Brain-Tumor-Detection-Using-Deep-Learning) | Python, TensorFlow, Keras, CNN, MobileNetV2, ResNet50, DenseNet169, FLAML|

---



## üíª Tech Stack  
<p align="left">
  <img src="https://img.shields.io/badge/Python-blue?style=flat&logo=python" height="30">
  <img src="https://img.shields.io/badge/R-lightgrey?style=flat&logo=r" height="30">
  <img src="https://img.shields.io/badge/SQL-orange?style=flat&logo=postgresql" height="30">
  <img src="https://img.shields.io/badge/TensorFlow-orange?style=flat&logo=tensorflow" height="30">
  <img src="https://img.shields.io/badge/PyTorch-red?style=flat&logo=pytorch" height="30">
  <img src="https://img.shields.io/badge/Scikit--Learn-yellow?style=flat&logo=scikit-learn" height="30">
  <img src="https://img.shields.io/badge/XGBoost-orange?style=flat&logo=xgboost" height="30">
  <img src="https://img.shields.io/badge/CatBoost-blue?style=flat&logo=catboost" height="30">
  <img src="https://img.shields.io/badge/LightGBM-green?style=flat&logo=lightgbm" height="30">
  <img src="https://img.shields.io/badge/Sktime-blue?style=flat" height="30">
  <img src="https://img.shields.io/badge/LSTM-red?style=flat" height="30">
  <img src="https://img.shields.io/badge/Django-green?style=flat&logo=django" height="30">
  <img src="https://img.shields.io/badge/Flask-black?style=flat&logo=flask" height="30">
  <img src="https://img.shields.io/badge/FastAPI-blue?style=flat&logo=fastapi" height="30">
  <img src="https://img.shields.io/badge/Docker-blue?style=flat&logo=docker" height="30">
  <img src="https://img.shields.io/badge/Kubernetes-darkblue?style=flat&logo=kubernetes" height="30">
  <img src="https://img.shields.io/badge/AWS-yellow?style=flat&logo=amazon-aws" height="30">
  <img src="https://img.shields.io/badge/Gurobi-red?style=flat" height="30">
  <img src="https://img.shields.io/badge/CPLEX-darkblue?style=flat" height="30">
  <img src="https://img.shields.io/badge/Power%20BI-yellow?style=flat&logo=power-bi" height="30">
  <img src="https://img.shields.io/badge/Tableau-blueviolet?style=flat&logo=tableau" height="30">
  <img src="https://img.shields.io/badge/Excel-green?style=flat&logo=microsoft-excel" height="30">
  <img src="https://img.shields.io/badge/STATA-blue?style=flat" height="30">
  <img src="https://img.shields.io/badge/Matlab-orange?style=flat&logo=mathworks" height="30">
  <img src="https://img.shields.io/badge/SPSS-lightblue?style=flat" height="30">
  <img src="https://img.shields.io/badge/Gretl-darkgreen?style=flat" height="30">
  <img src="https://img.shields.io/badge/Singular-purple?style=flat" height="30">
  <img src="https://img.shields.io/badge/Preset-black?style=flat" height="30">
  <img src="https://img.shields.io/badge/DBeaver-darkred?style=flat" height="30">
  <img src="https://img.shields.io/badge/SageMath-darkblue?style=flat" height="30">
  <img src="https://img.shields.io/badge/Git-black?style=flat&logo=git" height="30">
  <img src="https://img.shields.io/badge/LATEX-blue?style=flat&logo=latex" height="30">
  <img src="https://img.shields.io/badge/FLAML-orange?style=flat" height="30">
  <img src="https://img.shields.io/badge/Looker-purple?style=flat&logo=looker" height="30">
</p>

## üèÜ GitHub Trophies

![GitHub Trophies](https://github-profile-trophy.vercel.app/?username=Vinylango25&theme=dracula&width=1200)  
![GitHub Trophies](https://github-profile-trophy.vercel.app/?username=vinylango&theme=dracula&width=1200)

---


## üìä GitHub Stats

<table>
<tr>
<td align="center">
  
### **Vinylango25 Stats**  
 
<img src="https://streak-stats.demolab.com/?user=Vinylango25&theme=dark&hide_border=false" alt="GitHub Streak" />  
<img src="https://github-readme-stats.vercel.app/api?username=Vinylango25&show_icons=true&theme=dark" alt="GitHub Stats" />  
<img src="https://github-readme-stats.vercel.app/api/top-langs/?username=Vinylango25&theme=dark&layout=compact" alt="Top Languages" />

</td>

<td align="center">

### **Vinylango Stats**  

<img src="https://streak-stats.demolab.com/?user=vinylango&theme=dark&hide_border=false" alt="GitHub Streak" />  
<img src="https://github-readme-stats.vercel.app/api?username=vinylango&show_icons=true&theme=dark" alt="GitHub Stats" />  
<img src="https://github-readme-stats.vercel.app/api/top-langs/?username=vinylango&theme=dark&layout=compact" alt="Top Languages" />

</td>
</tr>
</table>




## üìú Random Dev Quote  
![Quote](https://quotes-github-readme.vercel.app/api?type=horizontal)  

---

## üöÄ Let's Collaborate!  
I am open to working on **AI, ML, data science, and fintech projects**.  
üí¨ **Reach out to me for exciting collaborations!**  


# vinylango25.github.io


# Chirpy Starter

[![Gem Version](https://img.shields.io/gem/v/jekyll-theme-chirpy)][gem]&nbsp;
[![GitHub license](https://img.shields.io/github/license/cotes2020/chirpy-starter.svg?color=blue)][mit]

When installing the [**Chirpy**][chirpy] theme through [RubyGems.org][gem], Jekyll can only read files in the folders
`_data`, `_layouts`, `_includes`, `_sass` and `assets`, as well as a small part of options of the `_config.yml` file
from the theme's gem. If you have ever installed this theme gem, you can use the command
`bundle info --path jekyll-theme-chirpy` to locate these files.

The Jekyll team claims that this is to leave the ball in the user‚Äôs court, but this also results in users not being
able to enjoy the out-of-the-box experience when using feature-rich themes.

To fully use all the features of **Chirpy**, you need to copy the other critical files from the theme's gem to your
Jekyll site. The following is a list of targets:

```shell
.
‚îú‚îÄ‚îÄ _config.yml
‚îú‚îÄ‚îÄ _plugins
‚îú‚îÄ‚îÄ _tabs
‚îî‚îÄ‚îÄ index.html
```

To save you time, and also in case you lose some files while copying, we extract those files/configurations of the
latest version of the **Chirpy** theme and the [CD][CD] workflow to here, so that you can start writing in minutes.

## Usage

Check out the [theme's docs](https://github.com/cotes2020/jekyll-theme-chirpy/wiki).

## Contributing

This repository is automatically updated with new releases from the theme repository. If you encounter any issues or want to contribute to its improvement, please visit the [theme repository][chirpy] to provide feedback.

## License

This work is published under [MIT][mit] License.

[gem]: https://rubygems.org/gems/jekyll-theme-chirpy
[chirpy]: https://github.com/cotes2020/jekyll-theme-chirpy/
[CD]: https://en.wikipedia.org/wiki/Continuous_deployment
[mit]: https://github.com/cotes2020/chirpy-starter/blob/master/LICENSE


# website


# [louisite.com](https://louisite.com/)

[![Netlify Status](https://api.netlify.com/api/v1/badges/b5ed574c-98f0-4d7e-bec5-ebcf23f65039/deploy-status)](https://app.netlify.com/sites/louisite/deploys)

**louisite.com** (stylized as LOUI**SITE**, a portmanteau of _Louis_ and _site_) is my personal website‚Äîthis is the second iteration‚Äîshowcasing my accomplishments. This website was built with [Tailwind CSS](https://tailwindcss.com/), [React](https://reactjs.org/), and [TypeScript](https://www.typescriptlang.org/).

[Visit the website here](https://louisite.com/)

![Preview](src/assets/images/preview.png)

## Overview

### Background

As an undergraduate looking for opportunities, I realized that I needed to build a visually appealing personal website to showcase my accomplishments and to provide hiring managers with a better idea of who I am.

### The problem

[The first iteration](https://github.com/leejhlouis/louisite-v1) failed to highlight my featured projects, the most important aspect of this website, due to other sections‚Äîto name a few, my experience, educational background, and my skills‚Äîtaking precedence over the works section. In addition, I was personally not satisfied with the first iteration‚Äôs appearance. This prompted me to develop the second iteration with an all-new and refreshed layout and appearance.

### A solution

To prevent stuff from bloating the single index page, I decided to build a multi-page website, in which I extracted the about me section to another page. I also merged several sections‚Äîmy experience, educational background, and skills‚Äîto the ‚Äúabout me‚Äù page to minimize content. The website should not repeat what my [LinkedIn](https://www.linkedin.com/in/louis-gustavo) page does.

Originally, I intended to feature a detailed page for each work. However, amidst the development phase, I personally thought that it would be better if I include them on their respective GitHub repository‚Äôs `README.md` for easier content management.

On top of that, I implemented glassmorphism‚Äîa frosted glass effect popularized by [Michal Malewicz](https://uxdesign.cc/glassmorphism-in-user-interfaces-1f39bb1308c9)‚Äîin the navbar with Tailwind CSS‚Äôs [backdrop blur](https://tailwindcss.com/docs/backdrop-blur) utility classes. I also created a dark mode for this iteration by utilizing Tailwind CSS‚Äôs [dark mode variant](https://tailwindcss.com/docs/dark-mode). By default, this website uses the users‚Äô preferred color theme, although the users can also manually override the theme.

### Technologies

The website was initially bootstrapped with [Create React App](https://create-react-app.dev/) (CRA) and built with Tailwind CSS, React, and TypeScript. Tailwind CSS was used as the CSS framework for its practical utility classes to build the interface. Meanwhile, React was chosen due to its beautifully curated JavaScript library for UI components and TypeScript was chosen due to its type-checking feature. Furthermore, I use `.markdown` for managing the content of the [about me](https://louisite.com/about) page.

In 2023, I [migrated](https://github.com/leejhlouis/louisite.com/pull/9) CRA to [Vite](https://vitejs.dev/), a simpler and faster build tool alternative to CRA.

## How to run the website locally

1. Clone this repository to your local machine

```
git clone https://github.com/leejhlouis/louisite.com.git
```

2. Change directory to `/louisite.com`, the project directory

```
cd louisite.com
```

3. Install the dependencies locally (**make sure to have Node.js or any other package manager installed**)

```
npm install
```

4. Run the website in the development mode

```
npm run dev
```

5. Open http://localhost:5173 to view the website.


# Zindi-competition-challenge


